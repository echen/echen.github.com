
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Netflix Prize Summary: Factorization Meets the Neighborhood - My Octopress Blog</title>
  <meta name="author" content="Your Name">

  
  <meta name="description" content="(Way back when, I went through all the Netflix prize papers. I&#8217;m now (very slowly) trying to clean up my notes and put them online. Eventually &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://echen.github.com/blog/2011/03/14/summary-factorization-meets-the-neighborhood">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="My Octopress Blog" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">My Octopress Blog</a></h1>
  
    <h2>A blogging framework for hackers.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:echen.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Netflix Prize Summary: Factorization Meets the Neighborhood</h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-14T04:21:52-07:00" pubdate data-updated="true">Mar 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>(Way back when, I went through all the Netflix prize papers. I&#8217;m now (very slowly) trying to clean up my notes and put them online. Eventually, I hope to have a more integrated tutorial, but here&#8217;s a rough draft for now.)</p>

<p>This is a summary of Koren&#8217;s 2008 <a href="public.research.att.com/~volinsky/netflix/kdd08koren.pdf">Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model</a>.</p>

<p>There are two approaches to collaborative filtering: neighborhood methods and latent factor models.</p>

<ul>
<li>Neighborhood models are most effective at detecting very localized relationships (e.g., that people who like X-Men also like Spiderman), but poor at detecting a user&#8217;s overall signals.</li>
<li>Latent factor models are best at estimating overall structure (e.g., that a user likes horror movies), but are poor at detecting strong associations among small sets of closely related items.</li>
</ul>


<p>Since the two approaches have complementary strengths and weaknesses, we should integrate the two; this integration is the focus of this paper.</p>

<h1>Preliminaries</h1>

<p>As mentioned in previous papers, we should normalize out common effects from movies. Throughout the rest of this paper, Koren uses a baseline estimate of overall rating mean + user deviation from average + movie deviation from average for the rating of user i on movie i; estimation of the latter two parameters are done by solving a regularized least squares problem.</p>

<p>Koren then describes using a binary matrix (1 for rated, 0 for not rated) as a source of implicit feedback. This is useful because the mere fact that a user rated many science fiction movies (say) suggests that the user likes science fiction movies.</p>

<h1>A Neighborhood Model</h1>

<p>Recall the previous paper, where we modeled each rating $r_{ui}$ as</p>

<p>$$r<em>{ui} = b</em>{ui}+ \sum<em>{N \in N(i; u)} (r</em>{uj} - b<em>{uj}) w</em>{ij},$$</p>

<p>where $N(i; u)$ is the k items most similar to i among the items user u rated, and the $w_{ij}$ are parameters to be learned by solving a regularized least squares problem.</p>

<p>This paper makes several enhancements to that model. First, we replace $N(i; u)$ with $R<sup>k(i;</sup> u)$, the intersection of the k items most similar to i (among all items) intersected with the items user u rated. Also, we denote by $N<sup>k(i;</sup> u)$ the intersection of the k items most similar to i with the items user u has provided implicit feedback for. This gives us</p>

<p>$$r<em>{ui} = b</em>{ui} + \sum<em>{j \in R<sup>k(i;</sup> u)} (r</em>{uj} - b<em>{uj}) w</em>{ij} + \sum<em>{j \in N<sup>k(i;</sup> u)} c</em>{ij},$$</p>

<p>where the $c_{ij}$ are another set of parameters to learn.</p>

<p>Notice that by taking the intersection of the k items most similar to i with the items user u rated (giving perhaps a set of size less than k), rather than taking the k items most similar to i among the items user u rated, we let our model be influenced not only by what a user rates, but also by what a user does not rate. For example, if a user does not rate LOTR 1 or LOTR 2, his predicted rating for LOTR 3 is penalized.</p>

<p>This implies that our current model encourages greater deviations from baseline estimates for users that provided many ratings or plenty of implicit feedback. In other words, for well-modeled users with a lot of input, we are willing to predict quirkier and less common recommendations; users we have less information about, on the other hand, receive safer, baseline estimates.</p>

<p>Nonetheless, this dichotomy between power users and newbie users is perhaps overemphasized by our current model, so we moderate the dichotomy by modifying our model to be</p>

<p>$$r<em>{ui} = b</em>{ui} + |R<sup>k(i;</sup> u)|<sup>{-0.5}</sup> \sum<em>{j \in R<sup>k(i;</sup> u)} (r</em>{uj} - b<em>{uj}) w</em>{ij} + |N<sup>k(i;</sup> u)|<sup>{-0.5}</sup> \sum<em>{j \in N<sup>k(i;</sup> u)} c</em>{ij}.$$</p>

<p>Parameters are determined by solving a regularized least squares problem.</p>

<h1>Latent Factor Models Revisited</h1>

<p>Typical SVD approaches are based on the following rule:</p>

<p>$$r<em>{ui} = b</em>{ui} + p_u<sup>T</sup> q_i,$$</p>

<p>where $p_u$ is a user-factors vector and $q_i$ is an item-factors vector. We describe two enhancements.</p>

<h2>Asymmetric-SVD</h2>

<p>One suggestion is to replace $p_u$ with</p>

<p>$$|R(u)|<sup>{-0.5}</sup> + \sum<em>{j \in R(u)} (r</em>{uj} - b<em>{uj}) x_j + |N(u)|<sup>{-0.5}</sup> \sum</em>{j \in N(u)} y_j,$$</p>

<p>where $R(u)$ is the set of items user u has rated, and $N(u)$ is the set of items user u has provided implicit feedback for. In other words, this model represents users through the items they prefer, rather than expressing users in a latent feature space. This model has several advantages:</p>

<ul>
<li>Asymmetric-SVD does not parameterize users, so we do not need to wait to retrain the model when a user comes in. Instead, we can handle new users as soon as they provide feedback.</li>
<li>Predictions are a direct function of past feedback, so we can easily explain predictions. (When using a pure latent feature solution, however, explainability is difficult.)</li>
</ul>


<p>As usual, parameters are learned via a regularized least-squares minimization.</p>

<h2>SVD++</h2>

<p>Another approach is to continue modeling users as latent features, while adding implicit feedback. Thus, we replace $p_u$ with $p_u + |N(u)|<sup>{-0.5}</sup> \sum_{j \in N(u)} y_j$. While we lose the easily explainability and immediate feedback of the Asymmetric-SVD model, this approach is likely more accurate.</p>

<h1>An Integrated Model</h1>

<p>An integrated model incorporating baseline estimates, the neighborhood approach, and the latent factor approach is as follows:</p>

<p>$$r<em>{ui} = \left[\mu + b_u + b_i\right] +\left[q_i<sup>T</sup> \big(p_u + \sqrt{|N(u)|}\sum</em>{j \in N(u)} y_j \big)\right] + \left[\sqrt{|R<sup>k(i;u)}</sup> \sum<em>{j \in R<sup>k(i;</sup> u)}(r</em>{uj} - b<em>{uj})w</em>{ij}+\sqrt{|N<sup>k(i;u)|}</sup> \sum<em>{j \in N<sup>k(i;</sup> u)} c</em>{ij}\right].$$</p>

<p>Note that we have used $(\mu + b_u + b_i)$ as our baseline estimate. We also used the SVD++ model, but we could use the Asymmetric-SVD model instead.</p>

<p>This rule provides a 3-tier model for recommendations:</p>

<ul>
<li>The first baseline group describes general properties of the item and user. For example, it may say that &#8220;The Sixth Sense&#8221; movie is known to be a good movie in general, and that Joe rates like the average user.</li>
<li>The next latent factor group may say that since &#8220;The Sixth Sense&#8221; and Joe rate high on the Psychological Thrillers Scale, Joe may like The Sixth Sense because he likes this genre of movies in general.</li>
<li>The final neighborhood tier makes fine-grained adjustments that are hard to file, such as the fact that Joe rated low the movie &#8220;Signs&#8221;, a similar psychological thriller by the same director.</li>
</ul>


<p>As usual, model parameters are determined by minimizing the regularized squared error function through gradient descent.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Your Name</span></span>

      








  


<time datetime="2011-03-14T04:21:52-07:00" pubdate data-updated="true">Mar 14<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/expository/'>expository</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://echen.github.com/blog/2011/03/14/summary-factorization-meets-the-neighborhood/" data-via="" data-counturl="http://echen.github.com/blog/2011/03/14/summary-factorization-meets-the-neighborhood/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2011/03/14/pca-transpose-trick/" title="Previous Post: PCA Transpose Trick">&laquo; PCA Transpose Trick</a>
      
      
        <a class="basic-alignment right" href="/blog/2011/03/14/summary-scalable-collaborative-filtering-with-jointly-derived-neighborhood-interpolation-weights/" title="next Post: Netflix Prize Summary: Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights">Netflix Prize Summary: Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/01/17/quick-introduction-to-ggplot2/">Quick Introduction to ggplot2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/10/24/winning-the-netflix-prize-a-summary/">Winning the Netflix Prize: A Summary</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/09/29/stuff-harvard-people-like/">Stuff Harvard People Like</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/09/07/information-transmission-in-a-social-network-dissecting-the-spread-of-a-quora-post/">Information Transmission in a Social Network: Dissecting the Spread of a Quora Post</a>
      </li>
    
  </ul>
</section>






  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Your Name -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
