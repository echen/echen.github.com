<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: expository | My Octopress Blog]]></title>
  <link href="http://echen.github.com/blog/categories/expository/atom.xml" rel="self"/>
  <link href="http://echen.github.com/"/>
  <updated>2012-02-04T17:29:48-08:00</updated>
  <id>http://echen.github.com/</id>
  <author>
    <name><![CDATA[Your Name]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to Restricted Boltzmann Machines]]></title>
    <link href="http://echen.github.com/blog/2011/07/18/introduction-to-restricted-boltzmann-machines/"/>
    <updated>2011-07-18T09:32:52-07:00</updated>
    <id>http://echen.github.com/blog/2011/07/18/introduction-to-restricted-boltzmann-machines</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>Suppose you ask a bunch of users to rate a set of movies on a 0-100 scale. In classical <a href="http://en.wikipedia.org/wiki/Factor_analysis">factor analysis</a>, you could then try to explain each movie and user in terms of a set of latent <em>factors</em>. For example, movies like Star Wars and Lord of the Rings might have strong associations with a latent science fiction and fantasy factor, and users who like Wall-E and Toy Story might have strong associations with a latent Pixar factor.</p>

<p>Restricted Boltzmann Machines essentially perform a <em>binary</em> version of factor analysis. (This is one way of thinking about RBMs; there are, of course, others, and lots of different ways to use RBMs, but I'll adopt this approach for this post.) Instead of users rating a set of movies on a continuous scale, they simply tell you whether they like a movie or not, and the RBM will try to discover latent factors that can explain the activation of these movie choices.</p>

<p>More technically, a Restricted Boltzmann Machine is a <strong>stochastic neural network</strong> (<em>neural network</em> meaning we have neuron-like units whose binary activations depend on the neighbors they're connected to; <em>stochastic</em> meaning these activations have a probabilistic element) consisting of:</p>

<ul>
<li>One layer of <strong>visible units</strong> (users' movie preferences whose states we know and set);</li>
<li>One layer of <strong>hidden units</strong> (the latent factors we try to learn); and</li>
<li>A bias unit (whose state is always on, and is a way of adjusting for the different inherent popularities of each movie).</li>
</ul>


<p>Furthermore, each visible unit is connected to all the hidden units (this connection is undirected, so each hidden unit is also connected to all the visible units), and the bias unit is connected to all the visible units and all the hidden units. To make learning easier, we restrict the network so that no visible unit is connected to any other visible unit and no hidden unit is connected to any other hidden unit.</p>

<p>For example, suppose we have a set of six movies (Harry Potter, Avatar, LOTR 3, Gladiator, Titanic, and Glitter) and we ask users to tell us which ones they want to watch. If we want to learn two latent units underlying movie preferences -- for example, two natural groups in our set of six movies appear to be SF/fantasy (containing Harry Potter, Avatar, and LOTR 3) and Oscar winners (containing LOTR 3, Gladiator, and Titanic), so we might hope that our latent units will correspond to these categories -- then our RBM would look like the following:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png"><img src="http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png" alt="RBM Example" /></a></p>

<p>(Note the resemblance to a factor analysis graphical model.)</p>

<h1>State Activation</h1>

<p>Restricted Boltzmann Machines, and neural networks in general, work by updating the states of some neurons given the states of others, so let's talk about how the states of individual units change. Assuming we know the connection weights in our RBM (we'll explain how to learn these below), to update the state of unit $i$:</p>

<ul>
<li>Compute the <strong>activation energy</strong> $a_i = \sum_j w<em>{ij} x_j$ of unit $i$, where the sum runs over all units $j$ that unit $i$ is connected to, $w</em>{ij}$ is the weight of the connection between $i$ and $j$, and $x_j$ is the 0 or 1 state of unit $j$. In other words, all of unit $i$'s neighbors send it a message, and we compute the sum of all these messages.</li>
<li>Let $p_i = \sigma(a_i)$, where $\sigma(x) = 1/(1 + exp(-x))$ is the logistic function. Note that $p_i$ is close to 1 for large positive activation energies, and $p_i$ is close to 0 for negative activation energies.</li>
<li>We then turn unit $i$ on with probability $p_i$, and turn it off with probability $1 - p_i$.</li>
<li>(In layman's terms, units that are positively connected to each other try to get each other to share the same state (i.e., be both on or off), while units that are negatively connected to each other are enemies that prefer to be in different states.)</li>
</ul>


<p>For example, let's suppose our two hidden units really do correspond to SF/fantasy and Oscar winners.</p>

<ul>
<li>If Alice has told us her six binary preferences on our set of movies, we could then ask our RBM which of the hidden units her preferences activate (i.e., ask the RBM to explain her preferences in terms of latent factors). So the six movies send messages to the hidden units, telling them to update themselves. (Note that even if Alice has declared she wants to watch Harry Potter, Avatar, and LOTR 3, this doesn't guarantee that the SF/fantasy hidden unit will turn on, but only that it will turn on with high <em>probability</em>. This makes a bit of sense: in the real world, Alice wanting to watch all three of those movies makes us highly suspect she likes SF/fantasy in general, but there's a small chance she wants to watch them for other reasons. Thus, the RBM allows us to <em>generate</em> models of people in the messy, real world.)</li>
<li>Conversely, if we know that one person likes SF/fantasy (so that the SF/fantasy unit is on), we can then ask the RBM which of the movie units that hidden unit turns on (i.e., ask the RBM to generate a set of movie recommendations). So the hidden units send messages to the movie units, telling them to update their states. (Again, note that the SF/fantasy unit being on doesn't guarantee that we'll always recommend all three of Harry Potter, Avatar, and LOTR 3 because, hey, not everyone who likes science fiction liked Avatar.)</li>
</ul>


<h1>Learning Weights</h1>

<p>So how do we learn the connection weights in our network? Suppose we have a bunch of training examples, where each training example is a binary vector with six elements corresponding to a user's movie preferences. Then for each epoch, do the following:</p>

<ul>
<li>Take a training example (a set of six movie preferences). Set the states of the visible units to these preferences.</li>
<li>Next, update the states of the hidden units using the logistic activation rule described above: for the $j$th hidden unit, compute its activation energy $a_j = \sum_i w<em>{ij} x_i$, and set $x_j$ to 1 with probability $\sigma(a_j)$ and to 0 with probability $1 - \sigma(a_j)$. Then for each edge $e</em>{ij}$, compute $Positive(e_{ij}) = x_i * x_j$ (i.e., for each pair of units, measure whether they're both on).</li>
<li>Now <strong>reconstruct</strong> the visible units in a similar manner: for each visible unit, compute its activation energy $a_i$, and update its state. (Note that this <em>reconstruction</em> may not match the original preferences.) Then update the hidden units again, and compute $Negative(e_{ij}) = x_i * x_j$ for each edge.</li>
<li>Update the weight of each edge $e<em>{ij}$ by setting $w</em>{ij} = w<em>{ij} + L * (Positive(e</em>{ij}) - Negative(e_{ij}))$, where $L$ is a learning rate.</li>
<li>Repeat over all training examples.</li>
</ul>


<p>Continue until the network converges (i.e., the error between the training examples and their reconstructions falls below some threshold) or we reach some maximum number of epochs.</p>

<p>Why does this update rule make sense? Note that</p>

<ul>
<li>In the first phase, $Positive(e_{ij})$ measures the association between the $i$th and $j$th unit that we <em>want</em> the network to learn from our training examples;</li>
<li>In the "reconstruction" phase, where the RBM generates the states of visible units based on its hypotheses about the hidden units alone, $Negative(e_{ij})$ measures the association that the network <em>itself</em> generates (or "daydreams" about) when no units are fixed to training data.</li>
</ul>


<p>So by adding $Positive(e<em>{ij}) - Negative(e</em>{ij})$ to each edge weight, we're helping the network's daydreams better match the reality of our training examples.</p>

<p>(You may hear this update rule called <strong>contrastive divergence</strong>, which is basically a funky term for "approximate gradient descent".)</p>

<h1>Examples</h1>

<p>I wrote <a href="https://github.com/echen/restricted-boltzmann-machines">a simple RBM implementation</a> in Python (the code is heavily commented, so take a look if you're still a little fuzzy on how everything works), so let's use it to walk through some examples.</p>

<p>First, I trained the RBM using some fake data.</p>

<ul>
<li>Alice: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.</li>
<li>Bob: (Harry Potter = 1, Avatar = 0, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). SF/fantasy fan, but doesn't like Avatar.</li>
<li>Carol: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.</li>
<li>David: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.</li>
<li>Eric:  (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Oscar winners fan, except for Titanic.</li>
<li>Fred: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.</li>
</ul>


<p>The network learned the following weights:</p>

<pre><code>                 Bias Unit       Hidden 1        Hidden 2
Bias Unit       -0.08257658     -0.19041546      1.57007782 
Harry Potter    -0.82602559     -7.08986885      4.96606654 
Avatar          -1.84023877     -5.18354129      2.27197472 
LOTR 3           3.92321075      2.51720193      4.11061383 
Gladiator        0.10316995      6.74833901     -4.00505343 
Titanic         -0.97646029      3.25474524     -5.59606865 
Glitter         -4.44685751     -2.81563804     -2.91540988
</code></pre>

<p>Note that the first hidden unit seems to correspond to the Oscar winners, and the second hidden unit seems to correspond to the SF/fantasy movies, just as we were hoping.</p>

<p>What happens if we give the RBM a new user, George, who has (Harry Potter = 0, Avatar = 0, LOTR 3 = 0, Gladiator = 1, Titanic = 1, Glitter = 0) as his preferences? It turns the Oscar winners unit on (but not the SF/fantasy unit), correctly guessing that George probably likes movies that are Oscar winners.</p>

<p>What happens if we activate only the SF/fantasy unit, and run the RBM a bunch of different times? In my trials, it turned on Harry Potter, Avatar, and LOTR 3 three times; it turned on Avatar and LOTR 3, but not Harry Potter, once; and it turned on Harry Potter and LOTR 3, but not Avatar, twice. Note that, based on our training examples, these generated preferences do indeed match what we might expect real SF/fantasy fans want to watch.</p>

<h1>Modifications</h1>

<p>I tried to keep the connection-learning algorithm I described above pretty simple, so here are some modifications that often appear in practice:</p>

<ul>
<li>Above, $Negative(e_{ij})$ was determined by taking the product of the $i$th and $j$th units after reconstructing the visible units <em>once</em> and then updating the hidden units again. We could also take the product after some larger number of reconstructions (i.e., repeat updating the visible units, then the hidden units, then the visible units again, and so on); this is slower, but describes the network's daydreams more accurately.</li>
<li>Instead of using $Positive(e<em>{ij})=x_i * x_j$, where $x_i$ and $x_j$ are binary 0 or 1 <em>states</em>, we could also let $x_i$ and/or $x_j$ be activation <em>probabilities</em>. Similarly for $Negative(e</em>{ij})$.</li>
<li>We could penalize larger edge weights, in order to get a sparser or more regularized model.</li>
<li>When updating edge weights, we could use a momentum factor: we would add to each edge a weighted sum of the current step as described above (i.e., $L * (Positive(e<em>{ij}) - Negative(e</em>{ij})$) and the step previously taken.</li>
<li>Instead of using only one training example in each epoch, we could use <em>batches</em> of examples in each epoch, and only update the network's weights after passing through all the examples in the batch. This can speed up the learning by taking advantage of fast matrix-multiplication algorithms.</li>
</ul>


<h1>Further</h1>

<p>If you're interested in learning more about Restricted Boltzmann Machines, here are some good links.</p>

<ul>
<li><a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">A Practical guide to training restricted Boltzmann machines</a>, by Geoffrey Hinton.</li>
<li>A talk by Andrew Ng on <a href="http://www.youtube.com/watch?v=ZmNOAtZIgIk">Unsupervised Feature Learning and Deep Learning</a>.</li>
<li><a href="http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf">Restricted Boltzmann Machines for Collaborative Filtering</a>. I found this paper hard to read, but it's an interesting application to the Netflix Prize.</li>
<li><a href="http://arxiv.org/abs/0908.4425">Geometry of the Restricted Boltzmann Machine</a>. A very readable introduction to RBMs, "starting with the observation that its Zariski closure is a Hadamard power of the first secant variety of the Segre variety of projective lines". (I kid, I kid.)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Choosing a Machine Learning Classifier]]></title>
    <link href="http://echen.github.com/blog/2011/04/27/choosing-a-machine-learning-classifier/"/>
    <updated>2011-04-27T18:43:15-07:00</updated>
    <id>http://echen.github.com/blog/2011/04/27/choosing-a-machine-learning-classifier</id>
    <content type="html"><![CDATA[<p>How do you know what machine learning algorithm to choose for your classification problem? Of course, if you really care about accuracy, your best bet is to test out a couple different ones (making sure to try different parameters within each algorithm as well), and select the best one by cross-validation. But if you're simply looking for a "good enough" algorithm for your problem, or a place to start, here are some general guidelines I've found to work well over the years.</p>

<h1>How large is your training set?</h1>

<p>If your training set is small, high bias/low variance classifiers (e.g., Naive Bayes) have an advantage over low bias/high variance classifiers (e.g., kNN), since the latter will overfit. But low bias/high variance classifiers start to win out as your training set grows (they have lower asymptotic error), since high bias classifiers aren't powerful enough to provide accurate models.</p>

<p>You can also think of this as a generative model vs. discriminative model distinction.</p>

<h1>Advantages of some particular algorithms</h1>

<p><strong>Advantages of Naive Bayes:</strong> Super simple, you're just doing a bunch of counts. If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. And even if the NB assumption doesn't hold, a NB classifier still often does a great job in practice. A good bet if  want something fast and easy that performs pretty well. Its main disadvantage is that it can't learn interactions between features (e.g., it can't learn that although you love movies with Brad Pitt and Tom Cruise, you hate movies where they're together).</p>

<p><strong>Advantages of Logistic Regression:</strong> Lots of ways to regularize your model, and you don't have to worry as much about your features being correlated, like you do in Naive Bayes. You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust classification thresholds, to say when you're unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.</p>

<p><strong>Advantages of Decision Trees:</strong> Easy to interpret and explain (for some people -- I'm not sure I fall into this camp). They easily handle feature interactions and they're non-parametric, so you don't have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end). One disadvantage is that they don't support online learning, so you have to rebuild your tree when new examples come on. Another disadvantage is that they easily overfit, but that's where ensemble methods like random forests (or boosted trees) come in. Plus, random forests are often the winner for lots of problems in classification (usually slightly ahead of SVMs, I believe), they're fast and scalable, and you don't have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days.</p>

<p><strong>Advantages of SVMs:</strong> High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you're data isn't linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive, hard to interpret, and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.</p>

<h1>But...</h1>

<p>Recall, though, that better data often beats better algorithms, and designing good features goes a long way. And if you have a huge dataset, then whichever classification algorithm you use might not matter so much in terms of classification performance (so choose your algorithm based on speed or ease of use instead).</p>

<p>And to reiterate what I said above, if you really care about accuracy, you should definitely try a bunch of different classifiers and select the best one by cross-validation. Or, to take a lesson from the Netflix Prize (and Middle Earth), just use an ensemble method to choose them all.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Mathematical Introduction to Least Angle Regression]]></title>
    <link href="http://echen.github.com/blog/2011/04/21/a-mathematical-introduction-to-least-angle-regression/"/>
    <updated>2011-04-21T00:16:36-07:00</updated>
    <id>http://echen.github.com/blog/2011/04/21/a-mathematical-introduction-to-least-angle-regression</id>
    <content type="html"><![CDATA[<p>(For a layman's introduction, see <a href="/2011/03/14/least-angle-regression-for-the-hungry-layman/">here</a>.)</p>

<p>Least Angle Regression (aka LARS) is a <strong>model selection method</strong> for linear regression (when you're worried about overfitting or want your model to be easily interpretable). To motivate it, let's consider some other model selection methods:</p>

<ul>
<li><p><strong>Forward selection</strong> starts with no variables in the model, and at each step it adds to the model the variable with the most explanatory power, stopping if the explanatory power falls below some threshold. This is a fast and simple method, but it can also be too greedy: we fully add variables at each step, so correlated predictors don't get much of a chance to be included in the model. (For example, suppose we want to build a model for the deliciousness of a PB&amp;J sandwich, and two of our variables are the amount of peanut butter and the amount of jelly. We'd like both variables to appear in our model, but since amount of peanut butter is (let's assume) strongly correlated with the amount of jelly, once we fully add peanut butter to our model, jelly doesn't add much explanatory power anymore, and so it's unlikely to be added.)</p></li>
<li><p><strong>Forward stagewise regression</strong> tries to remedy the greediness of forward selection by only partially adding variables. Whereas forward selection finds the variable with the most explanatory power and goes all out in adding it to the model, forward stagewise finds the variable with the most explanatory power and updates its weight by only epsilon in the correct direction. (So we might first increase the weight of peanut butter a little bit, then increase the weight of peanut butter again, then increase the weight of jelly, then increase the weight of bread, and then increase the weight of peanut butter once more.) The problem now is that we have to make a ton of updates, so forward stagewise can be very inefficient.</p></li>
</ul>


<p>LARS, then, is essentially forward stagewise made fast. Instead of making tiny hops in the direction of one variable at a time, LARS makes optimally-sized leaps in optimal directions. These directions are chosen to make equal angles (equal correlations) with each of the variables currently in our model. (We like peanut butter best, so we start eating it first; as we eat more, we get a little sick of it, so jelly starts looking equally appetizing, and we start eating peanut butter and jelly simultaneously; later, we add bread to the mix, etc.)</p>

<p>In more detail, LARS works as follows:</p>

<ul>
<li><p>Assume for simplicity that we've standardized our explanatory variables to have zero mean and unit variance, and that our response variable also has zero mean.</p></li>
<li><p>Start with no variables in your model.</p></li>
<li><p>Find the variable $latex x _ 1 $ most correlated with the residual. (Note that the variable most correlated with the residual is equivalently the one that makes the least angle with the residual, whence the name.)</p></li>
<li><p>Move in the direction of this variable until some other variable $latex x _ 2 $ is just as correlated.</p></li>
<li><p>At this point, start moving in a direction such that the residual stays equally correlated with $latex x _ 1 $ and $latex x _ 2 $ (i.e., so that the residual makes equal angles with both variables), and keep moving until some variable $latex x _ 3 $ becomes equally correlated with our residual.</p></li>
<li><p>And so on, stopping when we've decided our model is big enough.</p></li>
</ul>


<p>For example, consider the following image (slightly simplified from the <a href="http://www.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">original LARS paper</a>; $latex x _ 1, x _ 2$ are our variables, and $latex y$ is our response):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/lars/lars-example.png"><img src="http://dl.dropbox.com/u/10506/blog/lars/lars-example.png" alt="LARS Example" /></a></p>

<p>Our model starts at $latex \hat{\mu _ 0} $.</p>

<ul>
<li><p>The residual (the green line) makes a smaller angle with $latex x _ 1 $ than with $latex x _ 2 $, so we start moving in the direction of $latex x _ 1 $.
At $latex \hat{\mu _ 1} $, the residual now makes equal angles with $latex x _ 1, x _ 2 $, and so we start moving in a new direction that preserves this equiangularity/equicorrelation.</p></li>
<li><p>If there were more variables, we'd change directions again once a new variable made equal angles with our residual, and so on.</p></li>
</ul>


<p>So when should you use LARS, as opposed to some other regularization method like lasso? There's not really a clear-cut answer, but LARS tends to give very similar results as both lasso and forward stagewise (in fact, slight modifications to LARS give you lasso and forward stagewise), so I tend to just use lasso when I do these kinds of things, since the justifications for lasso make a little more sense to me. In fact, I don't usually even think of LARS as a model selection method in its own right, but rather as a way to efficiently implement lasso (especially if you want to compute the full regularization path).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Cointegration and Pairs Trading]]></title>
    <link href="http://echen.github.com/blog/2011/04/16/what-is-cointegration-2/"/>
    <updated>2011-04-16T03:44:14-07:00</updated>
    <id>http://echen.github.com/blog/2011/04/16/what-is-cointegration-2</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>Suppose you see two drunks (i.e., two random walks) wandering around. The drunks don't know each other (they're independent), so there's no meaningful relationship between their paths.</p>

<p>But suppose instead you have a drunk walking with her dog. This time there <em>is</em> a connection. What's the nature of this connection? Notice that although each path individually is still an unpredictable random walk, given the location of one of the drunk or dog, we have a pretty good idea of where the other is; that is, the distance between the two is fairly predictable. (For example, if the dog wanders too far away from his owner, she'll tend to move in his direction to avoid losing him, so the two stay close together despite a tendency to wander around on their own.) We describe this relationship by saying that the drunk and her dog form a cointegrating pair.</p>

<p>In more technical terms, if we have two non-stationary time series X and Y that become stationary when differenced (these are called integrated of order one series, or I(1) series; random walks are one example) such that some linear combination of X and Y is stationary (aka, I(0)), then we say that X and Y are cointegrated. In other words, while neither X nor Y alone hovers around a constant value, some combination of them does, so we can think of cointegration as describing a particular kind of long-run equilibrium relationship. (The definition of cointegration can be extended to multiple time series, with higher orders of integration.)</p>

<p>Other examples of cointegrated pairs:</p>

<ul>
<li><p>Income and consumption: as income increases/decreases, so too does consumption.</p></li>
<li><p>Size of police force and amount of criminal activity</p></li>
<li><p>A book and its movie adaptation: while the book and the movie may differ in small details, the overall plot will remain the same.</p></li>
<li><p>Number of patients entering or leaving a hospital</p></li>
</ul>


<h1>An application</h1>

<p>So why do we care about cointegration? In quantitative finance, cointegration forms the basis of the pairs trading strategy: suppose we have two cointegrated stocks X and Y, with the particular (for concreteness) cointegrating relationship X - 2Y = Z, where Z is a stationary series of zero mean. For example, X could be McDonald's, Y could be Burger King, and the cointegration relationship would mean that X tends to be priced twice as high as Y, so that when X is more than twice the price of Y, we expect X to move down or Y to move up in the near future (and analogously, if X is less than twice the price of Y, we expect X to move up or Y to move down). This suggests the following trading strategy: if X - 2Y > d, for some positive threshold d, then we should sell X and buy Y (since we expect X to decrease in price and Y to increase), and similarly, if X - 2Y &lt; -d, then we should buy X and sell Y.</p>

<h1>Spurious regression</h1>

<p>But why do we need the notion of cointegration at all? Why can't we simply use, say, the R-squared between X or Y to see if X and Y have some kind of relationship? The reason is that standard regression analysis fails when dealing with non-stationary variables, leading to spurious regressions that suggest relationships even when there are none.</p>

<p>For example, suppose we regress two independent random walks against each other, and test for a linear relationship. A large percentage of the time, we'll find high R-squared values and low p-values when using standard OLS statistics, even though there's absolutely no relationship between the two random walks. As an illustration, here I simulated 1000 pairs of random walks of length 100, and found p-values less than 0.05 in 77% of the cases:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/cointegration/spurious-regression.png"><img src="http://dl.dropbox.com/u/10506/blog/cointegration/spurious-regression.png" alt="Spurious Regression" /></a></p>

<h1>A Cointegration Test</h1>

<p>So how do you detect cointegration? There are several different methods, but the simplest is the Engle-Granger test, which works roughly as follows:</p>

<ul>
<li><p>Check that $latex X _ t $ and $latex Y _ t $ are both I(1).</p></li>
<li><p>Estimate the cointegrating relationship $latex Y _ t = aX _ t + e _ t $ by ordinary least squares.</p></li>
<li><p>Check that the cointegrating residuals $latex e _ t $ are stationary (say, by using a so-called unit root test, e.g., the Dickey-Fuller test).</p></li>
</ul>


<h1>Error-correction and Granger representation</h1>

<p>Something else that should perhaps be mentioned is the relationship between cointegration and error-correction mechanisms: suppose we have two cointegrated series $latex X _ t, Y _ t $, with autoregressive representations</p>

<p>$latex X _ t = a X _ {t-1} + b Y _ {t-1} + u _ t $
$latex Y _ t = c X _ {t-1} + d Y _ {t-1} + v _ t $</p>

<p>By the Granger representation theorem (which is actually a bit more general than this), we then have</p>

<p>$latex \Delta X _ t = \alpha _ 1 (Y _ {t-1} - \beta X _ {t-1}) + u _ t $
$latex \Delta Y _ t = \alpha _ 2 (Y _ {t-1} - \beta X _ {t-1}) + v _ t $</p>

<p>where $latex Y _ {t-1} - \beta X _ {t-1} \sim I(0) $ is the cointegrating relationship. Regarding $latex Y _ {t-1} - \beta X _ {t-1} $ as the extent of disequilibrium from the long-run relationship, and the $latex \alpha _ i $ as the speed (and direction) at which the time series correct themselves from this disequilibrium, we can see that this formalizes the way cointegrated variables adjust to match their long-run equilbrium.</p>

<h1>Summary</h1>

<p>So, just to summarize a bit, cointegration is an equilibrium relationship between time series that individually aren't in equilbrium (you can kind of contrast this with (Pearson) correlation, which describes a linear relationship), and it's useful because it allows us to incorporate both short-term dynamics (deviations from equilibrium) and long-run expectations (corrections to equilibrium).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Monads and Typeclasses in Haskell]]></title>
    <link href="http://echen.github.com/blog/2011/04/16/monads-typeclasses-and-celebrities/"/>
    <updated>2011-04-16T03:40:23-07:00</updated>
    <id>http://echen.github.com/blog/2011/04/16/monads-typeclasses-and-celebrities</id>
    <content type="html"><![CDATA[<p>My favorite problems are the ones that you can turn into a mini-lesson on something new. I realized the following problem could make a nice introduction to the Maybe monad and typeclasses in Haskell, so I'm going to try lessonizing it here.</p>

<h1>Finding Celebrities</h1>

<p>The problem is this: suppose you have a group of people, and you want to find a "celebrity" among them, where a celebrity is defined as a person P such that everybody else knows P, but P knows no one. (Note that, by the definition of celebrity, there can be at most one celebrity in a group.) So you're given a group of people, a black box oracle that tells you whether person X knows person Y, and you want to efficiently find a celebrity in the group (if such a celebrity exists).</p>

<p>The solution I came up with is the following two-step algorithm:</p>

<ul>
<li><p>Step 1 finds a candidate celebrity. We keep a temporary candidate C and walk through the group: if C knows the current person P under our pointer, then C can't be a celebrity, so we change the temporary candidate to be P and move on; otherwise, if C does not know the current person P, then we keep C as our candidate and move to the next person on the list.</p></li>
<li><p>Step 2 checks that our candidate celebrity C is actually a celebrity. We walk through the list of people, and check that for every other person P, P knows C but C does not know P.</p></li>
</ul>


<p>Let's code this up now in Haskell. Since the algorithm has two steps, let's code up each step individually.</p>

<pre><code>-- Step 1
findCandidate :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Maybe Person
findCandidate [] _ = Nothing
findCandidate (x:[]) _ = Just x
findCandidate (x:y:rest) knows =
    if x `knows` y
        then findCandidate (y:rest) knows
        else findCandidate (x:rest) knows

-- Step2
checkCandidate :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Person -&gt; Maybe Person
checkCandidate [] _ candidate = Just candidate
checkCandidate (x:xs) knows candidate
    | x == candidate = checkCandidate xs knows candidate
    | otherwise      =
        if (x `knows` candidate) &amp;&amp; (not $ candidate `knows` x)
            then checkCandidate xs knows candidate
            else Nothing
</code></pre>

<p>Now how do we combine the first two steps? A first approach might be the following:</p>

<pre><code>badFindCelebrity :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Maybe Person
badFindCelebrity xs knows = checkCandidate xs knows (findCandidate xs knows)
</code></pre>

<p>But this doesn't work! There's a <em>type mismatch</em> in our definition: <code>checkCandidate</code> takes a <code>Person</code> in its last argument, but <code>findCandidate xs knows</code> returns something of type <code>Maybe Person</code>.</p>

<p>How do we fix this? One approach would be to do some checking of our candidate celebrity: if it's <code>Nothing</code>, then don't even bother passing it to <code>checkCandidate</code> and just fail immediately; otherwise, extract the person from its <code>Just x</code> wrapper, and pass this extraction to <code>checkCandidate</code>. This works as follows:</p>

<pre><code>messyFindCelebrity :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Maybe Person
messyFindCelebrity xs knows =
    case findCandidate xs knows of
        Nothing -&gt; Nothing
        Just x  -&gt; checkCandidate xs knows x
</code></pre>

<p>This isn't too bad, but what if we later want to add some additional checks? For example, instead of finding any old celebrity, we might want to find only celebrities that have recently starred in a movie, in which case we'd have to modify our algorithm to:</p>

<pre><code>recentMovie :: Person -&gt; Maybe Person
recentMovie x = ...

messyFindCelebrity' :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Maybe Person
messyFindCelebrity' xs knows =
    case findCandidate xs knows of
        Nothing -&gt; Nothing
        Just x  -&gt; case checkCandidate xs knows x of
        Nothing -&gt; Nothing
        Just y  -&gt; recentMovie y
</code></pre>

<h1>Maybe Monad</h1>

<p>Enter the Maybe monad. Instead of forcing us to make these <code>Nothing</code> checks and <code>Just x</code> extractions ourselves, the Maybe monad has a "chaining" operator <code>&gt;&gt;=</code> that performs checks and extractions automatically. That is, <code>Nothing &gt;&gt;= f</code> is always <code>Nothing</code>, and <code>Just x &gt;&gt;= f</code> returns <code>f x</code>, so we can rewrite our <code>findCelebrity</code> method as:</p>

<pre><code>findCelebrity :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Maybe Person
findCelebrity xs knows =
    findCandidate xs knows &gt;&gt;= checkCandidate xs knows
</code></pre>

<p>Or, in the sugared <code>do</code> syntax:</p>

<pre><code>findCelebrity :: [Person] -&gt; (Person -&gt; Person -&gt; Bool) -&gt; Maybe Person
findCelebrity xs knows = do
    candidate &lt;- findCandidate xs knows
    celebrity &lt;- checkCandidate xs knows candidate
    return celebrity
</code></pre>

<p>So that's the Maybe monad.</p>

<h1>Typeclasses</h1>

<p>Now let's see if we can generalize our solution a bit. In our algorithm above, we didn't really make use of the fact that we were dealing with <code>Person</code> data types. The only assumption we made was that we could test whether two Persons were equal, in the <code>x == candidate</code> check of <code>checkCandidate</code>*. Thus, if we replace the <code>Person</code> data type in all our type signatures with a data type that supports equality testing, our function will be a bit more general and work just as well.</p>

<p>To do this, we write</p>

<pre><code>findCandidate :: (Eq a) =&gt; [a] -&gt; (a -&gt; a -&gt; Bool) -&gt; Maybe a
checkCandidate :: (Eq a) =&gt; [a] -&gt; (a -&gt; a -&gt; Bool) -&gt; a -&gt; Maybe a
findCelebrity :: (Eq a) =&gt; [a] -&gt; (a -&gt; a -&gt; Bool) -&gt; Maybe a
</code></pre>

<p><code>Eq a</code> simply means that <code>a</code> is a type that supports equality testing, i.e., given any two objects <code>x</code> and <code>y</code> of type <code>a</code>, writing <code>x == y</code> works. So now our <code>findCelebrity</code> function can not only find celebrities in a group of Persons, but also find things like the majority or minority in a group of integers (if we use <code>(&lt;)</code> or <code>(&gt;)</code> as our oracle), and sources or sinks in a graph.</p>

<p>*Note that we can actually get rid of this equality testing assumption, by making <code>findCandidate</code> return a list of rejected persons (in addition to the candidate celebrity) that we check against in <code>checkCandidate</code>.</p>

<h1>An evasive segue</h1>

<p>So there's an introduction to the Maybe monad and typeclasses (that maybe you could have discovered yourself!). For just one more quick intro, note that the celebrity question provides a counterexample to the <a href="http://blog.echen.me/2011/03/14/topological-combinatorics-and-the-evasiveness-conjecture">Evasiveness Conjecture</a> on <em>non-monotone</em> graphs, showing why the monotonicity assumption in the conjecture is indeed necessary.</p>
]]></content>
  </entry>
  
</feed>
