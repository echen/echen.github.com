<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: projects | Edwin Chen's Blog]]></title>
  <link href="http://blog.echen.me/categories/projects/atom.xml" rel="self"/>
  <link href="http://blog.echen.me/"/>
  <updated>2013-03-31T16:41:11-07:00</updated>
  <id>http://blog.echen.me/</id>
  <author>
    <name><![CDATA[Edwin Chen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Topic Modeling the Sarah Palin Emails]]></title>
    <link href="http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails/"/>
    <updated>2011-06-27T17:19:42-07:00</updated>
    <id>http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails</id>
    <content type="html"><![CDATA[<h1>LDA-based Email Browser</h1>

<p>Earlier this month, several thousand emails from Sarah Palin's time as governor of Alaska were <a href="http://sunlightlabs.com/blog/2011/sarahs-inbox/">released</a>. The emails weren't organized in any fashion, though, so to make them easier to browse, I've been working on some topic modeling (in particular, using latent Dirichlet allocation) to separate the documents into different groups.</p>

<p>I threw up <a href="http://sarah-palin.heroku.com/">a simple demo app</a> to view the organized documents <a href="http://sarah-palin.heroku.com/">here</a>.</p>

<h1>What is Latent Dirichlet Allocation?</h1>

<p>Briefly, given a set of documents, LDA tries to learn the latent topics underlying the set. It represents each document as a mixture of topics (generated from a Dirichlet distribution), each of which emits words with a certain probability.</p>

<p>For example, given the sentence "I listened to Justin Bieber and Lady Gaga on the radio while driving around in my car", an LDA model might represent this sentence as 75% about music (a topic which, say, emits the words <em>Bieber</em> with 10% probability, <em>Gaga</em> with 5% probability, <em>radio</em> with 1% probability, and so on) and 25% about cars (which might emit <em>driving</em> with 15% probability and <em>cars</em> with 10% probability).</p>

<p>If you're familiar with latent semantic analysis, you can think of LDA as a generative version. (For a more in-depth explanation, I wrote an introduction to LDA <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">here</a>.)</p>

<h1>Sarah Palin Email Topics</h1>

<p>Here's a sample of the topics learnt by the model, as well as the top words for each topic. (Names, of course, are based on my own interpretation.)</p>

<ul>
<li><a href="http://sarah-palin.heroku.com/topics/24"><strong>Wildlife/BP Corrosion</strong></a>: game, fish, moose, wildlife, hunting, bears, polar, bear, subsistence, management, area, board, hunt, wolves, control, department, year, use, wolf, habitat, hunters, caribou, program, denby, fishing, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/0"><strong>Energy/Fuel/Oil/Mining</strong></a>: energy, fuel, costs, oil, alaskans, prices, cost, nome, now, high, being, home, public, power, mine, crisis, price, resource, need, community, fairbanks, rebate, use, mining, villages, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/19"><strong>Trig/Family/Inspiration</strong></a>: family, web, mail, god, son, from, congratulations, children, life, child, down, trig, baby, birth, love, you, syndrome, very, special, bless, old, husband, years, thank, best, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/6"><strong>Gas</strong></a>: gas, oil, pipeline, agia, project, natural, north, producers, companies, tax, company, energy, development, slope, production, resources, line, gasline, transcanada, said, billion, plan, administration, million, industry, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/12"><strong>Education/Waste</strong></a>: school, waste, education, students, schools, million, read, email, market, policy, student, year, high, news, states, program, first, report, business, management, bulletin, information, reports, 2008, quarter, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/15"><strong>Presidential Campaign/Elections</strong></a>: mail, web, from, thank, you, box, mccain, sarah, very, good, great, john, hope, president, sincerely, wasilla, work, keep, make, add, family, republican, support, doing, p.o, …</li>
</ul>


<p>Here's a sample email from the wildlife topic:</p>

<p><a href="http://sarah-palin.heroku.com/emails/6719"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-email.png" alt="Wildlife Email" /></a></p>

<p>I also thought the classification for <a href="http://sarah-palin.heroku.com/emails/12900">this email</a> was really neat: the LDA model labeled it as 10% in the <a href="http://sarah-palin.heroku.com/topics/15">Presidential Campaign/Elections</a> topic and 90% in the <a href="http://sarah-palin.heroku.com/topics/24">Wildlife</a> topic, and it's precisely a wildlife-based protest against Palin as a choice for VP:</p>

<p><a href="http://sarah-palin.heroku.com/emails/12900"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-vp.png" alt="Wildlife-VP Protest" /></a></p>

<h1>Future Analysis</h1>

<p>In a future post, I'll perhaps see if we can glean any interesting patterns from the email topics. For example, for a quick graph now, if we look at the percentage of emails in the <a href="http://sarah-palin.heroku.com/topics/19">Trig/Family/Inspiration topic</a> across time, we see that there's a spike in April 2008 -- exactly (and unsurprisingly) the month in which Trig was born.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-topic.png"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-topic.png" alt="Trig" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Filtering for English Tweets: Unsupervised Language Detection on Twitter]]></title>
    <link href="http://blog.echen.me/2011/05/01/unsupervised-language-detection-algorithms/"/>
    <updated>2011-05-01T16:28:07-07:00</updated>
    <id>http://blog.echen.me/2011/05/01/unsupervised-language-detection-algorithms</id>
    <content type="html"><![CDATA[<p>(See a demo <a href="http://babel-fett.heroku.com/">here</a>.)</p>

<p>While working on a Twitter sentiment analysis project, I ran into the problem of needing to filter out all non-English tweets. (Asking the Twitter API for English-only tweets doesn't seem to work, as it nonetheless returns tweets in Spanish, Portuguese, Dutch, Russian, and a couple other languages.)</p>

<p>Since I didn't have any labeled data, I thought it would be fun to build an <strong>unsupervised</strong> language classifier. In particular, using an EM algorithm to build a naive Bayes model of English vs. non-English n-gram probabilities turned out to work quite well, so here's a description.</p>

<h1>EM Algorithm</h1>

<p>Let's recall the naive Bayes algorithm: given a tweet (a set of <em>character</em> n-grams), we estimate its language to be the language $L$ that maximizes</p>

<p>$$P(language = L | ngrams) \propto P(ngrams | language = L) P(language = L)$$</p>

<p>Thus, we need to estimate $P(ngram | language = L)$ and $P(language = L)$.</p>

<p>This would be easy <strong>if we knew the language of each tweet</strong>, since we could estimate</p>

<ul>
<li>$P(xyz| language = English)$ as #(number of times "xyz" is a trigram in the English tweets) / #(total trigrams in the English tweets)</li>
<li>$P(language = English)$ as the proportion of English tweets.</li>
</ul>


<p>Or, it would also be easy <strong>if we knew the n-gram probabilities for each language</strong>, since we could use Bayes' theorem to compute the language <em>probabilities</em> for each tweet, and then take a weighted variant of the previous paragraph.</p>

<p><strong>The problem is that we know neither of these.</strong> So what the EM algorithm says is that that we can simply <strong>guess</strong>:</p>

<ul>
<li>Pretend we know the language of each tweet (by randomly assigning them at the beginning).</li>
<li>Using this guess, we can compute the n-gram probabilities for each language.</li>
<li>Using the n-gram probabilities for each language, we can recompute the language probabilities of each tweet.</li>
<li>Using these recomputed language probabilities, we can recompute the n-gram probabilities.</li>
<li>And so on, recomputing the language probabilities and n-gram probabilities over and over. While our guesses will be off in the beginning, the probabilities will eventually converge to (locally) minimize the likelihood. (In my tests, my language detector would sometimes correctly converge to an English detector, and sometimes it would converge to an English-and-Dutch detector.)</li>
</ul>


<h2>EM Analogy for the Layman</h2>

<p>Why does this work? Suppose you suddenly move to New York, and you want a way to differentiate between tourists and New Yorkers based on their activities. Initially, you don't know who's a tourist and who's a New Yorker, and you don't know which are touristy activities and which are not. So you randomly place people into two groups A and B. (You randomly assign all tweets to a language)</p>

<p>Now, given all the people in group A, you notice that a large number of them visit the Statue of Liberty; similarly, you notice that a large number of people in group B walk really quickly. (You notice that one set of words often has the n-gram "ing", and that another set of words often has the n-gram "ias"; that is, you fix the language probabilities for each tweet, and recompute the n-gram probabilities for each language.)</p>

<p>So you start to put people visiting the Statue of Liberty in group A, and you start to put fast walkers in group B. (You fix the n-gram probabilities for each language, and recompute the language probabilities for each tweet.)</p>

<p>With your new A and B groups, you notice more differentiating factors: group A people tend to carry along cameras, and group B people tend to be more finance-savvy.</p>

<p>So you start to put camera-carrying folks in group A, and finance-savvy folks in group B.</p>

<p>And so on. Eventually, you settle on two groups of people and differentiating activities: people who walk slowly and visit the Statue of Liberty, and busy-looking people who walk fast and don't visit. Assuming there are more native New Yorkers than tourists, you can then guess that the natives are the larger group.</p>

<h1>Results</h1>

<p>I wrote some Ruby code to implement the above algorithm, and trained it on half a million tweets, using English and "not English" as my two languages. The results looked surprisingly good from just eyeballing:</p>

<p><a href="https://img.skitch.com/20110303-qfrnb8gstgheh4xech4iutfskd.jpg"><img src="https://img.skitch.com/20110303-qfrnb8gstgheh4xech4iutfskd.jpg" alt="Example Results" /></a></p>

<p>But in order to get some hard metrics and to tune parameters (e.g., n-gram size), I needed a labeled dataset. So I pulled a set of English-language and Spanish-language documents from Project Gutenberg, and split them to form training and test sets (the training set consisted of 2000 lines of English and 1000 lines of Spanish, and  1000 lines of English and 1000 lines of Spanish for the test set).</p>

<p>Trained on bigrams, the detector resulted in:</p>

<ul>
<li>991 true positives (English lines correctly classified as English)</li>
<li>9 false negatives (English lines incorrectly classified as Spanish</li>
<li>11 false positives (Spanish lines incorrectly classified as English)</li>
<li>989 true negatives (Spanish lines correctly classified as English)</li>
</ul>


<p>for a precision of 0.989 and a recall of 0.991.</p>

<p>Trained on trigrams, the detector resulted in:</p>

<ul>
<li>992 true positives</li>
<li>8 false negatives</li>
<li>10 false positives</li>
<li>990 true negatives</li>
</ul>


<p>for a precision of 0.990 and a recall of 0.992.</p>

<p>Also, when I looked at the sentences the detector was making errors on, I saw that they almost always consisted of only one or two words (e.g., the incorrectly classified sentences were lines like "inmortal", "autumn", and "salir"). So the detector pretty much never made a mistake on a normal sentence!</p>

<h1>Code/Demo</h1>

<p>I put the code on <a href="https://github.com/echen/unsupervised-language-identification">my Github account</a>, and a quick <a href="http://babel-fett.heroku.com/">demo app</a>, trained on trigrams from tweets with lang="en" according to the Twitter API, is <a href="http://babel-fett.heroku.com/">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kickstarter Data Analysis: Success and Pricing]]></title>
    <link href="http://blog.echen.me/2011/04/25/kickstarter-data-analysis-success-and-pricing/"/>
    <updated>2011-04-25T21:19:40-07:00</updated>
    <id>http://blog.echen.me/2011/04/25/kickstarter-data-analysis-success-and-pricing</id>
    <content type="html"><![CDATA[<p><a href="http://www.kickstarter.com/">Kickstarter</a> is an online crowdfunding platform for launching creative projects. When starting a new project, project owners specify a deadline and the minimum amount of money they need to raise. They receive the money (less a transaction fee) only if they reach or exceed that minimum; otherwise, no money changes hands.</p>

<p>What's particularly fun about Kickstarter is that in contrast to <a href="http://www.kiva.org/">that other microfinance site</a>, Kickstarter projects don't ask for loans; instead, patrons receive pre-specified rewards unique to each project. For example, someone donating money to help an artist record an album might receive a digital copy of the album if they donate 20 dollars, or a digital copy plus a signed physical cd if they donate 50 dollars.</p>

<p>There are <a href="http://www.kickstarter.com/discover/hall-of-fame?ref=sidebar">a bunch</a> of <a href="http://www.kickstarter.com/projects/1104350651/tiktok-lunatik-multi-touch-watch-kits">neat</a> <a href="http://www.kickstarter.com/projects/2024077040/neil-gaimans-the-price">projects</a>, and I'm tempted to put one of my own on there soon, so I thought it would be fun to gather some data from the site and see what makes a project successful.</p>

<h1>Categories</h1>

<p>I started by scraping the categories section.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-projects-by-category.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-projects-by-category.png" alt="Successful projects by category" /></a></p>

<p>In true indie fashion, the artsy categories tend to dominate. (I'm surprised/disappointed how little love the Technology category gets.)</p>

<h1>Ending Soon</h1>

<p>The categories section really only provides a history of <em>successful</em> projects, though, so to get some data on unsuccessful projects as well, I took a look at the <a href="http://www.kickstarter.com/discover/ending-soon?ref=sidebar">Ending Soon</a> section of projects whose deadlines are about to pass.</p>

<p>It looks like about 50% of all Kickstarter projects get successfully funded by the deadline:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/ending-soon-success.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/ending-soon-success.png" alt="Successful projects as deadline approaches" /></a></p>

<p>Interestingly, most of the final funding seems to happen in the final few days: with just 5 days left, only about 20% of all projects have been funded. (In other words, with just 5 days left, 60% of the projects that will eventually be successful are still unfunded.) So the approaching deadline seems to really spur people to donate. I wonder if it's because of increased publicity in the final few days (the project owners begging everyone for help!) or if it's simply procrastination in action (perhaps people want to wait to see if their donation is really necessary)?</p>

<p>Lesson: if you're still not fully funded with only a couple days remaining, don't despair.</p>

<h1>Success vs. Failure</h1>

<p>What factors lead a project to succeed? Are there any quantitative differences between projects that eventually get funded and those that don't?</p>

<p>Two simple (if kind of obvious) things I noticed are that unsuccessful projects tend to require a larger amount of money:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-goal.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-goal.png" alt="Unsuccessful projects tend to ask for more money" /></a></p>

<p>and unsuccessful projects also tend to raise less money in absolute terms (i.e., it's not just that they ask for too much money to reach their goal -- they're simply not receiving enough money as well):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-amount-pledged.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/successful-vs-unsuccessful-amount-pledged.png" alt="Unsuccessful projects received less money" /></a></p>

<p>Not terribly surprising, but it's good to confirm (and I'm still working on finding other predictors).</p>

<h1>Pledge Rewards</h1>

<p>There's a lot of interesting work in behavioral economics on pricing and choice -- for example, the <a href="http://youarenotsosmart.com/2010/07/27/anchoring-effect/">anchoring effect</a> suggests that when building a menu, you should <a href="http://www.neurosciencemarketing.com/blog/articles/neuro-menus-and-restaurant-psychology.htm">include an expensive item</a> to make other menu items look reasonably priced in comparison, and the <a href="http://en.wikipedia.org/wiki/The_Paradox_of_Choice:_Why_More_Is_Less">paradox of choice </a> suggests that too many choices lead to a decision freeze -- so one aspect of the Kickstarter data I was especially interested in was how pricing of rewards affects donations. For example, does pricing the lowest reward at 25 dollars lead to more money donated (people don't lowball at 5 dollars instead) or less money donated (25 dollars is more money than most people are willing to give)? And what happens if a new reward at 5 dollars is added -- again, does it lead to more money (now people can donate something they can afford) or less money (the people that would have paid 25 dollars switch to a 5 dollar donation)?</p>

<p>First, here's a look at the total number of pledges at each price. (More accurately, it's the number of claimed rewards at each price.) [Update: the original version of this graph was wrong, but I've since fixed it.]</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/pledge%20amounts.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/pledge%20amounts.png" alt="Pledge Amounts" /></a></p>

<p>Surprisingly, 5 dollar and 1 dollar donations are actually not the most common contribution.</p>

<p>To investigate pricing effects, I started by looking at all (successful) projects that had a reward priced at 1 dollar, and compared the number of donations at 1 dollar with the number of donations at the next lowest reward.</p>

<p>Up to about 15-20 dollars, there's a steady increase in the proportion of people who choose the second reward over the first reward, but after that, the proportion decreases.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring.png" alt="Anchoring" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring-abline-b.png"><img src="http://dl.dropbox.com/u/10506/blog/kickstarter/anchoring-abline-b.png" alt="Anchoring with Regression Lines" /></a></p>

<p>So this perhaps suggests that if you're going to price your lowest reward at 1 dollar, your next reward should cost roughly 20 dollars (or slightly more, to maximize your total revenue). Pricing above 20 dollars is a little too expensive for the folks who want to support you, but aren't rich enough to throw gads of money; maybe rewards below 20 dollars aren't good enough to merit the higher donation.</p>

<p>Next, I'm planning on digging a little deeper into pricing effects and what makes a project successful, so I'll hopefully have some more Kickstarter analysis in a future post. In the meantime, in case anyone else wants to take a look, I put the data onto <a href="https://github.com/echen/kickstarter-data-analysis">my Github account</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hacker News Analysis]]></title>
    <link href="http://blog.echen.me/2011/03/14/hacker-news-analysis/"/>
    <updated>2011-03-14T04:27:32-07:00</updated>
    <id>http://blog.echen.me/2011/03/14/hacker-news-analysis</id>
    <content type="html"><![CDATA[<p>I was playing around with the <a href="http://api.ihackernews.com/?db">Hacker News database</a> <a href="http://ronnieroller.com/">Ronnie Roller</a> made (thanks!), so I thought I'd post some of the things I looked at.</p>

<h1>Activity on the Site</h1>

<p>My first question was how activity on the site has increased over time. I looked at number of posts, points on posts, comments on posts, and number of users.</p>

<h2>Posts</h2>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/posts_by_month.png" alt="Hacker News Posts by Month" /></p>

<p>This looks like a strong linear fit, with an increase of 292 posts every month.</p>

<h2>Comments</h2>

<p>For comments, I fit a quadratic regression:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/comments_by_month.png" alt="Hacker News Comments by Month" /></p>

<h2>Points</h2>

<p>A quadratic regression was also a better fit for points by month:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/points_by_month.png" alt="Hacker News Points by Month" /></p>

<h2>Users</h2>

<p>And again for the number of distinct users with a submission:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/users.png" alt="Hacker News Users by Month" /></p>

<h1>Points and Comments</h1>

<p>My next question was how points and comments related. Intuitively, posts with more points should have more comments, but it's nice to check (maybe really good posts are kind of boring, so don't lead to much discussion).</p>

<p>First, I plotted the points and comments of each individual post:</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/all_points_vs_comments.png" alt="All Points vs. Comments" /></p>

<p>As expected, there’s an overall positive correlation between points and comments. Interestingly, there are quite a few high-points posts with no comments.</p>

<p>The plot’s quite noisy, though, so let’s try cleaning it up a bit, by taking the median number of comments per points level (and removing posts at the higher end, where we have little data):</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/points_vs_median_comments.png" alt="Points vs. Median Comments" /></p>

<p>We see that posts with more points do tend to have more comments. Also, variance in number of comments is indicated by size and color, so (unsurprisingly) posts with more points have larger variance in their number of comments.</p>

<h1>Quality of Posts</h1>

<p>Another question was whether the quality of posts has degraded over time.</p>

<p>First, I computed a normalized "score" for each post, where a post's score is defined as the number of points divided by the number of distinct users who made a submission in the same month. (The denominator is a rough proxy for the number of active users, and the goal of the score is to provide a way to compare posts across time.)</p>

<p>While the median score has declined over time (as perhaps should be expected, since only a fixed number of items can reach the front page):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/hn-analysis/median-score.png"><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/median-score.png" alt="Median Score" /></a></p>

<p>the absolute <em>number</em> of quality posts, defined as posts with a score greater than the (admittedly arbitrarily chosen) threshold 0.01, has increased (until possibly a dip starting in 2010):</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/num_quality_posts2.png" alt="Number of Quality Posts" /></p>

<p>(Of course, without some further analysis, it's not clear how well this score measures quality of posts, so take these numbers with a grain of salt.)</p>

<h1>Company Trends</h1>

<p>Also, I wanted to see how certain topics have trended over time, so I looked at how mentions of some of the big-name companies (Google, Facebook, Microsoft, Yahoo, Twitter, Apple) have changed. For each company, I plotted the percentage of posts with the company's name in the title, and also made a smoothed plot comparing all six at the end. Note that Microsoft and Yahoo seem to be trending slightly downward, and Apple seems to be trending upward.</p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_microsoft.png" alt="Mentions of Microsoft" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_yahoo.png" alt="Mentions of Yahoo" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_google.png" alt="Mentions of Google" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_facebook.png" alt="Mentions of Facebook" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_twitter.png" alt="Mentions of Twitter" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/pct_apple.png" alt="Mentions of Apple" /></p>

<p><img src="http://dl.dropbox.com/u/10506/blog/hn-analysis/all_trends2.png" alt="All Trends" /></p>
]]></content>
  </entry>
  
</feed>
