<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Edwin Chen's Blog]]></title>
  <link href="http://blog.echen.me/atom.xml" rel="self"/>
  <link href="http://blog.echen.me/"/>
  <updated>2012-07-31T01:19:39-07:00</updated>
  <id>http://blog.echen.me/</id>
  <author>
    <name><![CDATA[Edwin Chen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Soda vs. Pop with Twitter]]></title>
    <link href="http://blog.echen.me/2012/07/06/soda-vs-pop-with-twitter/"/>
    <updated>2012-07-06T10:51:00-07:00</updated>
    <id>http://blog.echen.me/2012/07/06/soda-vs-pop-with-twitter</id>
    <content type="html"><![CDATA[<p>One of the great things about Twitter is that it&#8217;s a global conversation anyone can join anytime. Eavesdropping on the world, what what!</p>

<p>Of course, it gets even better when you can <em>mine</em> all this chatter to study the way humans live and interact.</p>

<p>For example, <a href="http://blog.echen.me/2011/04/18/twifferences-between-californians-and-new-yorkers/">how do people in New York City differ from those in Silicon Valley?</a> We tend to think they&#8217;re more financially driven and restless with the world &#8211; is this true, and if so, <a href="http://blog.echen.me/2011/04/18/twifferences-between-californians-and-new-yorkers/">how much more</a>?</p>

<p>Or how does language change as you travel to different regions? Recall the classic soda vs. pop. vs. coke question: some people use the word &#8220;soda&#8221; to describe their soft drinks, others use &#8220;pop&#8221;, and still others use &#8220;coke&#8221;. Who says what where?</p>

<p>Let&#8217;s take a look.</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/united-states.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/united-states.png" alt="United States" /></a></p>

<p>To make this map, I sampled geo-tagged tweets containing the words &#8220;soda&#8221;, &#8220;pop&#8221;, or &#8220;coke&#8221;, performed some state-of-the-art NLP technology to ensure the tweets were soft drink related (e.g., the tweets had to contain &#8220;drink soda&#8221; or &#8220;drink a pop&#8221;), and tried to filter out coke tweets that were specifically about the Coke brand (e.g., Coke Zero).</p>

<p>It&#8217;s a little cluttered, though, so let&#8217;s clean it up by aggregating nearby tweets.</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/united_states_binned.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/united_states_binned.png" alt="United States Binned" /></a></p>

<p>Here, I bucketed all tweets within a 0.333 latitude/longitude radius, calculated the term distribution within each bucket, and colored each bucket with the word furthest from its overall mean. I also sized each point according to the (log-transformed) number of tweets in the bucket.</p>

<p>We can see that:</p>

<ul>
<li>The South is pretty Coke-heavy.</li>
<li>Soda belongs to the Northeast and far West.</li>
<li>Pop gets the mid-West, except for some interesting spots of blue around Wisconsin and the Illinois-Missouri border.</li>
</ul>


<p>For comparison, here&#8217;s another map based on a survey at <a href="http://www.popvssoda.com/">popvssoda.com</a>.</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/popvssoda.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/popvssoda.png" alt="Pop vs. Soda Map" /></a></p>

<p>We can see similar patterns, though interestingly, our map has less Coke in the Southeast and less pop in the Northwest.</p>

<p>Finally, here&#8217;s a world map of the terms, bucketed again. Notice that &#8220;pop&#8221; seems to be prevalent only in parts of the United States and Canada.</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/world-map.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/world-map.png" alt="World" /></a></p>

<p>As some astute readers noted, though, the seeming dominance of coke is probably due to the difficulty in distinguishing the generic use of coke for soft drinks in general from the particular use of coke for referring to the Coca-Cola brand.</p>

<p>So let&#8217;s instead look at a world map of a couple other soft drink terms (&#8220;fizzy drink&#8221;, &#8220;mineral&#8221;, and &#8220;tonic&#8221;):</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/fizzy-mineral-tonic.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/fizzy-mineral-tonic.png" alt="Fizzy Drink vs. Mineral vs. Tonic" /></a></p>

<p>Notice that:</p>

<ul>
<li>&#8220;Fizzy drink&#8221; shows up for the UK, New Zealand, and Maine.</li>
<li>&#8220;Tonic&#8221; appears in Massachusetts.</li>
<li>While South Africa gets &#8220;fizzy drink&#8221;, Nigeria gets &#8220;mineral&#8221;.</li>
</ul>


<p>I&#8217;ve been getting a lot of questions lately about interesting things you can do with the Twitter API, so this was just one small project I&#8217;ve worked on to illustrate. <a href="http://www.cc.gatech.edu/~jeisenst/papers/emnlp2010.pdf">This paper</a> contains another awesome application of Twitter data to geographic language variation, and just for fun, here are two other cute mini-projects I did a while ago:</p>

<p>What do people eat during the Super Bowl? (wings and beer, apparently)</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/superbowl-snacks.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/superbowl-snacks.png" alt="Superbowl Snacks" /></a></p>

<p>What do people want for Christmas, compared to what they actually get? (people are more satisfied than I thought!)</p>

<p><a href="https://dl.dropbox.com/u/10506/blog/sodapop/xmas.png"><img src="https://dl.dropbox.com/u/10506/blog/sodapop/xmas.png" alt="Christmas" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making the Most of Mechanical Turk: Tips and Best Practices]]></title>
    <link href="http://blog.echen.me/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/"/>
    <updated>2012-04-25T13:50:00-07:00</updated>
    <id>http://blog.echen.me/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices</id>
    <content type="html"><![CDATA[<p>Big data&#8217;s all the rage, but sometimes a couple thousand <em>human</em>-generated labels can be pretty effective as well. And since I&#8217;ve been using Amazon&#8217;s Mechanical Turk system a lot recently, I figured I&#8217;d share some of the things I&#8217;ve learned.</p>

<h1>What is MTurk?</h1>

<p><a href="https://www.mturk.com/mturk/welcome">Mechanical Turk</a> is a crowdsourcing system developed by Amazon that connects you to a relatively cheap source of human labor on the fly.</p>

<p>For example, suppose you have 10,000 websites that you want to classify as spam or not. To get these classifications, you (the <em>Requester</em>):</p>

<ol>
<li>Create a CSV file containing the links and any other information.</li>
<li>Log onto MTurk and create a <em>HIT</em> (Human Intelligence Task) describing the job (possibly by using Amazon&#8217;s WYSIWYG editor or writing your own HTML, which can refer to columns in your CSV). [There&#8217;s also an MTurk API, if you don&#8217;t want to use the terrible UI.]</li>
<li>Within hours of starting the task, your judgments will be completed by <em>Turkers</em> around the world for pennies each.</li>
</ol>


<h1>More Example Tasks</h1>

<p>So what can you use MTurk for? Here are three of my favorite uses:</p>

<ul>
<li><a href="http://boingboing.net/2011/02/18/straight-line-traced.html">A Sequence of Lines Consecutively Traced by Five Hundred Individuals</a></li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/lines.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/lines.png" alt="Lines Mutation" /></a></p>

<ul>
<li><a href="http://www.thesheepmarket.com/">The Sheep Market</a>: asking Turkers to draw sheep</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/sheep.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/sheep.png" alt="Sheep" /></a></p>

<ul>
<li><a href="http://groups.csail.mit.edu/uid/deneme/?p=329">Blurry Text Transcription</a> (Seriously! How is this possible?!)</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/blurry.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/blurry.png" alt="Blurry Text" /></a></p>

<p>And here are some more practical tasks, from HITs running right now:</p>

<ul>
<li>Categorize the sentiment of a tweet towards Panera Bread</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/panera.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/panera.png" alt="Panera" /></a></p>

<ul>
<li>Copy text from a business card</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/business-card.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/business-card.png" alt="Business Card" /></a></p>

<ul>
<li>Judge entity relatedness</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/entity.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/entity.png" alt="Angelina Jolie" /></a></p>

<h1>Increasing the quality of your judgments</h1>

<p>So what will the quality of your judgments look like?</p>

<p>If you don&#8217;t do anything special, then your output will contain a lot of garbage. I&#8217;ve thrown out entire tasks because of scammers who spend less than 5 seconds on each judgment (Amazon records the time each worker spends) and submit random clicks as output (e.g., labeling Nike as a food category).</p>

<p>Luckily, Amazon provides a few worker filters:</p>

<ul>
<li>You can require that only Turkers who have received at least (say) <strong>99% approval rate on at least 10,000 judgments in the past</strong> are allowed to work on your judgment. (If you see bad judgments from a worker, you can reject them and get your money back.)</li>
<li>About a year ago, Amazon launched a <strong>&#8220;categorization masters&#8221; and &#8220;photo masters&#8221;</strong> program, which allows only masters to work on your HITs. According to a chat with a member of the MTurk team, Amazon assigns these master badges by creating special tasks (anonymously, and for which Amazon already knows the answer) and measuring the quality of each worker&#8217;s response to these tasks.</li>
<li>You can also create a custom filter and <strong>handpick who gets allowed to work for you</strong>, or set up a <strong>qualification test</strong> that workers are required to take before working on your tasks.</li>
</ul>


<p>I&#8217;ve used different combinations of the first two filters, and gotten excellent results &#8211; compared to in-house judges I&#8217;ve worked with in person and paid \$20-30 an hour, the judgments on Mechanical Turk have been just as good and sometimes even better. (I often ask my judges to explain their judgments, which makes it easy to detect high quality workers.) For example, here are some typical response I&#8217;ve received when asking judges to determine which of two products a given Twitter user might be more interested in:</p>

<blockquote><p>The user is a female obsessed with Twilight Movies and Rob Pattinson. She tweets and follows both subjects. Movie tickets would be interesting to her.</p></blockquote>




<blockquote><p>He doesn&#8217;t seem to play video games, and he doesn&#8217;t seem technical enough to care about running Windows on a Mac. Neither of these products are a good fit for him.</p></blockquote>


<p>In fact, I&#8217;ll frequently also get emails from Turkers giving me suggestions on how to improve my tasks or asking how they can do them better. (Amazon allows workers to email you. The only way for the requester to initiate a conversation, though, is by paying the worker a small bonus for excellent work, and including a message with the bonus.) Here are excerpts from some emails I&#8217;ve received:</p>

<blockquote><p>I just wanted to check in to be sure that once I figured things out that I was doing your hits the way you intended them to be done. I want to be sure that you are getting the data that you need from the work. Please do not hesitate to let me know if there is anything that I can do to improve the way I am working your HITs. This is my full time job while I stay at home with my kids, so I like to check with the requesters to be sure that I am putting out the work that they are looking for. Any suggestion is welcome.</p></blockquote>




<blockquote><p>Frankly, lingerie, makeup, and feminine hygiene are the only male-exclusionary topics I can think of, and it feels knee-jerk sexist to mark any sports-related site for men. That said, should I hew more closely to gender stereotypes or be politically correct? (from a HIT where I was gathering gender classification data)</p></blockquote>




<blockquote><p>I do think a few more categories are needed but keeping the number down overall is good - 50 or 60 to choose from can be overwhelming and not worth the time. I may have mentioned I never used the Photography one (and I did a lot of those) so that is a good candidate for elimination.</p></blockquote>


<p>That said, despite the approval rate filters and masters badges, I do occasionally get a couple scammers in the mix (or even just judges who don&#8217;t produce as excellent work). So one suggestion is to run an initial task with these filters applied, find the workers with the best quality, and from then on use a custom pool containing these Turkers alone.</p>

<h1>How much to pay</h1>

<p>So how much should you pay your workers?</p>

<p>New Turkers and Turkers who don&#8217;t meet the strict filters can be paid less, but most of my high-quality workers expect to make about \$8-14 an hour. (You can only specify how much you pay per judgment, but Amazon will tell you how long each item ends up taking on average.) For example, here&#8217;s what several Turkers said what I asked them directly how much they make:</p>

<blockquote><p>Most of the work I do is either writing or editing.  When editing work is available, I make \$15-20 per hour.  I&#8217;m a slower writer than an editor, so I average \$10-12 per hour with writing.  I also judge sentiments of messages and average about \$8 per hour with that type of work. I would like to average a minimum of no less than \$8 per hour.</p><p>A big factor in deciding to do a task or not comes from the time investment involved. The two big time sinks are either googling/searching/having to go to another site, or having to write something as part of your reply. If I remember correctly, a) your tasks did require looking at another page but either the link was right there OR, better yet, you had that page embedded in the HIT itself so clicking out of the window wasn&#8217;t necessary (turkers get very excited about this), and b) the quality of the pay rate was such that it easily outweighed the time it took to leave an explanatory comment. </p><p>For me at least, those things can&#8217;t be underestimated. Sure, your tasks may be a little time-consuming, but I figure a good task is one I can make 10 to 12 cents a minute on. Your task might take longer but I&#8217;m definitely still coming out ahead.</p><p>From my own experience, I work hardest and best for a requester that pays well and doesn&#8217;t reject (or at least seems to have a reason for a rejection when it happens). If a requester is going to accept the majority of my work, I as a worker feel that obligates me to provide them with the best quality possible. Similarly, although I&#8217;m conscientious with all tasks, I&#8217;m especially so with a high-paying one: it would be easy to take advantage of a high-pay, low-reject requester - which would ultimately lead them to either lower the pay or change the acceptance criteria. I don&#8217;t want that!! That&#8217;s the kind of requester I want around. I&#8217;m grateful for high pay and fair policies and that kind of requester gets an above-and-beyond effort from me.</p></blockquote>




<blockquote><p>For the pay, I have worked on master&#8217;s hits that have ranged from \$6-\$16 per hour. Averaging them out works out to around \$9, which isn&#8217;t a bad wage. I have two requesters that I work for that don&#8217;t use the master qualification but instead have closed qualifications that they&#8217;ve assigned to their best workers. Those tasks pay between \$12 and \$15 per hour, so no matter what I&#8217;m working on I will stop what I&#8217;m doing to work on them. The best paying hits are always done very quickly, so most of the time if you check out mturk and look at the tasks available you won&#8217;t get a very good idea of average pay because the terrible paying hits will sit on the board until they expire.</p></blockquote>


<p>Obviously, this is self-reported, so there&#8217;s a strong possibility that the Turkers are artificially inflating their numbers. But this does match what I&#8217;ve been told by a manager on the MTurk team, as well as what Turkers self-report on <a href="http://turkernation.com/">TurkerNation</a>.</p>

<p>A good suggestion regarding pay is to start at the lower end of the scale, around \$6-8 per hour, and increase that until you get both the quality and speed you want.</p>

<h1>Other design tips</h1>

<p>Interestingly, according to what Turkers (see the excerpts above) and my Amazon contact say, as well as other research I&#8217;ve seen (e.g., <a href="http://groups.csail.mit.edu/uid/deneme/?p=680">this paper</a>), pay is <em>not</em> at the absolute forefront of Turkers&#8217; minds when they decide what to work on. Instead, they focus more on requesters they&#8217;ve already established a good relationship with, HITs with many items (so they can quickly settle into a rhythm), HITs they know they&#8217;ll be paid for (so they&#8217;re not worried about rejections), and HITs that they generally enjoy doing more.</p>

<p>So here are a couple suggestions:</p>

<ul>
<li>If your task is hard and there&#8217;s no clearly correct answer, even good Turkers might be worried that you&#8217;ll reject their judgments (and so they might skip over your HIT). So make it clear in your instructions that you won&#8217;t reject any judgments, or that you won&#8217;t reject any judgments with an honest effort.</li>
<li>Make your instructions collapsible, or link to them in a separate site. Scrolling is kind of annoying on Mechanical Turk (I know &#8211; I&#8217;ve tried working on HITs myself), so you should minimize the amount workers have to scroll. Ideally, everything fits on a single screen. Plus, the less workers have to scroll, the faster your HITs will get done. For example, here are excerpts from emails I received from two different Turkers when I first started out:</li>
</ul>


<blockquote><p>I have a suggestion that would really make things go a little quicker. Is there anyway you could script the twitter link to automatically open in a new tab? It amazes me how much it can slow you down to have to right click and open it manually in another tab, and when you forget, you have to take a few more steps to get back to where you were.</p></blockquote>




<blockquote><p>It would be amazing if the Twitter account could be on the same page instead of having to click to get to another screen - the work would go *exponentially* faster! Overall, I&#8217;m enjoying them - and I&#8217;m not the only one. Despite your stringent requirements these are disappearing pretty quickly.</p></blockquote>


<ul>
<li>Introduce yourself on <a href="http://turkernation.com/">TurkerNation</a>, a forum where Turkers and Requesters go to talk about Mechanical Turk. This helps establish your reputation as a good requester who listens to feedback, which will make good Turkers want to work for you. (More on this below.)</li>
<li>Approve judgments quickly: Turkers want <a href="http://en.wikipedia.org/wiki/Hyperbolic_discounting">money now instead of money later</a>. For example, one worker told me:</li>
</ul>


<blockquote><p>Quick approval is important, too. Watching that money pile up is a serious motivator; I&#8217;ll sometimes choose a lower-paying task that approves in close to real time over a higher-paying one that won&#8217;t pay out for several days.</p></blockquote>


<p>When using my trusted set of workers, I let Amazon auto-approve all judgments within a couple hours.</p>

<h1>Reputation</h1>

<p>Reputation is pretty important. Turkers love requesters who take the time to respond to emails and incorporate suggestions. Excerpts from emails I&#8217;ve received:</p>

<blockquote><p>I LOVE it when requesters care enough to ask the opinion of us lowly turkers and am more than willing to take a few minutes to help them with anything. I look forward to seeing what you cook up!</p></blockquote>




<blockquote><p>Thanks for taking the time to try to make your hits better in both pay and design. It&#8217;s great to see a requester that actually cares, when most don&#8217;t. If you have any other questions for me, feel free to ask. I hope to work for you again soon.</p></blockquote>




<blockquote><p>From my own experience, I work hardest and best for a requester that pays well and doesn&#8217;t reject (or at least seems to have a reason for a rejection when it happens). If a requester is going to accept the majority of my work, I as a worker feel that obligates me to provide them with the best quality possible. Similarly, although I&#8217;m conscientious with all tasks, I&#8217;m especially so with a high-paying one: it would be easy to take advantage of a high-pay, low-reject requester - which would ultimately lead them to either lower the pay or change the acceptance criteria. I don&#8217;t want that!! That&#8217;s the kind of requester I want around. I&#8217;m grateful for high pay and fair policies and that kind of requester gets an above-and-beyond effort from me.</p></blockquote>


<p>I&#8217;ve gotten great suggestions from a lot of Turkers (sometimes, when launching a new type of experiment, I&#8217;ll do a quick trial run in order to get some fast feedback before spending more time on the HIT design), and I suspect it&#8217;s partly because I&#8217;ve taken the time to connect with my workers.</p>

<p>So, as suggested above, one way of quickly garnering some goodwill when you&#8217;re first getting started is to make a post introducing yourself on TurkerNation. (There&#8217;s a <a href="http://turkernation.com/forumdisplay.php?23-Requester-Introductions">sub-forum</a> devoted to this exact purpose, in fact.)</p>

<p>This is useful because workers will often start new threads recommending particular requesters and encouraging other Turkers to work for them. In the amusing thread praising me, for example, one worker mentioned that she&#8217;d been hesitant to work on my HITs until she saw the post confirming I was a good requester.</p>

<p>Also, many Turkers mention that they always refer to <a href="http://turkopticon.differenceengines.com/">Turkopticon</a>, a Firefox extension that displays ratings of requesters by other Turkers, before accepting work from a requester they haven&#8217;t worked for before.</p>

<p>This is what TurkOpticon looks like:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/jimyoung.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/jimyoung.png" alt="Jim Young" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/mturk/productrnr.png"><img src="http://dl.dropbox.com/u/10506/blog/mturk/productrnr.png" alt="ProductRNR" /></a></p>

<p>Here are some comments about TurkOpticon on TurkerNation:</p>

<blockquote><p>I think that it is well worth taking the time to check reputation of requesters via TurkOpticon and/or in this forum. Checking first substantially minimizes your risk of rejection, of being blocked, and of being paid sub-human wages.</p></blockquote>




<blockquote><p>Blindly doing hits for requesters that were never heard of before got me with a pretty bad approval rate when I first started turking. After that, I rigorously inspect every requester that doesn&#8217;t have any ratings on Turkopticon. Actually, because of that little add-on I&#8217;ve been able to maintain a steady 98-99% approval rate ever since I began using it.</p></blockquote>


<h1>Waiting Time</h1>

<p>So how long does it take to get judgments? I&#8217;ve restricted the available worker pool pretty strongly to ensure high quality, and it&#8217;s still only taken a few hours to get a thousand judgments.</p>

<p>That&#8217;s pretty awesome. I&#8217;ve worked a lot with human evaluation systems before, but always using a small in-house set of judges &#8211; and what with constraints on when those judges were available, how much they were able to work each week, and other tasks taking higher priority, it&#8217;d invariably take at least a few days before I&#8217;d receive any useful data back.</p>

<p>Getting thousands of judgments in a couple hours means I can launch an MTurk task when I leave for work in the morning and have it done before lunch, which makes experimenting with a lot of different ideas much faster and easier.</p>

<h1>Scale</h1>

<p>So how many judgments can you actually get before you run out of workers? I&#8217;m still a small fish in the MTurk system, but I&#8217;m told by my MTurk contact at Amazon that there are companies getting over a million judgments each month.</p>

<p>I also asked my pool of workers how much they&#8217;re available to work, in case I would need to scale up to more judgments later on, and here are some samples from what they said:</p>

<blockquote><p>Typically, I work a total of 20-25 hours per week for a small select group of requesters.  I could put in at least 20 hours per week for you alone if you were to make a custom qualification for me.  If I know that I can continue to do exemplary work beyond 20 hours, I would be willing to put forth more hours of work.  I want to make sure that you are getting the quality of work that you need.</p></blockquote>




<blockquote><p>On a day when I don&#8217;t have those other assignments, I&#8217;d guess I&#8217;m turking 5 to 7 hours a day  (including weekends). I like to look for a large batch of HITs (preferably in the thousands) so that I can settle into a groove of being able to do them fairly quickly and once I find something like that I can happily settle in for several hours at a time.</p></blockquote>




<blockquote><p>I spend more time than the average person on mturk. I log on at about 5:30 AM and am constantly checking for work throughout the day. If the work is available, I will spend until 9PM working. Granted, I do have to take some breaks throughout the day to take care of my 3 year old, but for the most part, I am doing my best to earn while the hits are posted. If I take any time off, it is on the weekend (if I reach my earning goals for the week).</p></blockquote>




<blockquote><p>Of course, how much I can work varies. My main source of income is transcription for a market research company and mturk fills in my downtime. If I have an audio file from them, that gets my attention. If not, I&#8217;m on mturk. As a single mother working from home, I love the flexibility.</p></blockquote>


<h1>End</h1>

<p>I&#8217;ll end with a couple other notes.</p>

<ul>
<li>How do other companies use human evaluation systems? Google and Bing use human judgments in their search metrics, though I think they use an in-house set of judges rather than Mechanical Turk. I&#8217;ve heard Aardvark and Quora used Mechanical Turk to seed answers when they first launched their sites. There&#8217;s also a nice set of case studies <a href="http://aws.amazon.com/solutions/case-studies/">here</a> (search for the &#8220;On-Demand Workforce&#8221; section); in particular, Knewton&#8217;s use of <a href="http://aws.amazon.com/solutions/case-studies/knewton/">MTurk for performance and QA testing</a> is pretty interesting.</li>
<li>I&#8217;ve described one way of finding good workers, namely, using the filters Amazon provides. Another way could be to build a reputation system yourself, perhaps using an EM-style algorithm to determine judge quality.</li>
<li><a href="http://crowdflower.com/">Crowdflower</a> is another crowdsourcing system. There are a couple differences with MTurk:

<ul>
<li>Crowdflower&#8217;s worker pool consists of about 20 different sources, including Mechanical Turk, as well as sources like TrialPay (people can opt to complete a MTurk task to receive some kind of TrialPay deal).</li>
<li>Crowdflower offers both a self-serve platform (like MTurk), as well as a more enterprise-centric solution (where you work directly with a Crowdflower employee). The enterprise offering is pretty nice, since that means Crowdflower will take care of the lower-level details for you (like actually designing and creating the job), and they can offer suggestions for designing the HIT based on their experience.</li>
<li>Crowdflower provides the option of adding gold standard judgments to your task (items where you provide a golden answer, which are then randomly shown to workers; these are then used to monitor judges) and they try to automatically determine judge quality and item accuracy for you (e.g., by having each item judged by three different workers).</li>
</ul>
</li>
<li>An excellent crowdsourcing resource is <a href="http://crowdscope.org/index.php?title=Main_Page">CrowdScope</a>. I also like the <a href="http://groups.csail.mit.edu/uid/deneme/">Deneme</a> blog (though it hasn&#8217;t been updated in a while) for a lot of fun experiments. Panos Ipeirotis&#8217; <a href="http://www.behind-the-enemy-lines.com/">blog</a> has good information as well.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process]]></title>
    <link href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"/>
    <updated>2012-03-20T09:14:00-07:00</updated>
    <id>http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process</id>
    <content type="html"><![CDATA[<p>Imagine you&#8217;re a budding chef. A data-curious one, of course, so you start by taking a set of foods (pizza, salad, spaghetti, etc.) and ask 10 friends how much of each they ate in the past day.</p>

<p>Your goal: to find natural <em>groups</em> of foodies, so that you can better cater to each cluster&#8217;s tastes. For example, your fratboy friends might love <a href="https://twitter.com/#!/edchedch/status/166343879547822080">wings and beer</a>, your anime friends might love soba and sushi, your hipster friends probably dig tofu, and so on.</p>

<p>So how can you use the data you&#8217;ve gathered to discover different kinds of groups?</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/clustering-example.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/clustering-example.png" alt="Clustering Example" /></a></p>

<p>One way is to use a standard clustering algorithm like <strong>k-means</strong> or <strong>Gaussian mixture modeling</strong> (see <a href="http://blog.echen.me/2011/03/19/counting-clusters/">this previous post</a> for a brief introduction). The problem is that these both assume a <em>fixed</em> number of clusters, which they need to be told to find. There are a couple methods for selecting the number of clusters to learn (e.g., the <a href="http://blog.echen.me/2011/03/19/counting-clusters/">gap and prediction strength statistics</a>), but the problem is a more fundamental one: most real-world data simply doesn&#8217;t have a fixed number of clusters.</p>

<p>That is, suppose we&#8217;ve asked 10 of our friends what they ate in the past day, and we want to find groups of eating preferences. There&#8217;s really an infinite number of foodie types (carnivore, vegan, snacker, Italian, healthy, fast food, heavy eaters, light eaters, and so on), but with only 10 friends, we simply don&#8217;t have enough data to detect them all. (Indeed, we&#8217;re limited to 10 clusters!) So whereas k-means starts with the incorrect assumption that there&#8217;s a fixed, finite number of clusters that our points come from, <em>no matter if we feed it more data</em>, what we&#8217;d really like is a method positing an infinite number of hidden clusters that naturally arise as we ask more friends about their food habits. (For example, with only 2 data points, we might not be able to tell the difference between vegans and vegetarians, but with 200 data points, we probably could.)</p>

<p>Luckily for us, this is precisely the purview of <strong>nonparametric Bayes</strong>.*</p>

<p>*Nonparametric Bayes refers to a class of techniques that allow some parameters to change with the data. In our case, for example, instead of fixing the number of clusters to be discovered, we allow it to grow as more data comes in.</p>

<h1>A Generative Story</h1>

<p>Let&#8217;s describe a generative model for finding clusters in any set of data. We assume an infinite set of latent groups, where each group is described by some set of parameters. For example, each group could be a Gaussian with a specified mean $\mu_i$ and standard deviation $\sigma_i$, and these group parameters themselves are assumed to come from some base distribution $G_0$. Data is then generated in the following manner:</p>

<ul>
<li>Select a cluster.</li>
<li>Sample from that cluster to generate a new point.</li>
</ul>


<p>(Note the resemblance to a <a href="http://en.wikipedia.org/wiki/Mixture_model">finite mixture model</a>.)</p>

<p>For example, suppose we ask 10 friends how many calories of pizza, salad, and rice they ate yesterday. Our groups could be:</p>

<ul>
<li>A Gaussian centered at (pizza = 5000, salad = 100, rice = 500) (i.e., a pizza lovers group).</li>
<li>A Gaussian centered at (pizza = 100, salad = 3000, rice = 1000) (maybe a vegan group).</li>
<li>A Gaussian centered at (pizza = 100, salad = 100, rice = 10000) (definitely Asian).</li>
<li>&#8230;</li>
</ul>


<p>When deciding what to eat when she woke up yesterday, Alice could have thought <em>girl, I&#8217;m in the mood for pizza</em> and her food consumption yesterday would have been a sample from the pizza Gaussian. Similarly, Bob could have spent the day in Chinatown, thereby sampling from the Asian Gaussian for his day&#8217;s meals. And so on.</p>

<p>The big question, then, is: how do we assign each friend to a group?</p>

<h1>Assigning Groups</h1>

<h2>Chinese Restaurant Process</h2>

<p>One way to assign friends to groups is to use a <strong>Chinese Restaurant Process</strong>. This works as follows: Imagine a restaurant where all your friends went to eat yesterday&#8230;</p>

<ul>
<li>Initially the restaurant is empty.</li>
<li>The first person to enter (Alice) sits down at a table (selects a group). She then orders food for the table (i.e., she selects parameters for the group); everyone else who joins the table will then be limited to eating from the food she ordered.</li>
<li>The second person to enter (Bob) sits down at a table. Which table does he sit at? With probability $\alpha / (1 + \alpha)$ he sits down at a new table (i.e., selects a new group) and orders food for the table; with probability $1 / (1 + \alpha)$ he sits with Alice and eats from the food she&#8217;s already ordered (i.e., he&#8217;s in the same group as Alice).</li>
<li>&#8230;</li>
<li>The (n+1)-st person sits down at a new table with probability $\alpha / (n + \alpha)$, and at table k with probability $n_k / (n + \alpha)$, where $n_k$ is the number of people currently sitting at table k.</li>
</ul>


<p>Note a couple things:</p>

<ul>
<li>The more people (data points) there are at a table (cluster), the more likely it is that people (new data points) will join it. In other words, our groups satisfy a <strong>rich get richer</strong> property.</li>
<li>There&#8217;s always a small probability that someone joins an entirely new table (i.e., a new group is formed).</li>
<li>The probability of a new group depends on $\alpha$. So we can think of $\alpha$ as a <strong>dispersion parameter</strong> that affects the dispersion of our datapoints. The lower alpha is, the more tightly clustered our data points; the higher it is, the more clusters we have in any finite set of points.</li>
</ul>


<p>(Also notice the resemblance between table selection probabilities and a Dirichlet distribution&#8230;)</p>

<p>Just to summarize, given n data points, the Chinese Restaurant Process specifies a distribution over partitions (table assignments) of these points. We can also generate parameters for each partition/table from a base distribution $G_0$ (for example, each table could represent a Gaussian whose mean and standard deviation are sampled from $G_0$), though to be clear, this is not part of the CRP itself.</p>

<h3>Code</h3>

<p>Since code makes everything better, here&#8217;s some Ruby to simulate a CRP:</p>

<figure class='code'><figcaption><span>Chinese Restaurant Process </span><a href='https://github.com/echen/dirichlet-process/blob/master/chinese_restaurant_process.rb'>chinese_restaurant_process.rb</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="c1"># Generate table assignments for `num_customers` customers, according to</span>
</span><span class='line'><span class="c1"># a Chinese Restaurant Process with dispersion parameter `alpha`.</span>
</span><span class='line'><span class="c1">#</span>
</span><span class='line'><span class="c1"># returns an array of integer table assignments</span>
</span><span class='line'><span class="k">def</span> <span class="nf">chinese_restaurant_process</span><span class="p">(</span><span class="n">num_customers</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class='line'> <span class="k">return</span> <span class="o">[]</span> <span class="k">if</span> <span class="n">num_customers</span> <span class="o">&lt;=</span> <span class="mi">0</span>
</span><span class='line'>
</span><span class='line'> <span class="n">table_assignments</span> <span class="o">=</span> <span class="o">[</span><span class="mi">1</span><span class="o">]</span> <span class="c1"># first customer sits at table 1</span>
</span><span class='line'> <span class="n">next_open_table</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># index of the next empty table</span>
</span><span class='line'>
</span><span class='line'> <span class="c1"># Now generate table assignments for the rest of the customers.</span>
</span><span class='line'> <span class="mi">1</span><span class="o">.</span><span class="n">upto</span><span class="p">(</span><span class="n">num_customers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span>
</span><span class='line'>   <span class="k">if</span> <span class="nb">rand</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="o">.</span><span class="n">to_f</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
</span><span class='line'>     <span class="c1"># Customer sits at new table.</span>
</span><span class='line'>     <span class="n">table_assignments</span> <span class="o">&lt;&lt;</span> <span class="n">next_open_table</span>
</span><span class='line'>     <span class="n">next_open_table</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>   <span class="k">else</span>
</span><span class='line'>     <span class="c1"># Customer sits at an existing table.</span>
</span><span class='line'>     <span class="c1"># He chooses which table to sit at by giving equal weight to each</span>
</span><span class='line'>     <span class="c1"># customer already sitting at a table. </span>
</span><span class='line'>     <span class="n">which_table</span> <span class="o">=</span> <span class="n">table_assignments</span><span class="o">[</span><span class="nb">rand</span><span class="p">(</span><span class="n">table_assignments</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">]</span>
</span><span class='line'>     <span class="n">table_assignments</span> <span class="o">&lt;&lt;</span> <span class="n">which_table</span>
</span><span class='line'>   <span class="k">end</span>
</span><span class='line'> <span class="k">end</span>
</span><span class='line'>
</span><span class='line'> <span class="n">table_assignments</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>And here&#8217;s some sample output:</p>

<figure class='code'><figcaption><span>Chinese Restaurant Process </span><a href='https://github.com/echen/dirichlet-process/blob/master/chinese_restaurant_process.rb'>chinese_restaurant_process.rb</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="o">&gt;</span> <span class="n">chinese_restaurant_process</span><span class="p">(</span><span class="n">num_customers</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># table assignments from run 1</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># table assignments from run 2</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span> <span class="c1"># table assignments from run 3</span>
</span><span class='line'>
</span><span class='line'><span class="o">&gt;</span> <span class="n">chinese_restaurant_process</span><span class="p">(</span><span class="n">num_customers</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'><span class="o">&gt;</span> <span class="n">chinese_restaurant_process</span><span class="p">(</span><span class="n">num_customers</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span>
</span><span class='line'><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span>
</span></code></pre></td></tr></table></div></figure>


<p>Notice that as we increase $\alpha$, so too does the number of distinct tables increase.</p>

<h2>Polya Urn Model</h2>

<p>Another method for assigning friends to groups is to follow the <strong>Polya Urn Model</strong>. This is basically the same model as the Chinese Restaurant Process, just with a different metaphor.</p>

<ul>
<li>We start with an urn containing $\alpha G_0(x)$ balls of &#8220;color&#8221; $x$, for each possible value of $x$. ($G_0$ is our base distribution, and $G_0(x)$ is the probability of sampling $x$ from $G_0$). Note that these are possibly fractional balls.</li>
<li>At each time step, draw a ball from the urn, note its color, and then drop both the original ball plus a new ball of the same color back into the urn.</li>
</ul>


<p>Note the connection between this process and the CRP: balls correspond to people (i.e., data points), colors correspond to table assignments (i.e., clusters), alpha is again a dispersion parameter (put differently, a prior), colors satisfy a rich-get-richer property (since colors with many balls are more likely to get drawn), and so on. (Again, there&#8217;s also a connection between this urn model and <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution#P.C3.B3lya.27s_urn">the urn model for the (finite) Dirichlet distribution</a>&#8230;)</p>

<p>To be precise, the difference between the CRP and the Polya Urn Model is that the CRP specifies only a distribution over <em>partitions</em> (i.e., table assignments), but doesn&#8217;t assign parameters to each group, whereas the Polya Urn Model does both.</p>

<h3>Code</h3>

<p>Again, here&#8217;s some code for simulating a Polya Urn Model:</p>

<figure class='code'><figcaption><span>Polya Urn Model </span><a href='https://github.com/echen/dirichlet-process/blob/master/polya_urn_model.rb'>polya_urn_model.rb</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="c1"># Draw `num_balls` colored balls according to a Polya Urn Model</span>
</span><span class='line'><span class="c1"># with a specified base color distribution and dispersion parameter</span>
</span><span class='line'><span class="c1"># `alpha`.</span>
</span><span class='line'><span class="c1">#</span>
</span><span class='line'><span class="c1"># returns an array of ball colors</span>
</span><span class='line'><span class="k">def</span> <span class="nf">polya_urn_model</span><span class="p">(</span><span class="n">base_color_distribution</span><span class="p">,</span> <span class="n">num_balls</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class='line'>  <span class="k">return</span> <span class="o">[]</span> <span class="k">if</span> <span class="n">num_balls</span> <span class="o">&lt;=</span> <span class="mi">0</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">balls_in_urn</span> <span class="o">=</span> <span class="o">[]</span>
</span><span class='line'>  <span class="mi">0</span><span class="o">.</span><span class="n">upto</span><span class="p">(</span><span class="n">num_balls</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span>
</span><span class='line'>    <span class="k">if</span> <span class="nb">rand</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="o">.</span><span class="n">to_f</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">balls_in_urn</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span><span class='line'>      <span class="c1"># Draw a new color, put a ball of this color in the urn.</span>
</span><span class='line'>      <span class="n">new_color</span> <span class="o">=</span> <span class="n">base_color_distribution</span><span class="o">.</span><span class="n">call</span>
</span><span class='line'>      <span class="n">balls_in_urn</span> <span class="o">&lt;&lt;</span> <span class="n">new_color</span>
</span><span class='line'>    <span class="k">else</span>
</span><span class='line'>      <span class="c1"># Draw a ball from the urn, add another ball of the same color.</span>
</span><span class='line'>      <span class="n">ball</span> <span class="o">=</span> <span class="n">balls_in_urn</span><span class="o">[</span><span class="nb">rand</span><span class="p">(</span><span class="n">balls_in_urn</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">]</span>
</span><span class='line'>      <span class="n">balls_in_urn</span> <span class="o">&lt;&lt;</span> <span class="n">ball</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">balls_in_urn</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<p>And here&#8217;s some sample output, using a uniform distribution over the unit interval as the color distribution to sample from:</p>

<figure class='code'><figcaption><span>Polya Urn Model </span><a href='https://github.com/echen/dirichlet-process/blob/master/polya_urn_model.rb'>polya_urn_model.rb</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="o">&gt;</span> <span class="n">unit_uniform</span> <span class="o">=</span> <span class="nb">lambda</span> <span class="p">{</span> <span class="p">(</span><span class="nb">rand</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to_i</span> <span class="o">/</span> <span class="mi">100</span><span class="o">.</span><span class="mi">0</span> <span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">&gt;</span> <span class="n">polya_urn_model</span><span class="p">(</span><span class="n">unit_uniform</span><span class="p">,</span> <span class="n">num_balls</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="mi">0</span><span class="o">.</span><span class="mi">27</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">89</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">89</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">89</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">73</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">98</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">43</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">98</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">89</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">53</span> <span class="c1"># colors in the urn from run 1</span>
</span><span class='line'><span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">46</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">26</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">85</span> <span class="c1"># colors in the urn from run 2</span>
</span><span class='line'><span class="mi">0</span><span class="o">.</span><span class="mi">96</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">87</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">96</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">87</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">96</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">96</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">87</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">96</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">96</span><span class="p">,</span> <span class="mi">0</span><span class="o">.</span><span class="mi">96</span> <span class="c1"># colors in the urn from run 3</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Code, Take 2</h3>

<p>Here&#8217;s the same code for a Polya Urn Model, but in R:</p>

<figure class='code'><figcaption><span>Polya Urn Model </span><a href='https://github.com/echen/dirichlet-process/blob/master/polya_urn_model.R'>polya_urn_model.R</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># Return a vector of `num_balls` ball colors according to a Polya Urn Model</span>
</span><span class='line'><span class="c1"># with dispersion `alpha`, sampling from a specified base color distribution.</span>
</span><span class='line'>polya_urn_model <span class="o">=</span> <span class="kr">function</span><span class="p">(</span>base_color_distribution<span class="p">,</span> num_balls<span class="p">,</span> alpha<span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  balls <span class="o">=</span> c<span class="p">()</span>
</span><span class='line'>
</span><span class='line'>  <span class="kr">for</span> <span class="p">(</span>i in <span class="m">1</span>:num_balls<span class="p">)</span> <span class="p">{</span>
</span><span class='line'>    <span class="kr">if</span> <span class="p">(</span>runif<span class="p">(</span><span class="m">1</span><span class="p">)</span> <span class="o">&lt;</span> alpha <span class="o">/</span> <span class="p">(</span>alpha <span class="o">+</span> length<span class="p">(</span>balls<span class="p">)))</span> <span class="p">{</span>
</span><span class='line'>      <span class="c1"># Add a new ball color.</span>
</span><span class='line'>      new_color <span class="o">=</span> base_color_distribution<span class="p">()</span>
</span><span class='line'>      balls <span class="o">=</span> c<span class="p">(</span>balls<span class="p">,</span> new_color<span class="p">)</span>
</span><span class='line'>    <span class="p">}</span> <span class="kr">else</span> <span class="p">{</span>
</span><span class='line'>      <span class="c1"># Pick out a ball from the urn, and add back a</span>
</span><span class='line'>      <span class="c1"># ball of the same color.</span>
</span><span class='line'>      ball <span class="o">=</span> balls<span class="p">[</span>sample<span class="p">(</span><span class="m">1</span>:length<span class="p">(</span>balls<span class="p">),</span> <span class="m">1</span><span class="p">)]</span>
</span><span class='line'>      balls <span class="o">=</span> c<span class="p">(</span>balls<span class="p">,</span> ball<span class="p">)</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>  balls
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here are some sample density plots of the colors in the urn, when using a unit normal as the base color distribution:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_1.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_1.png" alt="Polya Urn Model, Alpha = 1" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_5.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_5.png" alt="Polya Urn Model, Alpha = 5" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_25.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_25.png" alt="Polya Urn Model, Alpha = 25" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_50.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_50.png" alt="Polya Urn Model, Alpha = 50" /></a></p>

<p>Notice that as alpha increases (i.e., we sample more new ball colors from our base; i.e., as we place more weight on our prior), the colors in the urn tend to a unit normal (our base color distribution).</p>

<p>And here are some sample plots of points generated by the urn, for varying values of alpha:</p>

<ul>
<li>Each color in the urn is sampled from a uniform distribution over [0,10]x[0,10] (i.e., a [0, 10] square).</li>
<li>Each group is a Gaussian with standard deviation 0.1 and mean equal to its associated color, and these Gaussian groups generate points.</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.1.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.1.png" alt="Alpha 0.1" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.2.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.2.png" alt="Alpha 0.2" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.3.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.3.png" alt="Alpha 0.3" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.5.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-0.5.png" alt="Alpha 0.5" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-1.0.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/alpha-1.0.png" alt="Alpha 1.0" /></a></p>

<p>Notice that the points clump together in fewer clusters for low values of alpha, but become more dispersed as alpha increases.</p>

<h2>Stick-Breaking Process</h2>

<p>Imagine running either the Chinese Restaurant Process or the Polya Urn Model without stop. For each group $i$, this gives a proportion $w_i$ of points that fall into group $i$.</p>

<p>So instead of running the CRP or Polya Urn model to figure out these proportions, can we simply generate them directly?</p>

<p>This is exactly what the Stick-Breaking Process does:</p>

<ul>
<li>Start with a stick of length one.</li>
<li>Generate a random variable $\beta_1 \sim Beta(1, \alpha)$. By the definition of the <a href="http://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>, this will be a real number between 0 and 1, with expected value $1 / (1 + \alpha)$. Break off the stick at $\beta_1$; $w_1$ is then the length of the stick on the left.</li>
<li>Now take the stick to the right, and generate $\beta_2 \sim Beta(1, \alpha)$. Break off the stick $\beta_2$ into the stick. Again, $w_2$ is the length of the stick to the left, i.e., $w_2 = (1 - \beta_1) \beta_2$.</li>
<li>And so on.</li>
</ul>


<p>Thus, the Stick-Breaking process is simply the CRP or Polya Urn Model from a different point of view. For example, assigning customers to table 1 according to the Chinese Restaurant Process is equivalent to assigning customers to table 1 with probability $w_1$.</p>

<h3>Code</h3>

<p>Here&#8217;s some R code for simulating a Stick-Breaking process:</p>

<figure class='code'><figcaption><span>Stick-Breaking Process </span><a href='https://github.com/echen/dirichlet-process/blob/master/stick_breaking_process.R'>stick_breaking_process.R</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># Return a vector of weights drawn from a stick-breaking process</span>
</span><span class='line'><span class="c1"># with dispersion `alpha`.</span>
</span><span class='line'><span class="c1">#</span>
</span><span class='line'><span class="c1"># Recall that the kth weight is</span>
</span><span class='line'><span class="c1">#   \beta_k = (1 - \beta_1) * (1 - \beta_2) * ... * (1 - \beta_{k-1}) * beta_k</span>
</span><span class='line'><span class="c1"># where each $\\beta\_i$ is drawn from a Beta distribution</span>
</span><span class='line'><span class="c1">#   \beta_i ~ Beta(1, \alpha)</span>
</span><span class='line'>stick_breaking_process <span class="o">=</span> <span class="kr">function</span><span class="p">(</span>num_weights<span class="p">,</span> alpha<span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  betas <span class="o">=</span> rbeta<span class="p">(</span>num_weights<span class="p">,</span> <span class="m">1</span><span class="p">,</span> alpha<span class="p">)</span>
</span><span class='line'>  remaining_stick_lengths <span class="o">=</span> c<span class="p">(</span><span class="m">1</span><span class="p">,</span> cumprod<span class="p">(</span><span class="m">1</span> <span class="o">-</span> betas<span class="p">))[</span><span class="m">1</span>:num_weights<span class="p">]</span>
</span><span class='line'>  weights <span class="o">=</span> remaining_stick_lengths <span class="o">*</span> betas
</span><span class='line'>  weights
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>And here&#8217;s some sample output:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/sbp_alpha_1.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/sbp_alpha_1.png" alt="Stick-Breaking Process, alpha = 1" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/sbp_alpha_3.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/sbp_alpha_3.png" alt="Stick-Breaking Process, alpha = 3" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/sbp_alpha_5.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/sbp_alpha_5.png" alt="Stick-Breaking Process, alpha = 5" /></a></p>

<p>Notice that for low values of alpha, the stick weights are concentrated on the first few weights (meaning our data points are concentrated on a few clusters), while the weights become more evenly dispersed as we increase alpha (meaning we posit more clusters in our data points).</p>

<h2>Dirichlet Process</h2>

<p>Suppose we run a Polya Urn Model several times, where we sample colors from a base distribution $G_0$. Each run produces a distribution of colors in the urn (say, 5% blue balls, 3% red balls, 2% pink balls, etc.), and the distribution will be different each time (for example, 5% blue balls in run 1, but 1% blue balls in run 2).</p>

<p>For example, let&#8217;s look again at the plots from above, where I generated samples from a Polya Urn Model with the standard unit normal as the base distribution:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_1.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_1.png" alt="Polya Urn Model, Alpha = 1" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_5.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_5.png" alt="Polya Urn Model, Alpha = 5" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_25.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_25.png" alt="Polya Urn Model, Alpha = 25" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_50.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/polya_alpha_50.png" alt="Polya Urn Model, Alpha = 50" /></a></p>

<p>Each run of the Polya Urn Model produces a slighly different distribution, though each is &#8220;centered&#8221; in some fashion around the standard Gaussian I used as base. In other words, the Polya Urn Model gives us a <strong>distribution over distributions</strong> (we get a distribution of ball colors, and this distribution of colors changes each time) &#8211; and so we finally get to the Dirichlet Process.</p>

<p>Formally, given a base distribution $G_0$ and a dispersion parameter $\alpha$, a sample from the Dirichlet Process $DP(G_0, \alpha)$ is a distribution $G \sim DP(G_0, \alpha)$. This sample $G$ can be thought of as a distribution of colors in a single simulation of the Polya Urn Model; sampling from $G$ gives us the balls in the urn.</p>

<p>So here&#8217;s the connection between the Chinese Restaurant Process, the Polya Urn Model, the Stick-Breaking Process, and the Dirichlet Process:</p>

<ul>
<li><strong>Dirichlet Process</strong>: Suppose we want samples $x_i \sim G$, where $G$ is a distribution sampled from the Dirichlet Process $G \sim DP(G_0, \alpha)$.</li>
<li><strong>Polya Urn Model</strong>: One way to generate these values $x_i$ would be to take a Polya Urn Model with color distribution $G_0$ and dispersion $\alpha$. ($x_i$ would be the color of the ith ball in the urn.)</li>
<li><strong>Chinese Restaurant Process</strong>: Another way to generate $x_i$ would be to first assign tables to customers according to a Chinese Restaurant Process with dispersion $\alpha$. Every customer at the nth table would then be given the same value (color) sampled from $G_0$. ($x_i$ would be the value given to the ith customer; $x_i$ can also be thought of as the food at table $i$, or as the parameters of table $i$.)</li>
<li><strong>Stick-Breaking Process</strong>: Finally, we could generate weights $w_k$ according to a Stick-Breaking Process with dispersion $\alpha$. Next, we would give each weight $w_k$ a value (or color) $v_k$ sampled from $G_0$. Finally, we would assign $x_i$ to value (color) $v_k$ with probability $w_k$.</li>
</ul>


<h1>Recap</h1>

<p>Let&#8217;s summarize what we&#8217;ve discussed so far.</p>

<p>We have a bunch of data points $p_i$ that we want to cluster, and we&#8217;ve described four essentially equivalent generative models that allow us to describe how each cluster and point could have arisen.</p>

<p>In the <strong>Chinese Restaurant Process</strong>:</p>

<ul>
<li>We generate table assignments $g_1, \ldots, g_n \sim CRP(\alpha)$ according to a Chinese Restaurant Process. ($g_i$ is the table assigned to datapoint $i$.)</li>
<li>We generate table parameters $\phi_1, \ldots, \phi_n \sim G_0$ according to the base distribution $G_0$, where $\phi_k$ is the parameter for the kth distinct group.</li>
<li>Given table assignments and table parameters, we generate each datapoint $p_i \sim F(\phi_{g_i})$ from a distribution $F$ with the specified table parameters. (For example, $F$ could be a Gaussian, and $\phi_i$ could be a parameter vector specifying the mean and standard deviation).</li>
</ul>


<p>In the <strong>Polya Urn Model</strong>:</p>

<ul>
<li>We generate colors $\phi_1, \ldots, \phi_n \sim Polya(G_0, \alpha)$ according to a Polya Urn Model. ($\phi_i$ is the color of the ith ball.)</li>
<li>Given ball colors, we generate each datapoint $p_i \sim F(\phi_i)$.</li>
</ul>


<p>In the <strong>Stick-Breaking Process</strong>:</p>

<ul>
<li>We generate group probabilities (stick lengths) $w_1, \ldots, w_{\infty} \sim Stick(\alpha)$ according to a Stick-Breaking process.</li>
<li>We generate group parameters $\phi_1, \ldots, \phi_{\infty} \sim G_0$ from $G_0$, where $\phi_k$ is the parameter for the kth distinct group.</li>
<li>We generate group assignments $g_1, \ldots, g_n \sim Multinomial(w_1, \ldots, w_{\infty})$ for each datapoint.</li>
<li>Given group assignments and group parameters, we generate each datapoint $p_i \sim F(\phi_{g_i})$.</li>
</ul>


<p>In the <strong>Dirichlet Process</strong>:</p>

<ul>
<li>We generate a distribution $G \sim DP(G_0, \alpha)$ from a Dirichlet Process with base distribution $G_0$ and dispersion parameter $\alpha$.</li>
<li>We generate group-level parameters $x_i \sim G$ from $G$, where $x_i$ is the group parameter for the ith datapoint. (Note: this is not the same as $\phi_i$. $x_i$ is the parameter associated to the group that the ith datapoint belongs to, whereas $\phi_k$ is the parameter of the kth distinct group.)</li>
<li>Given group-level parameters $x_i$, we generate each datapoint $p_i \sim F(x_i)$.</li>
</ul>


<p>Also, remember that each model naturally allows the number of clusters to grow as more points come in.</p>

<h1>Inference in the Dirichlet Process Mixture</h1>

<p>So we&#8217;ve described a generative model that allows us to calculate the probability of any particular set of group assignments to data points, but we haven&#8217;t described how to actually learn a good set of group assignments.</p>

<p>Let&#8217;s briefly do this now. Very roughly, the <strong>Gibbs sampling</strong> approach works as follows:</p>

<ul>
<li>Take the set of data points, and randomly initialize group assignments.</li>
<li>Pick a point. Fix the group assignments of all the other points, and assign the chosen point a new group (which can be either an existing cluster or a new cluster) with a CRP-ish probability (as described in the models above) that depends on the group assignments and values of all the other points.</li>
<li>We will eventually converge on a good set of group assignments, so repeat the previous step until happy.</li>
</ul>


<p>For more details, <a href="http://www.cs.toronto.edu/~radford/ftp/mixmc.pdf">this paper</a> provides a good description. Philip Resnick and Eric Hardisty also have a friendlier, more general description of Gibbs sampling (plus an application to naive Bayes) <a href="http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf">here</a>.</p>

<h1>Fast Food Application: Clustering the McDonald&#8217;s Menu</h1>

<p>Finally, let&#8217;s show an application of the Dirichlet Process Mixture. Unfortunately, I didn&#8217;t have a data set of people&#8217;s food habits offhand, so instead I took <a href="http://nutrition.mcdonalds.com/nutritionexchange/nutritionfacts.pdf">this list</a> of McDonald&#8217;s foods and nutrition facts.</p>

<p>After normalizing each item to have an equal number of calories, and representing each item as a vector of <strong>(total fat, cholesterol, sodium, dietary fiber, sugars, protein, vitamin A, vitamin C, calcium, iron, calories from fat, satured fat, trans fat, carbohydrates)</strong>, I ran <a href="http://scikit-learn.sourceforge.net/dev/index.html">scikit-learn</a>&#8217;s <a href="http://scikit-learn.sourceforge.net/dev/modules/mixture.html">Dirichlet Process Gaussian Mixture Model</a> to cluster McDonald&#8217;s menu based on nutritional value.</p>

<p>First, how does the number of clusters inferred by the Dirichlet Process mixture vary as we feed in more (randomly ordered) points?</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/num-clusters-vary.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/num-clusters-vary.png" alt="Growth of Number of Clusters" /></a></p>

<p>As expected, the Dirichlet Process model discovers more and more clusters as more and more food items arrive. (And indeed, the number of clusters appears to grow logarithmically, which can in fact be proved.)</p>

<p>How many clusters does the mixture model infer from the entire dataset? Running the Gibbs sampler several times, we find that the number of clusters tends around 11:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/num_mcdonalds_clusters.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/num_mcdonalds_clusters_small.png" alt="Number of clusters" /></a></p>

<p>Let&#8217;s dive into one of these clusterings.</p>

<p><strong>Cluster 1 (Desserts)</strong></p>

<p>Looking at a sample of foods from the first cluster, we find a lot of desserts and dessert-y drinks:</p>

<ul>
<li>Caramel Mocha</li>
<li>Frappe Caramel</li>
<li>Iced Hazelnut Latte</li>
<li>Iced Coffee</li>
<li>Strawberry Triple Thick Shake</li>
<li>Snack Size McFlurry</li>
<li>Hot Caramel Sundae</li>
<li>Baked Hot Apple Pie</li>
<li>Cinnamon Melts</li>
<li>Kiddie Cone</li>
<li>Strawberry Sundae</li>
</ul>


<p>We can also look at the nutritional profile of some foods from this cluster (after <a href="http://en.wikipedia.org/wiki/Standard_score">z-scaling</a> each nutrition dimension to have mean 0 and standard deviation 1):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster1.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster1.png" alt="Cluster 1" /></a></p>

<p>We see that foods in this cluster tend to be high in trans fat and low in vitamins, protein, fiber, and sodium.</p>

<p><strong>Cluster 2 (Sauces)</strong></p>

<p>Here&#8217;s a sample from the second cluster, which contains a lot of sauces:</p>

<ul>
<li>Hot Mustard Sauce</li>
<li>Spicy Buffalo Sauce</li>
<li>Newman&#8217;s Own Low Fat Balsamic Vinaigrette</li>
</ul>


<p>And looking at the nutritional profile of points in this cluster, we see that it&#8217;s heavy in sodium and fat:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster2.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster2.png" alt="Cluster 2" /></a></p>

<p><strong>Cluster 3 (Burgers, Crispy Foods, High-Cholesterol)</strong></p>

<p>The third cluster is very burgery:</p>

<ul>
<li>Hamburger</li>
<li>Cheeseburger</li>
<li>Filet-O-Fish</li>
<li>Quarter Pounder with Cheese</li>
<li>Premium Grilled Chicken Club Sandwich</li>
<li>Ranch Snack Wrap</li>
<li>Premium Asian Salad with Crispy Chicken</li>
<li>Butter Garlic Croutons</li>
<li>Sausage McMuffin</li>
<li>Sausage McGriddles</li>
</ul>


<p>It&#8217;s also high in fat and sodium, and low in carbs and sugar</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster3.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster3.png" alt="Cluster 3" /></a></p>

<p><strong>Cluster 4 (Creamy Sauces)</strong></p>

<p>Interestingly, even though we already found a cluster of sauces above, we discover another one as well. These sauces appear to be much more cream-based:</p>

<ul>
<li>Creamy Ranch Sauce</li>
<li>Newman&#8217;s Own Creamy Caesar Dressing</li>
<li>Coffee Cream</li>
<li>Iced Coffee with Sugar Free Vanilla Syrup</li>
</ul>


<p>Nutritionally, these sauces are higher in calories from fat, and much lower in sodium:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster4.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster4.png" alt="Cluster 4" /></a></p>

<p><strong>Cluster 5 (Salads)</strong></p>

<p>Here&#8217;s a salad cluster. A lot of salads also appeared in the third cluster (along with hamburgers and McMuffins), but that&#8217;s because those salads also all contained crispy chicken. The salads in this cluster are either crisp-free or have their chicken grilled instead:</p>

<ul>
<li>Premium Southwest Salad with Grilled Chicken</li>
<li>Premium Caesar Salad with Grilled Chicken</li>
<li>Side Salad</li>
<li>Premium Asian Salad without Chicken</li>
<li>Premium Bacon Ranch Salad without Chicken</li>
</ul>


<p>This is reflected in the higher content of iron, vitamin A, and fiber:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster5.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster5.png" alt="Cluster 5" /></a></p>

<p><strong>Cluster 6 (More Sauces)</strong></p>

<p>Again, we find another cluster of sauces:</p>

<ul>
<li>Ketchup Packet</li>
<li>Barbeque Sauce</li>
<li>Chipotle Barbeque Sauce</li>
</ul>


<p>These are still high in sodium, but much lower in fat compared to the other sauce clusters:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster6.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster6.png" alt="Cluster 6" /></a></p>

<p><strong>Cluster 7 (Fruit and Maple Oatmeal)</strong></p>

<p>Amusingly, fruit and maple oatmeal is in a cluster by itself:</p>

<ul>
<li>Fruit &amp; Maple Oatmeal</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster7.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster7.png" alt="Cluster 7" /></a></p>

<p><strong>Cluster 8 (Sugary Drinks)</strong></p>

<p>We also get a cluster of sugary drinks:</p>

<ul>
<li>Strawberry Banana Smoothie</li>
<li>Wild Berry Smoothie</li>
<li>Iced Nonfat Vanilla Latte</li>
<li>Nonfat Hazelnut</li>
<li>Nonfat Vanilla Cappuccino</li>
<li>Nonfat Caramel Cappuccino</li>
<li>Sweet Tea</li>
<li>Frozen Strawberry Lemonade</li>
<li>Coca-Cola</li>
<li>Minute Maid Orange Juice</li>
</ul>


<p>In addition to high sugar content, this cluster is also high in carbohydrates and calcium, and low in fat.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster8.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster8.png" alt="Cluster 8" /></a></p>

<p><strong>Cluster 9 (Breakfast Foods)</strong></p>

<p>Here&#8217;s a cluster of high-cholesterol breakfast foods:</p>

<ul>
<li>Sausage McMuffin with Egg</li>
<li>Sausage Burrito</li>
<li>Egg McMuffin</li>
<li>Bacon, Egg &amp; Chees Biscuit</li>
<li>McSkillet Burrito with Sausage</li>
<li>Big Breakfast with Hotcakes</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster9.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster9.png" alt="Cluster 9" /></a></p>

<p><strong>Cluster 10 (Coffee Drinks)</strong></p>

<p>We find a group of coffee drinks next:</p>

<ul>
<li>Nonfat Cappuccino</li>
<li>Nonfat Latte</li>
<li>Nonfat Latte with Sugar Free Vanilla Syrup</li>
<li>Iced Nonfat Latte</li>
</ul>


<p>These are much higher in calcium and protein, and lower in sugar, than the other drink cluster above:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster11.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster11.png" alt="Cluster 11" /></a></p>

<p><strong>Cluster 11 (Apples)</strong></p>

<p>Here&#8217;s a cluster of apples:</p>

<ul>
<li>Apple Dippers with Low Fat Caramel Dip</li>
<li>Apple Slices</li>
</ul>


<p>Vitamin C, check.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster10.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/cluster10.png" alt="Cluster 10" /></a></p>

<p>And finally, here&#8217;s an overview of all the clusters at once (using a different clustering run):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/dirichlet-process/all-clusters.png"><img src="http://dl.dropbox.com/u/10506/blog/dirichlet-process/all-clusters-small.png" alt="All Clusters" /></a></p>

<h1>No More!</h1>

<p>I&#8217;ll end with a couple notes:</p>

<ul>
<li>Kevin Knight has a <a href="http://www.isi.edu/natural-language/people/bayes-with-tears.pdf">hilarious introduction</a> to Bayesian inference that describes some applications of nonparametric Bayesian techniques to computational linguistics (though I don&#8217;t think he ever quite says &#8220;nonparametric Bayes&#8221; directly).</li>
<li>In the Chinese Restaurant Process, each customer sits at a single table. The <a href="http://en.wikipedia.org/wiki/Chinese_restaurant_process#The_Indian_buffet_process">Indian Buffet Process</a> is an extension that allows customers to sample food from multiple tables (i.e., belong to multiple clusters).</li>
<li>The Chinese Restaurant Process, the Polya Urn Model, and the Stick-Breaking Process are all <em>sequential</em> models for generating groups: to figure out table parameters in the CRP, for example, you wait for customer 1 to come in, then customer 2, then customer 3, and so on. The equivalent Dirichlet Process, on the other hand, is a <em>parallel</em> model for generating groups: just sample $G \sim DP(G_0, alpha)$, and then all your group parameters can be independently generated by sampling from $G$ at once. This duality is an instance of a more general phenomenon known as <a href="http://en.wikipedia.org/wiki/De_Finetti's_theorem">de Finetti&#8217;s theorem</a>.</li>
</ul>


<p>And that&#8217;s it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Instant interactive visualization with d3 + ggplot2]]></title>
    <link href="http://blog.echen.me/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/"/>
    <updated>2012-03-05T09:37:00-08:00</updated>
    <id>http://blog.echen.me/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2</id>
    <content type="html"><![CDATA[<p>It&#8217;s often easier to understand a chart than a table. So why is it still so hard to make a simple data graphic, and why am I still bombarded by mind-numbing reams of raw <em>numbers</em>?</p>

<p>(Yeah, I love <a href="http://blog.echen.me/2012/01/17/quick-introduction-to-ggplot2/">ggplot2</a> to death. But sometimes I want a little more interaction, and sometimes all I want is to drag-and-drop and be done.)</p>

<p>So I&#8217;ve been experimenting with <a href="http://minifolds.herokuapp.com/graphs/1?x=health&amp;y=speed&amp;size=intelligence&amp;color=age&amp;group=height">a small, ggplot2-inspired d3 app</a>.</p>

<p>Simply drop a file, and bam! Instant scatterplot:</p>

<p><a href="http://minifolds.herokuapp.com/graphs/1?x=health&amp;y=speed"><img src="http://dl.dropbox.com/u/10506/blog/minifolds/swiss-roll-bw.png" alt="Swiss Roll B&amp;W" /></a></p>

<p>But wait &#8211; that&#8217;s only 2 dimensions. You can add some more through color, size, and groups:</p>

<p><a href="http://minifolds.herokuapp.com/graphs/1?x=health&amp;y=speed&amp;size=intelligence&amp;color=age&amp;group=height"><img src="http://dl.dropbox.com/u/10506/blog/minifolds/swiss-roll-edit.png" alt="Swiss Roll Edit" /></a></p>

<p>(Click <a href="http://minifolds.herokuapp.com/graphs/1?x=health&amp;y=speed&amp;size=intelligence&amp;color=age&amp;group=height">here</a> to play with the data yourself.)</p>

<p>And you can easily switch which variables are getting plotted, and see all the information associated with each point.</p>

<p><a href="http://minifolds.herokuapp.com/graphs/1?x=weight&amp;y=speed&amp;size=health&amp;color=age&amp;group=height"><img src="http://dl.dropbox.com/u/10506/blog/minifolds/swiss-roll-pivot.png" alt="Swiss Roll Pivot" /></a></p>

<p>(Same dataset, different aesthetic assignments.)</p>

<p>I&#8217;m thinking of adding more kinds of charts, support for categorical variables, more interactivity (sliders to interact with other dimensions?!), and making the UI even easier (e.g., simplify column naming). In the meantime, the code is <a href="https://github.com/echen/minifolds">here</a> on Github, and tips and suggestions are welcome!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Movie recommendations and more via MapReduce and Scalding]]></title>
    <link href="http://blog.echen.me/2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding/"/>
    <updated>2012-02-09T03:20:00-08:00</updated>
    <id>http://blog.echen.me/2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding</id>
    <content type="html"><![CDATA[<p><em>Scalding is an in-house MapReduce framework that Twitter recently open-sourced. Like <a href="http://pig.apache.org/">Pig</a>, it provides an abstraction on top of MapReduce that makes it easy to write big data jobs in a syntax that&#8217;s simple and concise. Unlike Pig, Scalding is written in pure Scala &#8211; which means all the power of Scala and the JVM is already built-in. No more UDFs, folks!</em></p>

<p>This is going to be an in-your-face introduction to <a href="https://github.com/twitter/scalding">Scalding</a>, Twitter&#8217;s (Scala + Cascading) MapReduce framework.</p>

<p>In 140: instead of forcing you to write raw <code>map</code> and <code>reduce</code> functions, Scalding allows you to write <em>natural</em> code like</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Create a histogram of tweet lengths.</span>
</span><span class='line'><span class="n">tweets</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="-Symbol">&#39;tweet</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;length</span><span class="o">)</span> <span class="o">{</span> <span class="n">tweet</span> <span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="n">tweet</span><span class="o">.</span><span class="n">size</span> <span class="o">}.</span><span class="n">groupBy</span><span class="o">(</span><span class="-Symbol">&#39;length</span><span class="o">)</span> <span class="o">{</span> <span class="n">_</span><span class="o">.</span><span class="n">size</span> <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Not much different from the Ruby you&#8217;d write to compute tweet distributions over <em>small</em> data? <strong>Exactly.</strong></p>

<p>Two notes before we begin:</p>

<ul>
<li><a href="https://github.com/echen/scaldingale">This Github repository</a> contains all the code used.</li>
<li>For a gentler introduction to Scalding, see <a href="https://github.com/twitter/scalding/wiki/Getting-Started">this Getting Started guide</a> on the Scalding wiki.</li>
</ul>


<h1>Movie Similarities</h1>

<p>Imagine you run an online movie business, and you want to generate movie recommendations. You have a rating system (people can rate movies with 1 to 5 stars), and we&#8217;ll assume for simplicity that all of the ratings are stored in a TSV file somewhere.</p>

<p>Let&#8217;s start by reading the ratings into a Scalding job.</p>

<figure class='code'><figcaption><span>Input </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * The input is a TSV file with three columns: (user, movie, rating).</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">val</span> <span class="nc">INPUT_FILENAME</span> <span class="k">=</span> <span class="s">&quot;data/ratings.tsv&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Read in the input and give each field a type and name.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">val</span> <span class="n">ratings</span> <span class="k">=</span> <span class="nc">Tsv</span><span class="o">(</span><span class="nc">INPUT_FILENAME</span><span class="o">,</span> <span class="o">(</span><span class="-Symbol">&#39;user</span><span class="o">,</span> <span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Let&#39;s also keep track of the total number of people who rated each movie.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">val</span> <span class="n">numRaters</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">ratings</span>
</span><span class='line'>    <span class="c1">// Put the number of people who rated each movie into a field called &quot;numRaters&quot;.    </span>
</span><span class='line'>    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="-Symbol">&#39;movie</span><span class="o">)</span> <span class="o">{</span> <span class="n">_</span><span class="o">.</span><span class="n">size</span> <span class="o">}.</span><span class="n">rename</span><span class="o">(</span><span class="-Symbol">&#39;size</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;numRaters</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// Merge `ratings` with `numRaters`, by joining on their movie fields.</span>
</span><span class='line'><span class="k">val</span> <span class="n">ratingsWithSize</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">ratings</span><span class="o">.</span><span class="n">joinWithSmaller</span><span class="o">(</span><span class="-Symbol">&#39;movie</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="n">numRaters</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// ratingsWithSize now contains the following fields: (user, movie, rating, numRaters).</span>
</span></code></pre></td></tr></table></div></figure>


<p>You want to calculate how similar pairs of movies are, so that if someone watches <em>The Lion King</em>, you can recommend films like <em>Toy Story</em>. So how should you define the similarity between two movies?</p>

<p>One way is to use their <strong>correlation</strong>:</p>

<ul>
<li>For every pair of movies A and B, find all the people who rated both A and B.</li>
<li>Use these ratings to form a Movie A vector and a Movie B vector.</li>
<li>Calculate the correlation between these two vectors.</li>
<li>Whenever someone watches a movie, you can then recommend the movies most correlated with it.</li>
</ul>


<p>Let&#8217;s start with the first two steps.</p>

<figure class='code'><figcaption><span>Find rating pairs </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * To get all pairs of co-rated movies, we&#39;ll join `ratings` against itself.</span>
</span><span class='line'><span class="cm"> * So first make a dummy copy of the ratings that we can join against.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">val</span> <span class="n">ratings2</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">ratingsWithSize</span>
</span><span class='line'>    <span class="o">.</span><span class="n">rename</span><span class="o">((</span><span class="-Symbol">&#39;user</span><span class="o">,</span> <span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">,</span> <span class="-Symbol">&#39;numRaters</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;user2</span><span class="o">,</span> <span class="-Symbol">&#39;movie2</span><span class="o">,</span> <span class="-Symbol">&#39;rating2</span><span class="o">,</span> <span class="-Symbol">&#39;numRaters2</span><span class="o">))</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Now find all pairs of co-rated movies (pairs of movies that a user has rated) by</span>
</span><span class='line'><span class="cm"> * joining the duplicate rating streams on their user fields, </span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">val</span> <span class="n">ratingPairs</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">ratingsWithSize</span>
</span><span class='line'>    <span class="o">.</span><span class="n">joinWithSmaller</span><span class="o">(</span><span class="-Symbol">&#39;user</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;user2</span><span class="o">,</span> <span class="n">ratings2</span><span class="o">)</span>
</span><span class='line'>    <span class="c1">// De-dupe so that we don&#39;t calculate similarity of both (A, B) and (B, A).</span>
</span><span class='line'>    <span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;movie2</span><span class="o">)</span> <span class="o">{</span> <span class="n">movies</span> <span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">movies</span><span class="o">.</span><span class="n">_1</span> <span class="o">&lt;</span> <span class="n">movies</span><span class="o">.</span><span class="n">_2</span> <span class="o">}</span>
</span><span class='line'>    <span class="o">.</span><span class="n">project</span><span class="o">(</span><span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">,</span> <span class="-Symbol">&#39;numRaters</span><span class="o">,</span> <span class="-Symbol">&#39;movie2</span><span class="o">,</span> <span class="-Symbol">&#39;rating2</span><span class="o">,</span> <span class="-Symbol">&#39;numRaters2</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// By grouping on (&#39;movie, &#39;movie2), we can now get all the people who rated any pair of movies.</span>
</span></code></pre></td></tr></table></div></figure>


<p>Before using these rating pairs to calculate correlation, let&#8217;s stop for a bit.</p>

<p>Since we&#8217;re explicitly thinking of movies as <strong>vectors</strong> of ratings, it&#8217;s natural to compute some very vector-y things like norms and dot products, as well as the length of each vector and the sum over all elements in each vector. So let&#8217;s compute these:</p>

<figure class='code'><figcaption><span>Vector calculations </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/**</span>
</span><span class='line'><span class="cm"> * Compute dot products, norms, sums, and sizes of the rating vectors.</span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">val</span> <span class="n">vectorCalcs</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">ratingPairs</span>
</span><span class='line'>    <span class="c1">// Compute (x*y, x^2, y^2), which we need for dot products and norms.</span>
</span><span class='line'>    <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="-Symbol">&#39;rating</span><span class="o">,</span> <span class="-Symbol">&#39;rating2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;ratingProd</span><span class="o">,</span> <span class="-Symbol">&#39;ratingSq</span><span class="o">,</span> <span class="-Symbol">&#39;rating2Sq</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">ratings</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Double</span><span class="o">,</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="o">(</span><span class="n">ratings</span><span class="o">.</span><span class="n">_1</span> <span class="o">*</span> <span class="n">ratings</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">ratings</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="mi">2</span><span class="o">),</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="n">ratings</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="mi">2</span><span class="o">))</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>    <span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;movie2</span><span class="o">)</span> <span class="o">{</span> <span class="n">group</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="n">group</span><span class="o">.</span><span class="n">size</span> <span class="c1">// length of each vector</span>
</span><span class='line'>        <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="-Symbol">&#39;ratingProd</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">)</span>
</span><span class='line'>        <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="-Symbol">&#39;rating</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;ratingSum</span><span class="o">)</span>
</span><span class='line'>        <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="-Symbol">&#39;rating2</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;rating2Sum</span><span class="o">)</span>
</span><span class='line'>        <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="-Symbol">&#39;ratingSq</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;ratingNormSq</span><span class="o">)</span>
</span><span class='line'>        <span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="-Symbol">&#39;rating2Sq</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;rating2NormSq</span><span class="o">)</span>
</span><span class='line'>        <span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="-Symbol">&#39;numRaters</span><span class="o">)</span> <span class="c1">// Just an easy way to make sure the numRaters field stays.</span>
</span><span class='line'>        <span class="o">.</span><span class="n">max</span><span class="o">(</span><span class="-Symbol">&#39;numRaters2</span><span class="o">)</span>                
</span><span class='line'>        <span class="c1">// All of these operations chain together like in a builder object.</span>
</span><span class='line'>    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>To summarize, each row in <code>vectorCalcs</code> now contains the following fields:</p>

<ul>
<li><strong>movie, movie2</strong></li>
<li><strong>numRaters, numRaters2</strong>: the total number of people who rated each movie</li>
<li><strong>size</strong>: the number of people who rated both movie and movie2</li>
<li><strong>dotProduct</strong>: dot product between the movie vector (a vector of ratings) and the movie2 vector (also a vector of ratings)</li>
<li><strong>ratingSum, rating2sum</strong>: sum over all elements in each ratings vector</li>
<li><strong>ratingNormSq, rating2Normsq</strong>: squared norm of each vector</li>
</ul>


<p>So let&#8217;s go back to calculating the correlation between movie and movie2. We could, of course, calculate correlation in the standard way: find the covariance between the movie and movie2 ratings, and divide by their standard deviations.</p>

<p>But recall that we can also write correlation in the following form:</p>

<p>$Corr(X, Y) = \frac{n \sum xy - \sum x \sum y}{\sqrt{n \sum x^2 - (\sum x)^2} \sqrt{n \sum y^2 - (\sum y)^2}}$</p>

<p>(See the <a href="http://en.wikipedia.org/wiki/Correlation_and_dependence">Wikipedia page</a> on correlation.)</p>

<p>Notice that every one of the elements in this formula is a field in <code>vectorCalcs</code>! So instead of using the standard calculation, we can use this form instead:</p>

<figure class='code'><figcaption><span>Correlation </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">correlations</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">vectorCalcs</span>
</span><span class='line'>    <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="-Symbol">&#39;size</span><span class="o">,</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">,</span> <span class="-Symbol">&#39;ratingSum</span><span class="o">,</span> <span class="-Symbol">&#39;rating2Sum</span><span class="o">,</span> <span class="-Symbol">&#39;ratingNormSq</span><span class="o">,</span> <span class="-Symbol">&#39;rating2NormSq</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="-Symbol">&#39;correlation</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">fields</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Double</span><span class="o">,</span> <span class="kt">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>      <span class="n">correlation</span><span class="o">(</span><span class="n">fields</span><span class="o">.</span><span class="n">_1</span><span class="o">,</span> <span class="n">fields</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">fields</span><span class="o">.</span><span class="n">_3</span><span class="o">,</span> <span class="n">fields</span><span class="o">.</span><span class="n">_4</span><span class="o">,</span> <span class="n">fields</span><span class="o">.</span><span class="n">_5</span><span class="o">,</span> <span class="n">fields</span><span class="o">.</span><span class="n">_6</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="n">correlation</span><span class="o">(</span><span class="n">size</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProduct</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">ratingSum</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span>
</span><span class='line'>  <span class="n">rating2Sum</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">ratingNormSq</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">rating2NormSq</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">numerator</span> <span class="k">=</span> <span class="n">size</span> <span class="o">*</span> <span class="n">dotProduct</span> <span class="o">-</span> <span class="n">ratingSum</span> <span class="o">*</span> <span class="n">rating2Sum</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">denominator</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">ratingNormSq</span> <span class="o">-</span> <span class="n">ratingSum</span> <span class="o">*</span> <span class="n">ratingSum</span><span class="o">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">rating2NormSq</span> <span class="o">-</span> <span class="n">rating2Sum</span> <span class="o">*</span> <span class="n">rating2Sum</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>And that&#8217;s it! To see the full code, check out the Github repository <a href="https://github.com/echen/scaldingale">here</a>.</p>

<h1>Book Similarities</h1>

<p>Let&#8217;s run this code over some real data. Unfortunately, I didn&#8217;t have a clean source of movie ratings available, so instead I used <a href="http://www.informatik.uni-freiburg.de/~cziegler/BX/">this dataset</a> of 1 million book ratings.</p>

<p>I ran a quick command, using the handy <a href="https://github.com/twitter/scalding/wiki/Scald.rb">scald.rb script</a> that Scalding provides&#8230;</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># Send the job off to a Hadoop cluster</span>
</span><span class='line'>scald.rb MovieSimilarities.scala --input ratings.tsv --output similarities.tsv
</span></code></pre></td></tr></table></div></figure>


<p>&#8230;and here&#8217;s a sample of the top output I got:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/top-book-crossing-sims-correlation.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/top-book-crossing-sims-correlation.png" alt="Top Book-Crossing Pairs" /></a></p>

<p>As we&#8217;d expect, we see that</p>

<ul>
<li><em>Harry Potter</em> books are similar to other <em>Harry Potter</em> books</li>
<li><em>Lord of the Rings</em> books are similar to other <em>Lord of the Rings</em> books</li>
<li>Tom Clancy is similar to John Grisham</li>
<li>Chick lit (<em>Summer Sisters</em>, by Judy Blume) is similar to chick lit (<em>Bridget Jones</em>)</li>
</ul>


<p>Just for fun, let&#8217;s also look at books similar to <em>The Great Gatsby</em>:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/great-gatsby-correlation.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/great-gatsby-correlation.png" alt="Great Gatsby" /></a></p>

<p>(Schoolboy memories, exactly.)</p>

<h1>More Similarity Measures</h1>

<p>Of course, there are lots of other similarity measures we could use besides correlation.</p>

<h2>Cosine Similarity</h2>

<p><a href="http://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a> is a another common vector-based similarity measure.</p>

<figure class='code'><figcaption><span>Cosine Similarity </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">cosineSimilarity</span><span class="o">(</span><span class="n">dotProduct</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">ratingNorm</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">rating2Norm</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">dotProduct</span> <span class="o">/</span> <span class="o">(</span><span class="n">ratingNorm</span> <span class="o">*</span> <span class="n">rating2Norm</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Correlation, Take II</h2>

<p>We can also also add a <em>regularized</em> correlation, by (say) adding N virtual movie pairs that have zero correlation. This helps avoid noise if some movie pairs have very few raters in common (for example, <em>The Great Gatsby</em> had an unlikely raw correlation of 1 with many other books, due simply to the fact that those book pairs had very few ratings).</p>

<figure class='code'><figcaption><span>Regularized Correlation </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">regularizedCorrelation</span><span class="o">(</span><span class="n">size</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">dotProduct</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">ratingSum</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span>
</span><span class='line'>  <span class="n">rating2Sum</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">ratingNormSq</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">rating2NormSq</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span>
</span><span class='line'>  <span class="n">virtualCount</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">priorCorrelation</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">unregularizedCorrelation</span> <span class="k">=</span> <span class="n">correlation</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">,</span> <span class="n">ratingSum</span><span class="o">,</span> <span class="n">rating2Sum</span><span class="o">,</span> <span class="n">ratingNormSq</span><span class="o">,</span> <span class="n">rating2NormSq</span><span class="o">)</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">w</span> <span class="k">=</span> <span class="n">size</span> <span class="o">/</span> <span class="o">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">virtualCount</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">w</span> <span class="o">*</span> <span class="n">unregularizedCorrelation</span> <span class="o">+</span> <span class="o">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">w</span><span class="o">)</span> <span class="o">*</span> <span class="n">priorCorrelation</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Jaccard Similarity</h2>

<p>Recall that <a href="http://blog.echen.me/blog/2011/10/24/winning-the-netflix-prize-a-summary/">one of the lessons of the Netflix prize</a> was that implicit data can be quite useful &#8211; the mere fact that you rate a James Bond movie, even if you rate it quite horribly, suggests that you&#8217;d probably be interested in similar action films. So we can also ignore the value itself of each rating and use a <em>set</em>-based similarity measure like <a href="http://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity</a>.</p>

<figure class='code'><figcaption><span>Jaccard Similarity </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">jaccardSimilarity</span><span class="o">(</span><span class="n">usersInCommon</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">totalUsers1</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">totalUsers2</span> <span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">union</span> <span class="k">=</span> <span class="n">totalUsers1</span> <span class="o">+</span> <span class="n">totalUsers2</span> <span class="o">-</span> <span class="n">usersInCommon</span>
</span><span class='line'>  <span class="n">usersInCommon</span> <span class="o">/</span> <span class="n">union</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Incorporation</h2>

<p>Finally, let&#8217;s add all these similarity measures to our output.</p>

<figure class='code'><figcaption><span>Similarity Measures </span><a href='https://github.com/echen/scaldingale/blob/master/MovieSimilarities.scala'>MovieSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="nc">PRIOR_COUNT</span> <span class="k">=</span> <span class="mi">10</span>
</span><span class='line'><span class="k">val</span> <span class="nc">PRIOR_CORRELATION</span> <span class="k">=</span> <span class="mi">0</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">similarities</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">vectorCalcs</span>
</span><span class='line'>    <span class="o">.</span><span class="n">map</span><span class="o">((</span><span class="-Symbol">&#39;size</span><span class="o">,</span> <span class="-Symbol">&#39;dotProduct</span><span class="o">,</span> <span class="-Symbol">&#39;ratingSum</span><span class="o">,</span> <span class="-Symbol">&#39;rating2Sum</span><span class="o">,</span> <span class="-Symbol">&#39;ratingNormSq</span><span class="o">,</span> <span class="-Symbol">&#39;rating2NormSq</span><span class="o">,</span> <span class="-Symbol">&#39;numRaters</span><span class="o">,</span> <span class="-Symbol">&#39;numRaters2</span><span class="o">)</span> <span class="o">-&gt;</span>
</span><span class='line'>      <span class="o">(</span><span class="-Symbol">&#39;correlation</span><span class="o">,</span> <span class="-Symbol">&#39;regularizedCorrelation</span><span class="o">,</span> <span class="-Symbol">&#39;cosineSimilarity</span><span class="o">,</span> <span class="-Symbol">&#39;jaccardSimilarity</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>      <span class="n">fields</span> <span class="k">:</span> <span class="o">(</span><span class="kt">Double</span><span class="o">,</span> <span class="kt">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">,</span> <span class="nc">Double</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>
</span><span class='line'>      <span class="k">val</span> <span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">,</span> <span class="n">ratingSum</span><span class="o">,</span> <span class="n">rating2Sum</span><span class="o">,</span> <span class="n">ratingNormSq</span><span class="o">,</span> <span class="n">rating2NormSq</span><span class="o">,</span> <span class="n">numRaters</span><span class="o">,</span> <span class="n">numRaters2</span><span class="o">)</span> <span class="k">=</span> <span class="n">fields</span>
</span><span class='line'>
</span><span class='line'>      <span class="k">val</span> <span class="n">corr</span> <span class="k">=</span> <span class="n">correlation</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">,</span> <span class="n">ratingSum</span><span class="o">,</span> <span class="n">rating2Sum</span><span class="o">,</span> <span class="n">ratingNormSq</span><span class="o">,</span> <span class="n">rating2NormSq</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">regCorr</span> <span class="k">=</span> <span class="n">regularizedCorrelation</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">dotProduct</span><span class="o">,</span> <span class="n">ratingSum</span><span class="o">,</span> <span class="n">rating2Sum</span><span class="o">,</span> <span class="n">ratingNormSq</span><span class="o">,</span> <span class="n">rating2NormSq</span><span class="o">,</span> <span class="nc">PRIOR_COUNT</span><span class="o">,</span> <span class="nc">PRIOR_CORRELATION</span><span class="o">)</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">cosSim</span> <span class="k">=</span> <span class="n">cosineSimilarity</span><span class="o">(</span><span class="n">dotProduct</span><span class="o">,</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">ratingNormSq</span><span class="o">),</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">rating2NormSq</span><span class="o">))</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">jaccard</span> <span class="k">=</span> <span class="n">jaccardSimilarity</span><span class="o">(</span><span class="n">size</span><span class="o">,</span> <span class="n">numRaters</span><span class="o">,</span> <span class="n">numRaters2</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>      <span class="o">(</span><span class="n">corr</span><span class="o">,</span> <span class="n">regCorr</span><span class="o">,</span> <span class="n">cosSim</span><span class="o">,</span> <span class="n">jaccard</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h1>Book Similarities Revisited</h1>

<p>Let&#8217;s take another look at the book similarities above, now that we have these new fields.</p>

<p>Here are some of the top Book-Crossing pairs, sorted by their shrunk correlation:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/top-book-crossing-sims.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/top-book-crossing-sims.png" alt="Top Book-Crossing Pairs" /></a></p>

<p>Notice how regularization affects things: the <em>Dark Tower</em> pair has a pretty high raw correlation, but relatively few ratings (reducing our confidence in the raw correlation), so it ends up below the others.</p>

<p>And here are books similar to <em>The Great Gatsby</em>, this time ordered by cosine similarity:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/great-gatsby.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/great-gatsby.png" alt="Great Gatsby" /></a></p>

<h1>Input Abstraction</h1>

<p>So our code right now is tied to our specific <code>ratings.tsv</code> input. But what if we change the way we store our ratings, or what if we want to generate similarities for something entirely different?</p>

<p>Let&#8217;s abstract away our input. We&#8217;ll create a <a href="https://github.com/echen/scaldingale/blob/master/VectorSimilarities.scala">VectorSimilarities class</a> that represents input data in the following format:</p>

<figure class='code'><figcaption><span>Input abstraction </span><a href='https://github.com/echen/scaldingale/blob/master/VectorSimilarities.scala'>VectorSimilarities.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// This is an abstract method that returns a Pipe (aka, a stream of rating tuples).</span>
</span><span class='line'><span class="c1">// It takes in three symbols that name the user, item, and rating fields.</span>
</span><span class='line'><span class="k">def</span> <span class="n">input</span><span class="o">(</span><span class="n">userField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">itemField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">ratingField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Pipe</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">ratings</span> <span class="k">=</span> <span class="n">input</span><span class="o">(</span><span class="-Symbol">&#39;user</span><span class="o">,</span> <span class="-Symbol">&#39;item</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">)</span>
</span><span class='line'><span class="c1">// ...</span>
</span><span class='line'><span class="c1">// The rest of the code remains essentially the same.</span>
</span></code></pre></td></tr></table></div></figure>


<p>Whenever we want to define a new input format, we simply subclass <code>VectorSimilarities</code> and provide a concrete implementation of the <code>input</code> method.</p>

<h2>Book-Crossings</h2>

<p>For example, here&#8217;s a class I could have used to generate the book recommendations above:</p>

<figure class='code'><figcaption><span>BookCrossing similarities </span><a href='https://github.com/echen/scaldingale/blob/master/BookCrossing.scala'>BookCrossing.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">BookCrossing</span><span class="o">(</span><span class="n">args</span> <span class="k">:</span> <span class="kt">Args</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">VectorSimilarities</span><span class="o">(</span><span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">input</span><span class="o">(</span><span class="n">userField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">itemField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">ratingField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Pipe</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">bookCrossingRatings</span> <span class="k">=</span>
</span><span class='line'>      <span class="nc">Tsv</span><span class="o">(</span><span class="s">&quot;book-crossing-ratings.tsv&quot;</span><span class="o">)</span>
</span><span class='line'>        <span class="o">.</span><span class="n">read</span>
</span><span class='line'>        <span class="o">.</span><span class="n">mapTo</span><span class="o">((</span><span class="mi">0</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">))</span> <span class="o">{</span> <span class="n">fields</span> <span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">String</span><span class="o">,</span> <span class="nc">Double</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">fields</span> <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">bookCrossingRatings</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The input method simply reads from a TSV file and lets the <code>VectorSimilarities</code> superclass do all the work. Instant recommendations, BOOM.</p>

<h2>Song Similarities with Twitter + iTunes</h2>

<p>But why limit ourselves to books? We do, after all, have Twitter at our fingertips&#8230;</p>

<blockquote class="twitter-tweet"><p>rated Born This Way by Lady GaGa 5 stars <a href="http://t.co/wTYAwWqm" title="http://itun.es/iSg92N">itun.es/iSg92N</a> <a href="https://twitter.com/search/%2523iTunes">#iTunes</a></p>&mdash; gggf (@GalMusic92) <a href="https://twitter.com/GalMusic92/status/167267017865428996" data-datetime="2012-02-08T15:22:19+00:00">February 8, 2012</a></blockquote>


<script src="http://blog.echen.me//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Since iTunes lets you send a tweet whenever you rate a song, we can use these to generate music recommendations!</p>

<p>Again, we create a new class that overrides the abstract <code>input</code> defined in <code>VectorSimilarities</code>&#8230;</p>

<figure class='code'><figcaption><span>Song similarities with Twitter + iTunes </span><a href='https://github.com/echen/scaldingale/blob/master/ITunes.scala'>ITunes.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">ITunes</span><span class="o">(</span><span class="n">args</span> <span class="k">:</span> <span class="kt">Args</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">VectorSimilarities</span><span class="o">(</span><span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// Example tweet:</span>
</span><span class='line'>  <span class="c1">// rated New Kids On the Block: Super Hits by New Kids On the Block 5 stars http://itun.es/iSg3Fc #iTunes</span>
</span><span class='line'>  <span class="k">val</span> <span class="nc">ITUNES_REGEX</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;rated (.+?) by (.+?) (\d) stars .*? #iTunes&quot;&quot;&quot;</span><span class="o">.</span><span class="n">r</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">input</span><span class="o">(</span><span class="n">userField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">itemField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">ratingField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Pipe</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">itunesRatings</span> <span class="k">=</span>
</span><span class='line'>      <span class="c1">// This is a Twitter-internal tweet source, but you could just as easily scrape </span>
</span><span class='line'>      <span class="c1">// Twitter yourself and provide your own source of tweets: https://dev.twitter.com/docs</span>
</span><span class='line'>      <span class="nc">TweetSource</span><span class="o">()</span>
</span><span class='line'>        <span class="o">.</span><span class="n">mapTo</span><span class="o">(</span><span class="-Symbol">&#39;userId</span><span class="o">,</span> <span class="-Symbol">&#39;text</span><span class="o">)</span> <span class="o">{</span> <span class="n">s</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="n">getUserId</span><span class="o">,</span> <span class="n">s</span><span class="o">.</span><span class="n">getText</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="-Symbol">&#39;text</span><span class="o">)</span> <span class="o">{</span> <span class="n">text</span> <span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="n">text</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="s">&quot;#iTunes&quot;</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="-Symbol">&#39;text</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;song</span><span class="o">,</span> <span class="-Symbol">&#39;artist</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">text</span> <span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span>
</span><span class='line'>          <span class="nc">ITUNES_REGEX</span><span class="o">.</span><span class="n">findFirstMatchIn</span><span class="o">(</span><span class="n">text</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">_</span><span class="o">.</span><span class="n">subgroups</span> <span class="o">}.</span><span class="n">map</span> <span class="o">{</span> <span class="n">l</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">l</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">l</span><span class="o">(</span><span class="mi">1</span><span class="o">),</span> <span class="n">l</span><span class="o">(</span><span class="mi">2</span><span class="o">))</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">rename</span><span class="o">((</span><span class="-Symbol">&#39;userId</span><span class="o">,</span> <span class="-Symbol">&#39;song</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">))</span>
</span><span class='line'>        <span class="o">.</span><span class="n">project</span><span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">itunesRatings</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>&#8230;and snap! Here are some songs you might like if you recently listened to <strong>Beyoncé</strong>:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/beyonce.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/beyonce.png" alt="Jason Mraz" /></a></p>

<p>And some recommended songs if you like <strong>Lady Gaga</strong>:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/lady-gaga.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/lady-gaga.png" alt="Lady Gaga" /></a></p>

<p>GG Pandora.</p>

<h2>Location Similarities with Foursquare Check-ins</h2>

<p>But what if we don&#8217;t have explicit ratings? For example, we could be a news site that wants to generate article recommendations, and maybe we only have user <em>visits</em> on each story.</p>

<p>Or what if we want to generate restaurant or tourist recommendations, when all we know is who visits each location?</p>

<blockquote class="twitter-tweet"><p>I&#8217;m at Empire State Building (350 5th Ave., btwn 33rd & 34th St., New York) <a href="http://t.co/q6tXzf3n" title="http://4sq.com/zZ5xGd">4sq.com/zZ5xGd</a></p>&mdash; Simon Ackerman (@SimonAckerman) <a href="https://twitter.com/SimonAckerman/status/167232054247956481" data-datetime="2012-02-08T13:03:23+00:00">February 8, 2012</a></blockquote>


<script src="http://blog.echen.me//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Let&#8217;s finally make Foursquare check-ins useful. (I kid, I kid.)</p>

<p>Instead of using an explicit rating given to us, we can simply generate a dummy rating of 1 for each check-in. Correlation doesn&#8217;t make sense any more, but we can still pay attention to a measure like Jaccard simiilarity.</p>

<p>So we simply create a new class that scrapes tweets for Foursquare check-in information&#8230;</p>

<figure class='code'><figcaption><span>Location similarities with Foursquare </span><a href='https://github.com/echen/scaldingale/blob/master/Foursquare.scala'>Foursquare.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">Foursquare</span><span class="o">(</span><span class="n">args</span> <span class="k">:</span> <span class="kt">Args</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">VectorSimilarities</span><span class="o">(</span><span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// Example tweet: I&#39;m at The Ambassador (673 Geary St, btw Leavenworth &amp; Jones, San Francisco) w/ 2 others http://4sq.com/xok3rI</span>
</span><span class='line'>  <span class="c1">// Let&#39;s limit to New York for simplicity.</span>
</span><span class='line'>  <span class="k">val</span> <span class="nc">FOURSQUARE_REGEX</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;I&#39;m at (.+?) \(.*? New York&quot;&quot;&quot;</span><span class="o">.</span><span class="n">r</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">input</span><span class="o">(</span><span class="n">userField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">itemField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">ratingField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Pipe</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">foursquareCheckins</span> <span class="k">=</span>
</span><span class='line'>      <span class="nc">TweetSource</span><span class="o">()</span>
</span><span class='line'>        <span class="o">.</span><span class="n">mapTo</span><span class="o">(</span><span class="-Symbol">&#39;userId</span><span class="o">,</span> <span class="-Symbol">&#39;text</span><span class="o">)</span> <span class="o">{</span> <span class="n">s</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="n">getUserId</span><span class="o">.</span><span class="n">toLong</span><span class="o">,</span> <span class="n">s</span><span class="o">.</span><span class="n">getText</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="-Symbol">&#39;text</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;location</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">text</span> <span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span>
</span><span class='line'>          <span class="nc">FOURSQUARE_REGEX</span><span class="o">.</span><span class="n">findFirstMatchIn</span><span class="o">(</span><span class="n">text</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">_</span><span class="o">.</span><span class="n">subgroups</span> <span class="o">}.</span><span class="n">map</span> <span class="o">{</span> <span class="n">l</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">l</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="mi">1</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">rename</span><span class="o">((</span><span class="-Symbol">&#39;userId</span><span class="o">,</span> <span class="-Symbol">&#39;location</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">))</span>
</span><span class='line'>        <span class="o">.</span><span class="n">unique</span><span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">foursquareCheckins</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>&#8230;and bam! Here are locations similar to the <strong>Empire State Building</strong>:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/empire-state-building.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/empire-state-building.png" alt="Empire State Building" /></a></p>

<p>Here are places you might want to check out, if you check-in at <strong>Bergdorf Goodman</strong>:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/bergdorf-goodman.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/bergdorf-goodman.png" alt="Bergdorf Goodman" /></a></p>

<p>And here&#8217;s where to go after the <strong>Statue of Liberty</strong>:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/statue-of-liberty.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/statue-of-liberty.png" alt="Statue of Liberty" /></a></p>

<p>Power of Twitter, yo.</p>

<h1>RottenTomatoes Similarities</h1>

<p>UPDATE: I found some movie data after all&#8230;</p>

<blockquote class="twitter-tweet"><p>My review for &#8216;How to Train Your Dragon&#8217; on Rotten Tomatoes: 4 1/2 stars &gt;<a href="http://t.co/YTOKWLEt" title="http://bit.ly/xtw3d3">bit.ly/xtw3d3</a></p>&mdash; Benjamin West (@BenTheWest) <a href="https://twitter.com/BenTheWest/status/171772890121895936" data-datetime="2012-02-21T01:47:03+00:00">February 21, 2012</a></blockquote>


<p>So let&#8217;s use RottenTomatoes tweets to recommend movies! Here&#8217;s the code for a class that searches for RottenTomatoes tweets:</p>

<figure class='code'><figcaption><span>Movie similarities with RottenTomatoes </span><a href='https://github.com/echen/scaldingale/blob/master/RottenTomatoes.scala'>RottenTomatoes.scala</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">RottenTomatoes</span><span class="o">(</span><span class="n">args</span> <span class="k">:</span> <span class="kt">Args</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">VectorSimilarities</span><span class="o">(</span><span class="n">args</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="cm">/**</span>
</span><span class='line'><span class="cm">   * Example tweets:</span>
</span><span class='line'><span class="cm">   * My review for &#39;Hop&#39; on Rotten Tomatoes: 1 star &gt; http://bit.ly/AB7Tl4</span>
</span><span class='line'><span class="cm">   * My review for &#39;The Bothersome Man (Den Brysomme mannen)&#39; on Rotten Tomatoes: 3 stars-A muddled Playtime in Paris,... http://tmto.es/AvPoO2</span>
</span><span class='line'><span class="cm">   */</span>
</span><span class='line'>  <span class="k">val</span> <span class="nc">ROTTENTOMATOES_REGEX</span> <span class="k">=</span> <span class="s">&quot;&quot;&quot;My review for &#39;(.+?)&#39; on Rotten Tomatoes: (\d) star&quot;&quot;&quot;</span><span class="o">.</span><span class="n">r</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">val</span> <span class="nc">MIN_NUM_RATERS</span> <span class="k">=</span> <span class="mi">2</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">val</span> <span class="nc">MAX_NUM_RATERS</span> <span class="k">=</span> <span class="mi">1000</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">val</span> <span class="nc">MIN_INTERSECTION</span> <span class="k">=</span> <span class="mi">2</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">input</span><span class="o">(</span><span class="n">userField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">itemField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">,</span> <span class="n">ratingField</span> <span class="k">:</span> <span class="kt">Symbol</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Pipe</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">rottenTomatoesRatings</span> <span class="k">=</span>
</span><span class='line'>      <span class="nc">TweetSource</span><span class="o">()</span>
</span><span class='line'>        <span class="o">.</span><span class="n">mapTo</span><span class="o">(</span><span class="-Symbol">&#39;userId</span><span class="o">,</span> <span class="-Symbol">&#39;text</span><span class="o">)</span> <span class="o">{</span> <span class="n">s</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="n">getUserId</span><span class="o">.</span><span class="n">toLong</span><span class="o">,</span> <span class="n">s</span><span class="o">.</span><span class="n">getText</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="-Symbol">&#39;text</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">text</span> <span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span>
</span><span class='line'>          <span class="nc">ROTTENTOMATOES_REGEX</span><span class="o">.</span><span class="n">findFirstMatchIn</span><span class="o">(</span><span class="n">text</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="n">_</span><span class="o">.</span><span class="n">subgroups</span> <span class="o">}.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">x</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">toInt</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="o">.</span><span class="n">rename</span><span class="o">((</span><span class="-Symbol">&#39;userId</span><span class="o">,</span> <span class="-Symbol">&#39;movie</span><span class="o">,</span> <span class="-Symbol">&#39;rating</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">))</span>
</span><span class='line'>        <span class="o">.</span><span class="n">unique</span><span class="o">(</span><span class="n">userField</span><span class="o">,</span> <span class="n">itemField</span><span class="o">,</span> <span class="n">ratingField</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">rottenTomatoesRatings</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>And here are the most similar movies discovered:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/top-rottentomatoes-sims.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/top-rottentomatoes-sims.png" alt="Top RottenTomatoes Movies" /></a></p>

<p>We see that</p>

<ul>
<li><em>Lord of the Rings</em>, <em>Harry Potter</em>, and <em>Star Wars</em> movies are similar to other <em>Lord of the Rings</em>, <em>Harry Potter</em>, and <em>Star Wars</em> movies</li>
<li>Big science fiction blockbusters (<em>Avatar</em>) are similar to big science fiction blockbusters (<em>Inception</em>)</li>
<li>People who like one Justin Timberlake movie (<em>Bad Teacher</em>) also like other Justin Timberlake Movies (<em>In Time</em>). Similarly with Michael Fassbender (<em>A Dangerous Method</em>, <em>Shame</em>)</li>
<li>Art house movies (<em>The Tree of Life</em>) stick together (<em>Tinker Tailor Soldier Spy</em>)</li>
</ul>


<p>Let&#8217;s also look at the movies with the most <em>negative</em> correlation:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/scaldingale/bottom-rottentomatoes-sims.png"><img src="http://dl.dropbox.com/u/10506/blog/scaldingale/bottom-rottentomatoes-sims.png" alt="Negative RottenTomatoes Movies" /></a></p>

<p>(The more you like loud and dirty popcorn movies (<em>Thor</em>) and vamp romance (<em>Twilight</em>), the less you like arthouse? SGTM.)</p>

<h1>Next Steps</h1>

<p>Hopefully I gave you a taste of the awesomeness of Scalding. To learn even more:</p>

<ul>
<li>Check out <a href="https://github.com/twitter/scalding">Scalding on Github</a>.</li>
<li>Read <a href="https://github.com/twitter/scalding/wiki/Getting-Started">this Getting Started Guide</a> on the Scalding wiki.</li>
<li>Run through <a href="https://github.com/twitter/scalding/tree/master/tutorial">this code-based introduction</a>, complete with Scalding jobs that you can run in local mode.</li>
<li>Browse <a href="https://github.com/twitter/scalding/wiki/API-Reference">the API reference</a>, which also contains many code snippets illustrating different Scalding functions (e.g., <code>map</code>, <code>filter</code>, <code>flatMap</code>, <code>groupBy</code>, <code>count</code>, <code>join</code>).</li>
<li>And all the code for this post is <a href="https://github.com/echen/scaldingale">here</a>.</li>
</ul>


<p>Watch out for more documentation soon, and you should most definitely <a href="https://twitter.com/#!/scalding">follow @Scalding</a> on Twitter for updates or to ask any questions.</p>

<h1>Mad Props</h1>

<p>And finally, a huge shoutout to <a href="https://twitter.com/argyris">Argyris Zymnis</a>, <a href="https://twitter.com/avibryant">Avi Bryant</a>, and <a href="https://twitter.com/posco">Oscar Boykin</a>, the mastermind hackers who have spent (and continue spending!) unimaginable hours making Scalding a joy to use.</p>

<p>@argyris, @avibryant, @posco: Thanks for it all. #awesomejobguys #loveit</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quick Introduction to ggplot2]]></title>
    <link href="http://blog.echen.me/2012/01/17/quick-introduction-to-ggplot2/"/>
    <updated>2012-01-17T10:28:55-08:00</updated>
    <id>http://blog.echen.me/2012/01/17/quick-introduction-to-ggplot2</id>
    <content type="html"><![CDATA[<p>This is a bare-bones introduction to <a href="http://had.co.nz/ggplot2/">ggplot2</a>, a visualization package in R. It assumes no knowledge of R.</p>

<p>For a better-looking version of this post, see <a href="https://github.com/echen/ggplot2-tutorial">this Github repository</a>, which also contains some of the <a href="https://github.com/echen/ggplot2-tutorial/tree/master/data">example datasets</a> I use and a <a href="https://github.com/echen/ggplot2-tutorial/blob/master/ggplot2-tutorial.R">literate programming version</a> of this tutorial.</p>

<h1>Preview</h1>

<p>Let&#8217;s start with a preview of what ggplot2 can do.</p>

<p>Given Fisher&#8217;s <a href="http://en.wikipedia.org/wiki/Iris_flower_data_set">iris</a> data set and one simple command&#8230;</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> color <span class="o">=</span> Species<span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>&#8230;we can produce this plot of sepal length vs. petal length, colored by species.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-specied.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-specied.png" alt="Sepal vs. Petal, Colored by Species" /></a></p>

<h1>Installation</h1>

<p>You can download R <a href="http://cran.opensourceresources.org/">here</a>. After installation, you can launch R in interactive mode by either typing <code>R</code> on the command line or opening the standard GUI (which should have been included in the download).</p>

<h1>R Basics</h1>

<h2>Vectors</h2>

<p>Vectors are a core data structure in R, and are created with <code>c()</code>. Elements in a vector must be of the same type.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>numbers <span class="o">=</span> c<span class="p">(</span><span class="m">23</span><span class="p">,</span> <span class="m">13</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">7</span><span class="p">,</span> <span class="m">31</span><span class="p">)</span>
</span><span class='line'>names <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;edwin&quot;</span><span class="p">,</span> <span class="s">&quot;alice&quot;</span><span class="p">,</span> <span class="s">&quot;bob&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Elements are indexed starting at 1, and are accessed with <code>[]</code> notation.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>numbers<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="c1"># 23</span>
</span><span class='line'>names<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="c1"># edwin</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Data frames</h2>

<p><a href="http://www.r-tutor.com/r-introduction/data-frame">Data frames</a> are like matrices, but with named columns of different types (similar to <a href="http://code.google.com/p/sqldf/">database tables</a>).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>books <span class="o">=</span> data.frame<span class="p">(</span>
</span><span class='line'>  title <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;harry potter&quot;</span><span class="p">,</span> <span class="s">&quot;war and peace&quot;</span><span class="p">,</span> <span class="s">&quot;lord of the rings&quot;</span><span class="p">),</span> <span class="c1"># column named &quot;title&quot;</span>
</span><span class='line'>  author <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;rowling&quot;</span><span class="p">,</span> <span class="s">&quot;tolstoy&quot;</span><span class="p">,</span> <span class="s">&quot;tolkien&quot;</span><span class="p">),</span>
</span><span class='line'>  num_pages <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;350&quot;</span><span class="p">,</span> <span class="s">&quot;875&quot;</span><span class="p">,</span> <span class="s">&quot;500&quot;</span><span class="p">)</span>
</span><span class='line'><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can access columns of a data frame with <code>$</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>books<span class="p">$</span>title <span class="c1"># c(&quot;harry potter&quot;, &quot;war and peace&quot;, &quot;lord of the rings&quot;)</span>
</span><span class='line'>books<span class="p">$</span>author<span class="p">[</span><span class="m">1</span><span class="p">]</span> <span class="c1"># &quot;rowling&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can also create new columns with <code>$</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>books<span class="p">$</span>num_bought_today <span class="o">=</span> c<span class="p">(</span><span class="m">10</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="m">8</span><span class="p">)</span>
</span><span class='line'>books<span class="p">$</span>num_bought_yesterday <span class="o">=</span> c<span class="p">(</span><span class="m">18</span><span class="p">,</span> <span class="m">13</span><span class="p">,</span> <span class="m">20</span><span class="p">)</span>
</span><span class='line'>  
</span><span class='line'>books<span class="p">$</span>total\_num\_bought <span class="o">=</span> books<span class="p">$</span>num_bought_today <span class="o">+</span> books<span class="p">$</span>num_bought_yesterday
</span></code></pre></td></tr></table></div></figure>


<h2>read.table</h2>

<p>Suppose you want to import a TSV file into R as a data frame.</p>

<h3>tsv file without header</h3>

<p>For example, consider the <a href="https://github.com/echen/r-tutorial/blob/master/data/students.tsv"><code>data/students.tsv</code></a> file (with columns describing each student&#8217;s age, test score, and name).</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="m">13</span>   <span class="m">100</span> alice
</span><span class='line'><span class="m">14</span>   <span class="m">95</span>  bob
</span><span class='line'><span class="m">13</span>   <span class="m">82</span>  eve
</span></code></pre></td></tr></table></div></figure>


<p>We can import this file into R using <a href="http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.table()</code></a>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>students <span class="o">=</span> read.table<span class="p">(</span><span class="s">&quot;data/students.tsv&quot;</span><span class="p">,</span>
</span><span class='line'>  header <span class="o">=</span> <span class="k-Variable">F</span><span class="p">,</span> <span class="c1"># file does not contain a header (`F` is short for `FALSE`), so we must manually specify column names                    </span>
</span><span class='line'>  sep <span class="o">=</span> <span class="s">&quot;\t&quot;</span><span class="p">,</span> <span class="c1"># file is tab-delimited        </span>
</span><span class='line'>  col.names <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;score&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">)</span> <span class="c1"># column names</span>
</span><span class='line'><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>We can now access the different columns in the data frame with <code>students$age</code>, <code>students$score</code>, and <code>students$name</code>.</p>

<h3>csv file with header</h3>

<p>For an example of a file in a different format, look at the <a href="https://github.com/echen/r-tutorial/blob/master/data/studentsWithHeader.tsv"><code>data/studentsWithHeader.tsv</code></a> file.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>age<span class="p">,</span>score<span class="p">,</span>name
</span><span class='line'><span class="m">13</span><span class="p">,</span><span class="m">100</span><span class="p">,</span>alice
</span><span class='line'><span class="m">14</span><span class="p">,</span><span class="m">95</span><span class="p">,</span>bob
</span><span class='line'><span class="m">13</span><span class="p">,</span><span class="m">82</span><span class="p">,</span>eve
</span></code></pre></td></tr></table></div></figure>


<p>Here we have the same data, but now the file is comma-delimited and contains a header. We can import this file with</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>students <span class="o">=</span> read.table<span class="p">(</span><span class="s">&quot;data/students.tsv&quot;</span><span class="p">,</span>
</span><span class='line'>  sep <span class="o">=</span> <span class="s">&quot;,&quot;</span><span class="p">,</span>
</span><span class='line'>  header <span class="o">=</span> <span class="k-Variable">T</span>  <span class="c1"># first line contains column names, so we can immediately call `students$age`        </span>
</span><span class='line'><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>(Note: there is also a <code>read.csv</code> function that uses <code>sep = ","</code> by default.)</p>

<h2>help</h2>

<p>There are many more options that <code>read.table</code> can take. For a list of these, just type <code>help(read.table)</code> (or <code>?read.table</code>) at the prompt to access documentation.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># These work for other functions as well.</span>
</span><span class='line'>help<span class="p">(</span>read.table<span class="p">)</span>
</span><span class='line'>?read.table
</span></code></pre></td></tr></table></div></figure>


<h1>ggplot2</h1>

<p>With these R basics in place, let&#8217;s dive into the ggplot2 package.</p>

<h2>Installation</h2>

<p>One of R&#8217;s greatest strengths is its excellent set of <a href="http://cran.r-project.org/web/packages/available_packages_by_name.html">packages</a>. To install a package, you can use the <code>install.packages()</code> function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>install.packages<span class="p">(</span><span class="s">&quot;ggplot2&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>To load a package into your current R session, use <code>library()</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>library<span class="p">(</span>ggplot2<span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Scatterplots with qplot()</h2>

<p>Let&#8217;s look at how to create a scatterplot in ggplot2. We&#8217;ll use the <code>iris</code> data frame that&#8217;s automatically loaded into R.</p>

<p>What does the data frame contain? We can use the <code>head</code> function to look at the first few rows.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>head<span class="p">(</span>iris<span class="p">)</span> <span class="c1"># by default, head displays the first 6 rows. see `?head`</span>
</span><span class='line'>head<span class="p">(</span>iris<span class="p">,</span> n <span class="o">=</span> <span class="m">10</span><span class="p">)</span> <span class="c1"># we can also explicitly set the number of rows to display</span>
</span><span class='line'>
</span><span class='line'>Sepal.Length Sepal.Width Petal.Length Petal.Width Species
</span><span class='line'>         <span class="m">5.1</span>         <span class="m">3.5</span>          <span class="m">1.4</span>         <span class="m">0.2</span>  setosa
</span><span class='line'>         <span class="m">4.9</span>         <span class="m">3.0</span>          <span class="m">1.4</span>         <span class="m">0.2</span>  setosa
</span><span class='line'>         <span class="m">4.7</span>         <span class="m">3.2</span>          <span class="m">1.3</span>         <span class="m">0.2</span>  setosa
</span><span class='line'>         <span class="m">4.6</span>         <span class="m">3.1</span>          <span class="m">1.5</span>         <span class="m">0.2</span>  setosa
</span><span class='line'>         <span class="m">5.0</span>         <span class="m">3.6</span>          <span class="m">1.4</span>         <span class="m">0.2</span>  setosa
</span><span class='line'>         <span class="m">5.4</span>         <span class="m">3.9</span>          <span class="m">1.7</span>         <span class="m">0.4</span>  setosa
</span></code></pre></td></tr></table></div></figure>


<p>(The data frame actually contains three types of species: setosa, versicolor, and virginica.)</p>

<p>Let&#8217;s plot <code>Sepal.Length</code> against <code>Petal.Length</code> using ggplot2&#8217;s <code>qplot()</code> function.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">)</span>
</span><span class='line'><span class="c1"># Plot Sepal.Length vs. Petal.Length, using data from the `iris` data frame.</span>
</span><span class='line'><span class="c1"># * First argument `Sepal.Length` goes on the x-axis.</span>
</span><span class='line'><span class="c1"># * Second argument `Petal.Length` goes on the y-axis.</span>
</span><span class='line'><span class="c1"># * `data = iris` means to look for this data in the `iris` data frame.    </span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal.png" alt="Sepal Length vs. Petal Length" /></a></p>

<p>To see where each species is located in this graph, we can color each point by adding a <code>color = Species</code> argument.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> color <span class="o">=</span> Species<span class="p">)</span> <span class="c1"># dude!</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-specied.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-specied.png" alt="Sepal vs. Petal, Colored by Species" /></a></p>

<p>Similarly, we can let the size of each point denote sepal width, by adding a <code>size = Sepal.Width</code> argument.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> color <span class="o">=</span> Species<span class="p">,</span> size <span class="o">=</span> Petal.Width<span class="p">)</span>
</span><span class='line'><span class="c1"># We see that Iris setosa flowers have the narrowest petals.</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-sized.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-sized.png" alt="Sepal vs. Petal, Sized by Petal Width" /></a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> color <span class="o">=</span> Species<span class="p">,</span> size <span class="o">=</span> Petal.Width<span class="p">,</span> alpha <span class="o">=</span> I<span class="p">(</span><span class="m">0.7</span><span class="p">))</span>
</span><span class='line'><span class="c1"># By setting the alpha of each point to 0.7, we reduce the effects of overplotting.</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-alpha.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-alpha.png" alt="Sepal vs. Petal, with Transparency" /></a></p>

<p>Finally, let&#8217;s fix the axis labels and add a title to the plot.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> color <span class="o">=</span> Species<span class="p">,</span>
</span><span class='line'>  xlab <span class="o">=</span> <span class="s">&quot;Sepal Length&quot;</span><span class="p">,</span> ylab <span class="o">=</span> <span class="s">&quot;Petal Length&quot;</span><span class="p">,</span>
</span><span class='line'>  main <span class="o">=</span> <span class="s">&quot;Sepal vs. Petal Length in Fisher&#39;s Iris data&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-titled.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-titled.png" alt="Sepal vs. Petal, Titled" /></a></p>

<h2>Other common geoms</h2>

<p>In the scatterplot examples above, we implicitly used a <em>point</em> <strong>geom</strong>, the default when you supply two arguments to <code>qplot()</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># These two invocations are equivalent.</span>
</span><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> geom <span class="o">=</span> <span class="s">&quot;point&quot;</span><span class="p">)</span>
</span><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>But we can also easily use other types of geoms to create more kinds of plots.</p>

<h3>Barcharts: geom = &#8220;bar&#8221;</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>movies <span class="o">=</span> data.frame<span class="p">(</span>
</span><span class='line'>  director <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;spielberg&quot;</span><span class="p">,</span> <span class="s">&quot;spielberg&quot;</span><span class="p">,</span> <span class="s">&quot;spielberg&quot;</span><span class="p">,</span> <span class="s">&quot;jackson&quot;</span><span class="p">,</span> <span class="s">&quot;jackson&quot;</span><span class="p">),</span>
</span><span class='line'>  movie <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;jaws&quot;</span><span class="p">,</span> <span class="s">&quot;avatar&quot;</span><span class="p">,</span> <span class="s">&quot;schindler&#39;s list&quot;</span><span class="p">,</span> <span class="s">&quot;lotr&quot;</span><span class="p">,</span> <span class="s">&quot;king kong&quot;</span><span class="p">),</span>
</span><span class='line'>  minutes <span class="o">=</span> c<span class="p">(</span><span class="m">124</span><span class="p">,</span> <span class="m">163</span><span class="p">,</span> <span class="m">195</span><span class="p">,</span> <span class="m">600</span><span class="p">,</span> <span class="m">187</span><span class="p">)</span>
</span><span class='line'><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1"># Plot the number of movies each director has.</span>
</span><span class='line'>qplot<span class="p">(</span>director<span class="p">,</span> data <span class="o">=</span> movies<span class="p">,</span> geom <span class="o">=</span> <span class="s">&quot;bar&quot;</span><span class="p">,</span> ylab <span class="o">=</span> <span class="s">&quot;# movies&quot;</span><span class="p">)</span>
</span><span class='line'><span class="c1"># By default, the height of each bar is simply a count.</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/num-movies.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/num-movies.png" alt="# Movies" /></a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># But we can also supply a different weight.</span>
</span><span class='line'><span class="c1"># Here the height of each bar is the total running time of the director&#39;s movies.</span>
</span><span class='line'>qplot<span class="p">(</span>director<span class="p">,</span> weight <span class="o">=</span> minutes<span class="p">,</span> data <span class="o">=</span> movies<span class="p">,</span> geom <span class="o">=</span> <span class="s">&quot;bar&quot;</span><span class="p">,</span> ylab <span class="o">=</span> <span class="s">&quot;total length (min.)&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/total-length.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/total-length.png" alt="Total Running Time" /></a></p>

<h3>Line charts: geom = &#8220;line&#8221;</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'>qplot<span class="p">(</span>Sepal.Length<span class="p">,</span> Petal.Length<span class="p">,</span> data <span class="o">=</span> iris<span class="p">,</span> geom <span class="o">=</span> <span class="s">&quot;line&quot;</span><span class="p">,</span> color <span class="o">=</span> Species<span class="p">)</span>
</span><span class='line'><span class="c1"># Using a line geom doesn&#39;t really make sense here, but hey.</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-lined.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/sepal-vs-petal-lined.png" alt="Sepal vs. Petal, Lined" /></a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># `Orange` is another built-in data frame that describes the growth of orange trees.</span>
</span><span class='line'>qplot<span class="p">(</span>age<span class="p">,</span> circumference<span class="p">,</span> data <span class="o">=</span> Orange<span class="p">,</span> geom <span class="o">=</span> <span class="s">&quot;line&quot;</span><span class="p">,</span>
</span><span class='line'>  colour <span class="o">=</span> Tree<span class="p">,</span>
</span><span class='line'>  main <span class="o">=</span> <span class="s">&quot;How does orange tree circumference vary with age?&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/orange-tree-growth.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/orange-tree-growth.png" alt="Orange Tree Growth" /></a></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='r'><span class='line'><span class="c1"># We can also plot both points and lines.</span>
</span><span class='line'>qplot<span class="p">(</span>age<span class="p">,</span> circumference<span class="p">,</span> data <span class="o">=</span> Orange<span class="p">,</span> geom <span class="o">=</span> c<span class="p">(</span><span class="s">&quot;point&quot;</span><span class="p">,</span> <span class="s">&quot;line&quot;</span><span class="p">),</span> colour <span class="o">=</span> Tree<span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><a href="http://dl.dropbox.com/u/10506/blog/r/ggplot2/orange-tree-pointed.png"><img src="http://dl.dropbox.com/u/10506/blog/r/ggplot2/orange-tree-pointed.png" alt="Orange Tree with Points" /></a></p>

<p>And that&#8217;s it with what I&#8217;ll cover.</p>

<h1>Next Steps</h1>

<p>I skipped over a lot of aspects of R and ggplot2 in this intro.</p>

<p>For example,</p>

<ul>
<li>There are many geoms (and other functionalities) in ggplot2 that I didn&#8217;t cover, e.g., <a href="http://had.co.nz/ggplot2/geom_boxplot.html">boxplots</a> and <a href="http://had.co.nz/ggplot2/geom_histogram.html">histograms</a>.</li>
<li>I didn&#8217;t talk about ggplot2&#8217;s layering system, or the <a href="http://www.amazon.com/Grammar-Graphics-Statistics-Computing/dp/0387245448">grammar of graphics</a> it&#8217;s based on.</li>
</ul>


<p>So I&#8217;ll end with some additional resources on R and ggplot2.</p>

<ul>
<li>I don&#8217;t use it myself, but <a href="http://rstudio.org/">RStudio</a> is a popular IDE for R.</li>
<li>The <a href="http://had.co.nz/ggplot2/">official ggplot2 documentation</a> is great and has lots of examples. There&#8217;s also an excellent <a href="http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403">book</a>.</li>
<li><a href="http://plyr.had.co.nz/">plyr</a> is another fantastic R package that&#8217;s also by Hadley Wickham (the author of ggplot2).</li>
<li>The <a href="http://cran.r-project.org/doc/manuals/R-intro.html">official R introduction</a> is okay, but definitely not great. I haven&#8217;t found any R tutorials I really like, but I&#8217;ve heard good things about <a href="http://www.amazon.com/Art-Programming-Statistical-Software-Design/dp/1593273843">The Art of R Programming</a>.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Conditional Random Fields]]></title>
    <link href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/"/>
    <updated>2012-01-03T16:02:25-08:00</updated>
    <id>http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>Imagine you have a sequence of snapshots from a day in Justin Bieber&#8217;s life, and you want to label each image with the activity it represents (eating, sleeping, driving, etc.). How can you do this?</p>

<p>One way is to ignore the sequential nature of the snapshots, and build a <em>per-image</em> classifier. For example, given a month&#8217;s worth of labeled snapshots, you might learn that dark images taken at 6am tend to be about sleeping, images with lots of bright colors tend to be about dancing, images of cars are about driving, and so on.</p>

<p>By ignoring this sequential aspect, however, you lose a lot of information. For example, what happens if you see a close-up picture of a mouth &#8211; is it about singing or eating? If you know that the <em>previous</em> image is a picture of Justin Bieber eating or cooking, then it&#8217;s more likely this picture is about eating; if, however, the previous image contains Justin Bieber singing or dancing, then this one probably shows him singing as well.</p>

<p>Thus, to increase the accuracy of our labeler, we should incorporate the labels of nearby photos, and this is precisely what a <strong>conditional random field</strong> does.</p>

<h1>Part-of-Speech Tagging</h1>

<p>Let&#8217;s go into some more detail, using the more common example of <strong>part-of-speech tagging</strong>.</p>

<p>In POS tagging, the goal is to label a sentence (a sequence of words or tokens) with tags like ADJECTIVE, NOUN, PREPOSITION, VERB, ADVERB, ARTICLE.</p>

<p>For example, given the sentence &#8220;Bob drank coffee at Starbucks&#8221;, the labeling might be &#8220;Bob (NOUN) drank (VERB) coffee (NOUN) at (PREPOSITION) Starbucks (NOUN)&#8221;.</p>

<p>So let&#8217;s build a conditional random field to label sentences with their parts of speech. Just like any classifier, we&#8217;ll first need to decide on a set of feature functions $f_i$.</p>

<h2>Feature Functions in a CRF</h2>

<p>In a CRF, each <strong>feature function</strong> is a function that takes in as input:</p>

<ul>
<li>a sentence s</li>
<li>the position i of a word in the sentence</li>
<li>the label $l_i$ of the current word</li>
<li>the label $l_{i-1}$ of the previous word</li>
</ul>


<p>and outputs a real-valued number (though the numbers are often just either 0 or 1).</p>

<p>(Note: by restricting our features to depend on only the <em>current</em> and <em>previous</em> labels, rather than arbitrary labels throughout the sentence, I&#8217;m actually building the special case of a <strong>linear-chain CRF</strong>. For simplicity, I&#8217;m going to ignore general CRFs in this post.)</p>

<p>For example, one possible feature function could measure how much we suspect that the current word should be labeled as an adjective given that the previous word is &#8220;very&#8221;.</p>

<h2>Features to Probabilities</h2>

<p>Next, assign each feature function $f_j$ a <strong>weight</strong> $\lambda_j$ (I&#8217;ll talk below about how to learn these weights from the data). Given a sentence s, we can now score a labeling l of s by adding up the weighted features over all words in the sentence:</p>

<p>$score(l | s) = \sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})$</p>

<p>(The first sum runs over each feature function $j$, and the inner sum runs over each position $i$ of the sentence.)</p>

<p>Finally, we can transform these scores into probabilities $p(l | s)$ between 0 and 1 by exponentiating and normalizing:</p>

<p>$p(l | s) = \frac{exp[score(l|s)]}{\sum_{l&#8217;} exp[score(l&#8217;|s)]} = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i, l_{i-1})]}{\sum_{l&#8217;} exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l&#8217;_i, l&#8217;_{i-1})]}$</p>

<h2>Example Feature Functions</h2>

<p>So what do these feature functions look like? Examples of POS tagging features could include:</p>

<ul>
<li>$f_1(s, i, l_i, l_{i-1}) = 1$ if $l_i =$ ADVERB and the ith word ends in &#8220;-ly&#8221;; 0 otherwise.

<ul>
<li>If the weight $\lambda_1$ associated with this feature is large and positive, then this feature is essentially saying that we prefer labelings where words ending in -ly get labeled as ADVERB.</li>
</ul>
</li>
<li>$f_2(s, i, l_i, l_{i-1}) = 1$ if $i = 1$, $l_i =$ VERB, and the sentence ends in a question mark; 0 otherwise.

<ul>
<li>Again, if the weight $\lambda_2$ associated with this feature is large and positive, then labelings that assign VERB to the first word in a question (e.g., &#8220;Is this a sentence beginning with a verb?&#8221;) are preferred.</li>
</ul>
</li>
<li>$f_3(s, i, l_i, l_{i-1}) = 1$ if $l_{i-1} =$ ADJECTIVE and $l_i =$ NOUN; 0 otherwise.

<ul>
<li>Again, a positive weight for this feature means that adjectives tend to be followed by nouns.</li>
</ul>
</li>
<li>$f_4(s, i, l_i, l_{i-1}) = 1$ if $l_{i-1} =$ PREPOSITION and $l_i =$ PREPOSITION.

<ul>
<li>A <em>negative</em> weight $\lambda_4$ for this function would mean that prepositions don&#8217;t tend to follow prepositions, so we should avoid labelings where this happens.</li>
</ul>
</li>
</ul>


<p>And that&#8217;s it! To sum up: to build a conditional random field, you just define a bunch of feature functions (which can depend on the entire sentence, a current position, and nearby labels), assign them weights, and add them all together, transforming at the end to a probability if necessary.</p>

<p>Now let&#8217;s step back and compare CRFs to some other common machine learning techniques.</p>

<h1>Smells like Logistic Regression&#8230;</h1>

<p>The form of the CRF probabilities
$p(l | s) = \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n f_j(s, i, l_i, l_{i-1})]}{\sum_{l&#8217;} exp[\sum_{j = 1}^m \sum_{i = 1}^n f_j(s, i, l&#8217;_i, l&#8217;_{i-1})]}$
might look <a href="http://en.wikipedia.org/wiki/Logistic_regression">familiar</a>.</p>

<p>That&#8217;s because CRFs are indeed basically the sequential version of <strong>logistic regression</strong>: whereas logistic regression is a log-linear model for <em>classification</em>, CRFs are a log-linear model for <em>sequential labels</em>.</p>

<h1>Looks like HMMs&#8230;</h1>

<p>Recall that <strong><a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Models</a></strong> are another model for part-of-speech tagging (and sequential labeling in general). Whereas CRFs throw any bunch of functions together to get a label score, HMMs take a <em>generative</em> approach to labeling, defining</p>

<p>$p(l,s) = p(l_1) \prod_i p(l_i | l_{i-1}) p(w_i | l_i)$</p>

<p>where</p>

<ul>
<li>$p(l_i | l_{i-1})$ are <strong>transition</strong> probabilities (e.g., the probability that a preposition is followed by a noun);</li>
<li>$p(w_i | l_i)$ are <strong>emission</strong> probabilities (e.g., the probability that a noun emits the word &#8220;dad&#8221;).</li>
</ul>


<p>So how do HMMs compare to CRFs? CRFs are more powerful &#8211; they can model everything HMMs can and more. One way of seeing this is as follows.</p>

<p>Note that the log of the HMM probability is $\log p(l,s) = \log p(l_0) + \sum_i \log p(l_i | l_{i-1}) + \sum_i \log p(w_i | l_i)$. This has exactly the log-linear form of a CRF if we consider these log-probabilities to be the weights associated to binary transition and emission indicator features.</p>

<p>That is, we can build a CRF equivalent to any HMM by&#8230;</p>

<ul>
<li>For each HMM <em>transition</em> probability $ p(l_i = y | l_{i-1} = x) $, define a set of CRF transition features of the form $f_{x,y}(s, i, l_i, l_{i-1}) = 1$ if $l_i = y$ and $l_{i-1} = x$. Give each feature a weight of $w_{x,y} = \log p(l_i = y | l_{i-1} = x)$.</li>
<li>Similarly, for each HMM <em>emission</em> probability $p(w_i = z | l_{i} = x)$, define a set of CRF emission features of the form $g_{x,y}(s, i, l_i, l_{i-1}) = 1$ if $w_i = z$ and $l_i = x$. Give each feature a weight of $w_{x,z} = \log p(w_i = z | l_i = x)$.</li>
</ul>


<p>Thus, the score $p(l|s)$ computed by a CRF using these feature functions is precisely proportional to the score computed by the associated HMM, and so every HMM is equivalent to some CRF.</p>

<p>However, CRFs can model a much richer set of label distributions as well, for two main reasons:</p>

<ul>
<li><strong>CRFs can define a much larger set of features.</strong> Whereas HMMs are necessarily <em>local</em> in nature (because they&#8217;re constrained to binary transition and emission feature functions, which force each word to depend only on the current label and each label to depend only on the previous label), CRFs can use more <em>global</em> features. For example, one of the features in our POS tagger above increased the probability of labelings that tagged the <em>first</em> word of a sentence as a VERB if the <em>end</em> of the sentence contained a question mark.</li>
<li><strong>CRFs can have arbitrary weights.</strong> Whereas the probabilities of an HMM must satisfy certain constraints (e.g., $0 &lt;= p(w_i | l_i) &lt;= 1, \sum_w p(w_i = w | l_1) = 1)$, the weights of a CRF are unrestricted (e.g., $\log p(w_i | l_i)$ can be anything it wants).</li>
</ul>


<h1>Learning Weights</h1>

<p>Let&#8217;s go back to the question of how to learn the feature weights in a CRF. One way is (surprise) to use <strong>gradient ascent</strong>.</p>

<p>Assume we have a bunch of training examples (sentences and associated part-of-speech labels). Randomly initialize the weights of our CRF model.
To shift these randomly initialized weights to the correct ones, for each training example&#8230;</p>

<ul>
<li>Go through each feature function $f_i$, and calculate the gradient of the log probability of the training example with respect to $\lambda_i$: $\frac{\partial}{\partial w_j} \log p(l | s) = \sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l&#8217;} p(l&#8217; | s) \sum_{j = 1}^m f_i(s, j, l&#8217;_j, l&#8217;_{j-1})$</li>
<li>Note that the first term in the gradient is the contribution of feature $f_i$ under the <em>true</em> label, and the second term in the gradient is the <em>expected</em> contribution of feature $f_i$ under the current model. This is exactly the form you&#8217;d expect gradient ascent to take.</li>
<li>Move $\lambda_i$ in the direction of the gradient: $\lambda_i = \lambda_i + \alpha [\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l&#8217;} p(l&#8217; | s) \sum_{j = 1}^m f_i(s, j, l&#8217;_j, l&#8217;_{j-1})]$ where $\alpha$ is some learning rate.</li>
<li>Repeat the previous steps until some stopping condition is reached (e.g., the updates fall below some threshold).</li>
</ul>


<p>In other words, every step takes the difference between what we want the model to learn and the model&#8217;s current state, and moves $\lambda_i$ in the direction of this difference.</p>

<h1>Finding the Optimal Labeling</h1>

<p>Suppose we&#8217;ve trained our CRF model, and now a new sentence comes in. How do we do label it?</p>

<p>The naive way is to calculate $p(l | s)$ for every possible labeling l, and then choose the label that maximizes this probability. However, since there are $k^m$ possible labels for a tag set of size k and a sentence of length m, this approach would have to check an exponential number of labels.</p>

<p>A better way is to realize that (linear-chain) CRFs satisfy an <a href="http://en.wikipedia.org/wiki/Optimal_substructure">optimal substructure</a> property that allows us to use a (polynomial-time) dynamic programming algorithm to find the optimal label, similar to the <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi algorithm</a> for HMMs.</p>

<h1>A More Interesting Application</h1>

<p>Okay, so part-of-speech tagging is kind of boring, and there are plenty of existing POS taggers out there. When might you use a CRF in real life?</p>

<p>Suppose you want to mine Twitter for the types of presents people received for Christmas:</p>

<blockquote class="twitter-tweet"><p>What people on Twitter wanted for Christmas, and what they got: <a href="http://t.co/EGeKTBgF" title="http://twitter.com/edchedch/status/153683967315419136/photo/1">twitter.com/edchedch/statu…</a></p>— Edwin Chen (@edchedch) <a href="https://twitter.com/edchedch/status/153683967315419136" data-datetime="2012-01-02T03:48:10+00:00">January 2, 2012</a></blockquote>


<script src="http://blog.echen.me//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>(Yes, I just embedded a tweet. BOOM.)</p>

<p>How can you figure out which words refer to gifts?</p>

<p>To gather data for the graphs above, I simply looked for phrases of the form &#8220;I want XXX for Christmas&#8221; and &#8220;I got XXX for Christmas&#8221;. However, a more sophisticated CRF variant could use a GIFT part-of-speech-like tag (even adding other tags like GIFT-GIVER and GIFT-RECEIVER, to get even more information on who got what from whom) and treat this like a POS tagging problem. Features could be based around things like &#8220;this word is a GIFT if the previous word was a GIFT-RECEIVER and the word before that was &#8216;gave&#8217;&#8221; or &#8220;this word is a GIFT if the next two words are &#8216;for Christmas&#8217;&#8221;.</p>

<h1>Fin</h1>

<p>I&#8217;ll end with some more random thoughts:</p>

<ul>
<li>I explicitly skipped over the graphical models framework that conditional random fields sit in, because I don&#8217;t think they add much to an initial understanding of CRFs. But if you&#8217;re interested in learning more, Daphne Koller is teaching a free, online course on <a href="http://www.pgm-class.org/">graphical models</a> starting in January.</li>
<li>Or, if you&#8217;re more interested in the many NLP applications of CRFs (like part-of-speech tagging or <a href="http://en.wikipedia.org/wiki/Named-entity_recognition">named entity extraction</a>), Manning and Jurafsky are teaching an <a href="http://www.nlp-class.org/">NLP class</a> in the same spirit.</li>
<li>I also glossed a bit over the analogy between CRFs:HMMs and Logistic Regression:Naive Bayes. This image (from <a href="http://arxiv.org/pdf/1011.4088v1">Sutton and McCallum&#8217;s introduction to conditional random fields</a>) sums it up, and shows the graphical model nature of CRFs as well:</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/crfs/crf-diagram.png"><img src="http://dl.dropbox.com/u/10506/blog/crfs/crf-diagram.png" alt="CRF Diagram" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Winning the Netflix Prize: A Summary]]></title>
    <link href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/"/>
    <updated>2011-10-24T16:27:01-07:00</updated>
    <id>http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary</id>
    <content type="html"><![CDATA[<p>How was the <a href="http://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a> won? I went through a lot of the Netflix Prize papers a couple years ago, so I&#8217;ll try to give an overview of the techniques that went into the winning solution here.</p>

<h1>Normalization of Global Effects</h1>

<p>Suppose Alice rates Inception 4 stars. We can think of this rating as composed of several parts:</p>

<ul>
<li>A <strong>baseline rating</strong> (e.g., maybe the mean over all user-movie ratings is 3.1 stars).</li>
<li>An <strong>Alice-specific effect</strong> (e.g., maybe Alice tends to rate movies lower than the average user, so her ratings are -0.5 stars lower than we normally expect).</li>
<li>An <strong>Inception-specific effect</strong> (e.g., Inception is a pretty awesome movie, so its ratings are 0.7 stars higher than we normally expect).</li>
<li>A less predictable effect based on the <strong>specific interaction</strong> between Alice and Inception that accounts for the remainder of the stars (e.g., Alice really liked Inception because of its particular combination of Leonardo DiCaprio and neuroscience, so this rating gets an additional 0.7 stars).</li>
</ul>


<p>In other words, we&#8217;ve decomposed the 4-star rating into:
4 = [3.1 (the baseline rating) - 0.5 (the Alice effect) + 0.7 (the Inception effect)] + 0.7 (the specific interaction)</p>

<p>So instead of having our models predict the 4-star rating itself, we could first try to remove the effect of the baseline predictors (the first three components) and have them predict the specific 0.7 stars. (I guess you can also think of this as a simple kind of boosting.)</p>

<p>More generally, additional baseline predictors include:</p>

<ul>
<li>A factor that allows Alice&#8217;s rating to (linearly) depend on the (square root of the) <strong>number of days since her first rating</strong>. (For example, have you ever noticed that you become a harsher critic over time?)</li>
<li>A factor that allows Alice&#8217;s rating to depend on the <strong>number of days since the movie&#8217;s first rating by anyone</strong>. (If you&#8217;re one of the first people to watch it, maybe it&#8217;s because you&#8217;re a huge fan and really excited to see it on DVD, so you&#8217;ll tend to rate it higher.)</li>
<li>A factor that allows Alice&#8217;s rating to depend on the <strong>number of people who have rated Inception</strong>. (Maybe Alice is a hipster who hates being part of the crowd.)</li>
<li>A factor that allows Alice&#8217;s rating to <strong>depend on the movie&#8217;s overall rating</strong>.</li>
<li>(Plus a bunch of others.)</li>
</ul>


<p>And, in fact, modeling these biases turned out to be fairly important: in their paper describing their final solution to the Netflix Prize, Bell and Koren write that</p>

<blockquote><p>Of the numerous new algorithmic contributions, I would like to highlight one &#8211; those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as signficant as coming up with modeling breakthroughs.</p></blockquote>


<p>(For a perhaps more concrete example of why removing these biases is useful, suppose you know that Bob likes the same kinds of movies that Alice does. To predict Bob&#8217;s rating of Inception, instead of simply predicting the same 4 stars that Alice rated, if we know that Bob tends to rate movies 0.3 stars higher than average, then we could first remove Alice&#8217;s bias and then add in Bob&#8217;s: 4 + 0.5 + 0.3 = 4.8.)</p>

<h1>Neighborhood Models</h1>

<p>Let&#8217;s now look at some slightly more sophisticated models. As alluded to in the section above, one of the standard approaches to collaborative filtering is to use neighborhood models.</p>

<p>Briefly, a neighborhood model works as follows. To predict Alice&#8217;s rating of Titanic, you could do two things:</p>

<ul>
<li><strong>Item-item approach</strong>: find a set of items similar to Titanic that Alice has also rated, and take the (weighted) mean of Alice&#8217;s ratings on them.</li>
<li><strong>User-user approach</strong>: find a set of users similar to Alice who rated Titanic, and again take the mean of their ratings of Titanic.</li>
</ul>


<p>(See also my post on <a href="http://blog.echen.me/2011/02/15/an-overview-of-item-to-item-collaborative-filtering-with-amazons-recommendation-system/">item-to-item collaborative filtering on Amazon</a>.)</p>

<p>The main questions, then, are (let&#8217;s stick to the item-item approach for simplicity):</p>

<ul>
<li>How do we find the set of similar items?</li>
<li>How do we weight these items when taking their mean?</li>
</ul>


<p>The standard approach is to take some similarity metric (e.g., correlation or a Jaccard index) to define similarities between pairs of movies, take the K most similar movies under this metric (where K is perhaps chosen via cross-validation), and then use the same similarity metric when computing the weighted mean.</p>

<p>This has a couple problems:</p>

<ul>
<li><strong>Neighbors aren&#8217;t independent</strong>, so using a standard similarity metric to define a weighted mean overcounts information. For example, suppose you ask five friends where you should eat tonight. Three of them went to Mexico last week and are sick of burritos, so they strongly recommend against a taqueria. Thus, your friends&#8217; recommendations have a stronger bias than what you&#8217;d get if you asked five friends who didn&#8217;t know each other at all. (Compare with the situation where all three Lord of the Rings Movies are neighbors of Harry Potter.)</li>
<li>Different movies should perhaps be using <strong>different numbers of neighbors</strong>. Some movies may be predicted well by only one neighbor (e.g., Harry Potter 2 could be predicted well by Harry Potter 1 alone), some movies may require more, and some movies may have no good neighbors (so you should ignore your neighborhood algorithms entirely and let your other ratings models stand on their own).</li>
</ul>


<p>So another approach is the following:</p>

<ul>
<li>You can still use a similarity metric like correlation or cosine similarity to choose the set of similar items.</li>
<li>But instead of using the similarity metric to define the interpolation weights in the mean calculations, you essentially perform a (sparse) <strong>linear regression to find the weights</strong> that minimize the squared error between an item&#8217;s rating and a linear combination of the ratings of its neighbors. Note that these weights are no longer constrained, so that if all neighbors are weak, then their weights will be close to zero and the neighborhood model will have a low effect.</li>
</ul>


<p>(A slightly more complicated user-user approach, similar to this item-item neighborhood approach, is also useful.)</p>

<h1>Implicit Data</h1>

<p>Adding on to the neighborhood approach, we can also let <strong>implicit data influence our predictions</strong>. The mere fact that a user rated lots of science fiction movies but no westerns, suggests that the user likes science fiction better than cowboys. So using a similar framework as in the neighborhood ratings model, we can learn for Inception a set of <strong>offset weights</strong> associated to Inception&#8217;s movie neighbors.</p>

<p>Whenever we want to predict how Bob rates Inception, we look at whether Bob rated each of Inception&#8217;s neighbors. If he did, we add in the corresponding offset; if not, then we add nothing (and, thus, Bob&#8217;s rating is implicitly penalized by the missing weight).</p>

<h1>Matrix Factorization</h1>

<p>Complementing the neighborhood approach to collaborative filtering is the matrix factorization approach. Whereas the neighborhood approach takes a very local approach to ratings (if you liked Harry Potter 1, then you&#8217;ll like Harry Potter 2!), the factorization approach takes a more global view (we know that you like fantasy movies and that Harry Potter has a strong fantasy element, so we think that you&#8217;ll like Harry Potter) that <strong>decomposes users and movies into a set of latent factors</strong> (which we can think of as categories like &#8220;fantasy&#8221; or &#8220;violence&#8221;).</p>

<p>In fact, matrix factorization methods were probably the most important class of techniques for winning the Netflix Prize. In their 2008 Progress Prize paper, Bell and Koren write</p>

<blockquote><p>It seems that models based on matrix-factorization were found to be most accurate (and thus popular), as evident by recent publications and discussions on the Netflix Prize forum. We definitely agree to that, and would like to add that those matrix-factorization models also offer the important flexibility needed for modeling temporal effects and the binary view. Nonetheless, neighborhood models, which have been dominating most of the collaborative filtering literature, are still expected to be popular due to their practical characteristics - being able to handle new users/ratings without re-training and offering direct explanations to the recommendations.</p></blockquote>


<p>The typical way to perform matrix factorizations is to perform a <strong>singular value decomposition</strong> on the (sparse) ratings matrix (using stochastic gradient descent and regularizing the weights of the factors, possibly constraining the weights to be positive to get a type of non-negative matrix factorization). (Note that this &#8220;SVD&#8221; is a little different from the standard SVD learned in linear algebra, since not every user has rated every movie and so the ratings matrix contains many missing elements that we don&#8217;t want to simply treat as 0.)</p>

<p>Some SVD-inspired methods used in the Netflix Prize include:</p>

<ul>
<li><strong>Standard SVD</strong>: Once you&#8217;ve represented users and movies as factor vectors, you can dot product Alice&#8217;s vector with Inception&#8217;s vector to get Alice&#8217;s predicted rating of Inception.</li>
<li><strong>Asymmetric SVD</strong>: Instead of users having their own notion of factor vectors, we can represent users as a bag of items they have rated (or provided implicit feedback for). So Alice is now represented as a (possibly weighted) sum of the factor vectors of the items she has rated, and to get her predicted rating of Titanic, we can dot product this  representation with the factor vector of Titanic. From a practical perspective, this model has an added benefit in that no user parameterizations are needed, so we can use this approach to generate recommendations as soon as a user provides some feedback (which could just be views or clicks on an item, and not necessarily ratings), without needing to retrain the model to factorize the user.</li>
<li><strong>SVD++</strong>: Incorporate both the standard SVD and the asymmetric SVD model by representing users both by their own factor representation and as a bag of item vectors.</li>
</ul>


<h1>Regression</h1>

<p>Some regression models were also used in the predictions. The models are fairly standard, I think, so I won&#8217;t spend too long here. Basically, just as with the neighborhood models, we can take a user-centric approach and a movie-centric approach to regression:</p>

<ul>
<li><strong>User-centric approach</strong>: We learn a regression model for each user, using all the movies that the user rated as the dataset. The response is the movie&#8217;s rating, and the predictor variables are attributes associated to that movie (which can be derived from, say, PCA, MDS, or an SVD).</li>
<li><strong>Movie-centric approach</strong>: Similarly, we can learn a regression model for each movie, using all the users that rated the movie as the dataset.</li>
</ul>


<h1>Restricted Boltzmann Machines</h1>

<p>Restricted Boltzmann Machines provide another kind of <strong>latent factor approach</strong> that can be used. See <a href="http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf">this paper</a> for a description of how to apply them to the Netflix Prize. (In case the paper&#8217;s a little difficult to read, I wrote an <a href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines">introduction to RBMs</a> a little while ago.)</p>

<h1>Temporal Effects</h1>

<p>Many of the models incorporate temporal effects. For example, when describing the baseline predictors above, we used a few temporal predictors that allowed a user&#8217;s rating to (linearly) depend on the time since the first rating he ever made and on the time since a movie&#8217;s first rating. We can also get more fine-grained temporal effects by, say, binning items into a couple months&#8217; worth of ratings at a time, and allowing movie biases to change within each bin. (For example, maybe in May 2006, Time Magazine nominated Titanic as the best movie ever made, which caused a spurt in glowing ratings around that time.)</p>

<p>In the matrix factorization approach, user factors were also allowed to be time-dependent (e.g., maybe Bob comes to like comedy movies more and more over time). We can also give more weight to recent user actions.</p>

<h1>Regularization</h1>

<p>Regularization was also applied throughout pretty much all the models learned, to <strong>prevent overfitting</strong> on the dataset. Ridge regression was heavily used in the factorization models to penalize large weights, and lasso regression (though less effective) was useful as well. Many other parameters (e.g., the baseline predictors, similarity weights and interpolation weights in the neighborhood models) were also estimated using fairly standard shrinkage techniques.</p>

<h1>Ensemble Methods</h1>

<p>Finally, let&#8217;s talk about how all of these different algorithms were combined to provide a single rating that <strong>exploits the strengths of each model</strong>. (Note that, as mentioned above, many of these models were not trained on the raw ratings data directly, but rather on the residuals of other models.)</p>

<p>In the paper detailing their final solution, the winners describe using <strong>gradient boosted decision trees to combine over 500 models</strong>; previous solutions used instead a <strong>linear regression</strong> to combine the predictors.</p>

<p>Briefly, gradient boosted decision trees work by sequentially fitting a series of decision trees to the data; each tree is asked to predict the error made by the previous trees, and is often trained on slightly perturbed versions of the data. (For a longer description of a similar technique, see <a href="http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/">my introduction to random forests</a>.)</p>

<p>Since GBDTs have a built-in ability to apply different methods to different slices of the data, we can add in some predictors that help the trees make useful clusterings:</p>

<ul>
<li>Number of movies each user rated</li>
<li>Number of users that rated each movie</li>
<li>Factor vectors of users and movies</li>
<li>Hidden units of a restricted Boltzmann Machine</li>
</ul>


<p>(For example, one thing that Bell and Koren found (when using an earlier ensemble method) was that RBMs are more useful when the movie or the user has a low number of ratings, and that matrix factorization methods are more useful when the movie or user has a high number of ratings.)</p>

<p>Here&#8217;s a graph of the effect of ensemble size from early on in the competition (in 2007), and the authors&#8217; take on it:</p>

<p><a href="http://www2.research.att.com/~volinsky/netflix/newensemble.gif"><img src="http://www2.research.att.com/~volinsky/netflix/newensemble.gif" alt="Ensemble Size vs. RMSE" /></a></p>

<blockquote><p>However, we would like to stress that it is not necessary to have such a large number of models to do well. The plot below shows RMSE as a function of the number of methods used. One can achieve our winning score (RMSE=0.8712) with less than 50 methods, using the best 3 methods can yield RMSE < 0.8800, which would land in the top 10. Even just using our single best method puts us on the leaderboard with an RMSE of 0.8890. The lesson here is that having lots of models is useful for the incremental results needed to win competitions, but practically, excellent systems can be built with just a few well-selected models.</p></blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stuff Harvard People Like]]></title>
    <link href="http://blog.echen.me/2011/09/29/stuff-harvard-people-like/"/>
    <updated>2011-09-29T10:48:09-07:00</updated>
    <id>http://blog.echen.me/2011/09/29/stuff-harvard-people-like</id>
    <content type="html"><![CDATA[<p>What types of students go to which schools? There are, of course, the classic stereotypes:</p>

<ul>
<li><strong>MIT</strong> has the hacker engineers.</li>
<li><strong>Stanford</strong> has the laid-back, social folks.</li>
<li><strong>Harvard</strong> has the prestigious leaders of the world.</li>
<li><strong>Berkeley</strong> has the activist hippies.</li>
<li><strong>Caltech</strong> has the hardcore science nerds.</li>
</ul>


<p>But how well do these perceptions match reality? What are students at Stanford, Harvard, MIT, Caltech, and Berkeley <em>really</em> interested in? Following the path of my previous data-driven post on <a href="http://blog.echen.me/2011/04/18/twifferences-between-californians-and-new-yorkers/">differences between Silicon Valley and NYC</a>, I scraped the Quora profiles of a couple hundred followers of each school to find out.</p>

<h1>Topics</h1>

<p>So let&#8217;s look at what kinds of topics followers of each school are interested in*. (Skip past the lists for a discussion.)</p>

<h2>MIT</h2>

<p>Topics are followed by p(school = MIT|topic).</p>

<ul>
<li><strong>MIT Media Lab</strong>                             0.893</li>
<li><strong>Ksplice</strong>                                   0.69</li>
<li><strong>Lisp (programming language)</strong>               0.677</li>
<li><strong>Nokia</strong>                                     0.659</li>
<li><strong>Public Speaking</strong>                           0.65</li>
<li><strong>Data Storage</strong>                              0.65</li>
<li><strong>Google Voice</strong>                              0.609</li>
<li><strong>Hacking</strong>                                   0.602</li>
<li><strong>Startups in Europe</strong>                        0.597</li>
<li><strong>Startup Names</strong>                             0.572</li>
<li><strong>Mechanical Engineering</strong>                    0.563</li>
<li><strong>Engineering</strong>                               0.563</li>
<li><strong>Distributed Databases</strong>                     0.544</li>
<li><strong>StackOverflow</strong>                             0.536</li>
<li><strong>Boston</strong>                                    0.513</li>
<li><strong>Learning</strong>                                  0.507</li>
<li><strong>Open Source</strong>                               0.498</li>
<li><strong>Cambridge</strong>                                 0.496</li>
<li><strong>Public Relations</strong>                          0.493</li>
<li><strong>Visualization</strong>                             0.492</li>
<li><strong>Semantic Web</strong>                              0.486</li>
<li><strong>Andreessen-Horowitz</strong>                       0.483</li>
<li><strong>Nature</strong>                                    0.475</li>
<li><strong>Cryptography</strong>                              0.474</li>
<li><strong>Startups in Boston</strong>                        0.452</li>
<li><strong>Adobe Photoshop</strong>                           0.451</li>
<li><strong>Computer Security</strong>                         0.447</li>
<li><strong>Sachin Tendulkar</strong>                          0.443</li>
<li><strong>Hacker News</strong>                               0.442</li>
<li><strong>Games</strong>                                     0.429</li>
<li><strong>Android Applications</strong>                      0.428</li>
<li><strong>Best Engineers and Programmers</strong>            0.427</li>
<li><strong>College Admissions &amp; Getting Into College</strong> 0.422</li>
<li><strong>Co-Founders</strong>                               0.419</li>
<li><strong>Big Data</strong>                                  0.41</li>
<li><strong>System Administration</strong>                     0.4</li>
<li><strong>Biotechnology</strong>                             0.398</li>
<li><strong>Higher Education</strong>                          0.394</li>
<li><strong>NoSQL</strong>                                     0.387</li>
<li><strong>User Experience</strong>                           0.386</li>
<li><strong>Career Advice</strong>                             0.377</li>
<li><strong>Artificial Intelligence</strong>                   0.375</li>
<li><strong>Scalability</strong>                               0.37</li>
<li><strong>Taylor Swift</strong>                              0.368</li>
<li><strong>Google Search</strong>                             0.368</li>
<li><strong>Functional Programming</strong>                    0.365</li>
<li><strong>Bing</strong>                                      0.363</li>
<li><strong>Bioinformatics</strong>                            0.361</li>
<li><strong>How I Met Your Mother (TV series)</strong>         0.361</li>
<li><strong>Operating Systems</strong>                         0.356</li>
<li><strong>Compilers</strong>                                 0.355</li>
<li><strong>Google Chrome</strong>                             0.354</li>
<li><strong>Management &amp; Organizational Leadership</strong>    0.35</li>
<li><strong>Literary Fiction</strong>                          0.35</li>
<li><strong>Intelligence</strong>                              0.348</li>
<li><strong>Fight Club (1999 movie)</strong>                   0.344</li>
<li><strong>Hip Hop Music</strong>                             0.34</li>
<li><strong>UX Design</strong>                                 0.337</li>
<li><strong>Web Application Frameworks</strong>                0.336</li>
<li><strong>Startups in New York City</strong>                 0.333</li>
<li><strong>Book Recommendations</strong>                      0.33</li>
<li><strong>Engineering Recruiting</strong>                    0.33</li>
<li><strong>Search Engines</strong>                            0.329</li>
<li><strong>Social Search</strong>                             0.329</li>
<li><strong>Data Science</strong>                              0.328</li>
<li><strong>History</strong>                                   0.328</li>
<li><strong>Interaction Design</strong>                        0.326</li>
<li><strong>Classification (machine learning)</strong>         0.322</li>
<li><strong>Startup Incubators and Seed Programs</strong>      0.321</li>
<li><strong>Graphic Design</strong>                            0.321</li>
<li><strong>Product Design (software)</strong>                 0.319</li>
<li><strong>The College Experience</strong>                    0.319</li>
<li><strong>Writing</strong>                                   0.319</li>
<li><strong>MapReduce</strong>                                 0.318</li>
<li><strong>Database Systems</strong>                          0.315</li>
<li><strong>User Interfaces</strong>                           0.314</li>
<li><strong>Literature</strong>                                0.314</li>
<li><strong>C (programming language)</strong>                  0.314</li>
<li><strong>Television</strong>                                0.314</li>
<li><strong>Reading</strong>                                   0.313</li>
<li><strong>Usability</strong>                                 0.312</li>
<li><strong>Books</strong>                                     0.312</li>
<li><strong>Computers</strong>                                 0.311</li>
<li><strong>Stealth Startups</strong>                          0.311</li>
<li><strong>Daft Punk</strong>                                 0.31</li>
<li><strong>Healthy Eating</strong>                            0.309</li>
<li><strong>Innovation</strong>                                0.309</li>
<li><strong>Skiing</strong>                                    0.305</li>
<li><strong>JavaScript</strong>                                0.304</li>
<li><strong>Rock Music</strong>                                0.304</li>
<li><strong>Mozilla Firefox</strong>                           0.304</li>
<li><strong>Self-Improvement</strong>                          0.303</li>
<li><strong>McKinsey &amp; Company</strong>                        0.302</li>
<li><strong>AngelList</strong>                                 0.301</li>
<li><strong>Data Visualization</strong>                        0.301</li>
<li><strong>Cassandra (database)</strong>                      0.301</li>
</ul>


<h2>Stanford</h2>

<p>Topics are followed by p(school = Stanford|topic).</p>

<ul>
<li><strong>Stanford Computer Science</strong>                 0.951</li>
<li><strong>Stanford Graduate School of Business</strong>      0.939</li>
<li><strong>Stanford</strong>                                  0.896</li>
<li><strong>Stanford Football</strong>                         0.896</li>
<li><strong>Stanford Cardinal</strong>                         0.896</li>
<li><strong>Social Dance</strong>                              0.847</li>
<li><strong>Stanford University Courses</strong>               0.847</li>
<li><strong>Romance</strong>                                   0.769</li>
<li><strong>Instagram</strong>                                 0.745</li>
<li><strong>College Football</strong>                          0.665</li>
<li><strong>Mobile Location Applications</strong>              0.634</li>
<li><strong>Online Communities</strong>                        0.621</li>
<li><strong>Interpersonal Relationships</strong>               0.585</li>
<li><strong>Food &amp; Restaurants in Palo Alto</strong>           0.572</li>
<li><strong>Your 20s</strong>                                  0.566</li>
<li><strong>Men&#8217;s Fashion</strong>                             0.548</li>
<li><strong>Flipboard</strong>                                 0.537</li>
<li><strong>Inception (2010 movie)</strong>                    0.535</li>
<li><strong>Tumblr</strong>                                    0.531</li>
<li><strong>People Skills</strong>                             0.522</li>
<li><strong>Exercise</strong>                                  0.52</li>
<li><strong>Joel Spolsky</strong>                              0.516</li>
<li><strong>Valuations</strong>                                0.515</li>
<li><strong>The Social Network (2010 movie)</strong>           0.513</li>
<li><strong>LeBron James</strong>                              0.506</li>
<li><strong>Northern California</strong>                       0.506</li>
<li><strong>Evernote</strong>                                  0.5</li>
<li><strong>Quora Community</strong>                           0.5</li>
<li><strong>Blogging</strong>                                  0.49</li>
<li><strong>Downtown Palo Alto</strong>                        0.487</li>
<li><strong>The College Experience</strong>                    0.485</li>
<li><strong>Consumer Internet</strong>                         0.477</li>
<li><strong>Restaurants in San Francisco</strong>              0.477</li>
<li><strong>Chad Hurley</strong>                               0.47</li>
<li><strong>Meditation</strong>                                0.468</li>
<li><strong>Yishan Wong</strong>                               0.466</li>
<li><strong>Arrested Development (TV series)</strong>          0.463</li>
<li><strong>fbFund</strong>                                    0.457</li>
<li><strong>Best Engineers at X Company</strong>               0.451</li>
<li><strong>Language</strong>                                  0.45</li>
<li><strong>Words</strong>                                     0.448</li>
<li><strong>Happiness</strong>                                 0.447</li>
<li><strong>Path (company)</strong>                            0.446</li>
<li><strong>Color Labs (startup)</strong>                      0.446</li>
<li><strong>Palo Alto</strong>                                 0.445</li>
<li><strong>Woot.com</strong>                                  0.442</li>
<li><strong>Beer</strong>                                      0.442</li>
<li><strong>PayPal</strong>                                    0.441</li>
<li><strong>Women in Startups</strong>                         0.438</li>
<li><strong>Techmeme</strong>                                  0.433</li>
<li><strong>Women in Engineering</strong>                      0.428</li>
<li><strong>The Mission (San Francisco neighborhood)</strong>  0.427</li>
<li><strong>iPhone Applications</strong>                       0.416</li>
<li><strong>Asana</strong>                                     0.413</li>
<li><strong>Monetization</strong>                              0.412</li>
<li><strong>Repetitive Strain Injury (RSI)</strong>            0.4</li>
<li><strong>IDEO</strong>                                      0.398</li>
<li><strong>Spotify</strong>                                   0.397</li>
<li><strong>San Francisco Giants</strong>                      0.396</li>
<li><strong>Fortune Magazine</strong>                          0.389</li>
<li><strong>Love</strong>                                      0.387</li>
<li><strong>Human-Computer Interaction</strong>                0.382</li>
<li><strong>Hip Hop Music</strong>                             0.378</li>
<li><strong>Self-Improvement</strong>                          0.378</li>
<li><strong>Food in San Francisco</strong>                     0.375</li>
<li><strong>Quora (company)</strong>                           0.374</li>
<li><strong>Quora Infrastructure</strong>                      0.373</li>
<li><strong>iPhone</strong>                                    0.371</li>
<li><strong>Square (company)</strong>                          0.369</li>
<li><strong>Social Psychology</strong>                         0.369</li>
<li><strong>Network Effects</strong>                           0.366</li>
<li><strong>Chris Sacca</strong>                               0.365</li>
<li><strong>Walt Mossberg</strong>                             0.364</li>
<li><strong>Salesforce.com</strong>                            0.362</li>
<li><strong>Sex</strong>                                       0.361</li>
<li><strong>Etiquette</strong>                                 0.361</li>
<li><strong>David Pogue</strong>                               0.361</li>
<li><strong>Gowalla</strong>                                   0.36</li>
<li><strong>iOS Development</strong>                           0.354</li>
<li><strong>Palantir Technologies</strong>                     0.353</li>
<li><strong>Mobile Computing</strong>                          0.347</li>
<li><strong>Sports</strong>                                    0.346</li>
<li><strong>Video Games</strong>                               0.345</li>
<li><strong>Burning Man</strong>                               0.345</li>
<li><strong>Engineering Management</strong>                    0.343</li>
<li><strong>Cognitive Science</strong>                         0.342</li>
<li><strong>Dating &amp; Relationships</strong>                    0.341</li>
<li><strong>Fred Wilson (venture investor)</strong>            0.337</li>
<li><strong>Taiwan</strong>                                    0.333</li>
<li><strong>Natural Language Processing</strong>               0.33</li>
<li><strong>Eric Schmidt</strong>                              0.329</li>
<li><strong>Social Advice</strong>                             0.329</li>
<li><strong>Engineering Recruiting</strong>                    0.328</li>
<li><strong>Job Interviews</strong>                            0.325</li>
<li><strong>Mobile Phones</strong>                             0.324</li>
<li><strong>Twitter Inc. (company)</strong>                    0.321</li>
<li><strong>Engineering in Silicon Valley</strong>             0.321</li>
<li><strong>San Francisco Bay Area</strong>                    0.321</li>
<li><strong>Google Analytics</strong>                          0.32</li>
<li><strong>Fashion</strong>                                   0.315</li>
<li><strong>Interaction Design</strong>                        0.314</li>
<li><strong>Open Graph</strong>                                0.313</li>
<li><strong>Drugs &amp; Pharmaceuticals</strong>                   0.312</li>
<li><strong>Electronic Music</strong>                          0.312</li>
<li><strong>Facebook Inc. (company)</strong>                   0.309</li>
<li><strong>Fitness</strong>                                   0.309</li>
<li><strong>YouTube</strong>                                   0.308</li>
<li><strong>TED Talks</strong>                                 0.308</li>
<li><strong>Freakonomics (2005 Book)</strong>                  0.307</li>
<li><strong>Jack Dorsey</strong>                               0.306</li>
<li><strong>Nutrition</strong>                                 0.305</li>
<li><strong>Puzzles</strong>                                   0.305</li>
<li><strong>Silicon Valley Mergers &amp; Acquisitions</strong>     0.304</li>
<li><strong>Viral Growth &amp; Analytics</strong>                  0.304</li>
<li><strong>Amazon Web Services</strong>                       0.304</li>
<li><strong>StumbleUpon</strong>                               0.303</li>
<li><strong>Exceptional Comment Threads</strong>               0.303</li>
</ul>


<h2>Harvard</h2>

<ul>
<li><strong>Harvard Business School</strong>                   0.968</li>
<li><strong>Harvard Business Review</strong>                   0.922</li>
<li><strong>Harvard Square</strong>                            0.912</li>
<li><strong>Harvard Law School</strong>                        0.912</li>
<li><strong>Jimmy Fallon</strong>                              0.899</li>
<li><strong>Boston Red Sox</strong>                            0.658</li>
<li><strong>Klout</strong>                                     0.644</li>
<li><strong>Oprah Winfrey</strong>                             0.596</li>
<li><strong>Ivanka Trump</strong>                              0.587</li>
<li><strong>Dalai Lama</strong>                                0.569</li>
<li><strong>Food in New York City</strong>                     0.565</li>
<li><strong>U2</strong>                                        0.562</li>
<li><strong>TwitPic</strong>                                   0.534</li>
<li><strong>37signals</strong>                                 0.522</li>
<li><strong>David Lynch (director)</strong>                    0.512</li>
<li><strong>Al Gore</strong>                                   0.508</li>
<li><strong>TechStars</strong>                                 0.49</li>
<li><strong>Baseball</strong>                                  0.487</li>
<li><strong>Private Equity</strong>                            0.471</li>
<li><strong>Classical Music</strong>                           0.46</li>
<li><strong>Startups in New York City</strong>                 0.458</li>
<li><strong>HootSuite</strong>                                 0.449</li>
<li><strong>Kiva</strong>                                      0.442</li>
<li><strong>Ultimate Frisbee</strong>                          0.441</li>
<li><strong>Huffington Post</strong>                           0.436</li>
<li><strong>New York City</strong>                             0.433</li>
<li><strong>Charlie Cheever</strong>                           0.433</li>
<li><strong>The New York Times</strong>                        0.431</li>
<li><strong>Technology Journalism</strong>                     0.431</li>
<li><strong>McKinsey &amp; Company</strong>                        0.427</li>
<li><strong>TweetDeck</strong>                                 0.422</li>
<li><strong>How Does X Work?</strong>                          0.417</li>
<li><strong>Ashton Kutcher</strong>                            0.414</li>
<li><strong>Coldplay</strong>                                  0.402</li>
<li><strong>Conan O&#8217;Brien</strong>                             0.397</li>
<li><strong>Fast Company</strong>                              0.397</li>
<li><strong>WikiLeaks</strong>                                 0.394</li>
<li><strong>Michael Jackson</strong>                           0.389</li>
<li><strong>Guy Kawasaki</strong>                              0.389</li>
<li><strong>Journalism</strong>                                0.384</li>
<li><strong>Wall Street Journal</strong>                       0.384</li>
<li><strong>Cambridge</strong>                                 0.371</li>
<li><strong>Seattle</strong>                                   0.37</li>
<li><strong>Cities &amp; Metro Areas</strong>                      0.357</li>
<li><strong>Boston</strong>                                    0.353</li>
<li><strong>Tim Ferriss (author)</strong>                      0.35</li>
<li><strong>The New Yorker</strong>                            0.343</li>
<li><strong>Law</strong>                                       0.34</li>
<li><strong>Mashable</strong>                                  0.338</li>
<li><strong>Politics</strong>                                  0.335</li>
<li><strong>The Economist</strong>                             0.334</li>
<li><strong>Barack Obama</strong>                              0.333</li>
<li><strong>Skiing</strong>                                    0.329</li>
<li><strong>McKinsey Quarterly</strong>                        0.325</li>
<li><strong>Wired (magazine)</strong>                          0.316</li>
<li><strong>Bill Gates</strong>                                0.31</li>
<li><strong>Mad Men (TV series)</strong>                       0.308</li>
<li><strong>India</strong>                                     0.306</li>
<li><strong>TED Talks</strong>                                 0.306</li>
<li><strong>Netflix</strong>                                   0.304</li>
<li><strong>Wine</strong>                                      0.303</li>
<li><strong>Angel Investors</strong>                           0.302</li>
<li><strong>Facebook Ads</strong>                              0.301</li>
</ul>


<h2>UC Berkeley</h2>

<ul>
<li><strong>Berkeley</strong>                                  0.978</li>
<li><strong>UC Riverside</strong>                              0.92</li>
<li><strong>California Golden Bears</strong>                   0.91</li>
<li><strong>Internships</strong>                               0.717</li>
<li><strong>Web Marketing</strong>                             0.484</li>
<li><strong>Google Social Strategy</strong>                    0.453</li>
<li><strong>Southwest Airlines</strong>                        0.451</li>
<li><strong>WordPress</strong>                                 0.429</li>
<li><strong>Stock Market</strong>                              0.429</li>
<li><strong>BMW (automobile)</strong>                          0.428</li>
<li><strong>Web Applications</strong>                          0.423</li>
<li><strong>Flickr</strong>                                    0.422</li>
<li><strong>Snowboarding</strong>                              0.42</li>
<li><strong>Electronic Music</strong>                          0.404</li>
<li><strong>MySQL</strong>                                     0.401</li>
<li><strong>Internet Advertising</strong>                      0.399</li>
<li><strong>Search Engine Optimization (SEO)</strong>          0.398</li>
<li><strong>Yelp</strong>                                      0.396</li>
<li><strong>Groupon</strong>                                   0.393</li>
<li><strong>In-N-Out Burger</strong>                           0.391</li>
<li><strong>The Matrix (1999 movie)</strong>                   0.389</li>
<li><strong>Trading (finance)</strong>                         0.385</li>
<li><strong>jQuery</strong>                                    0.381</li>
<li><strong>Hedge Funds</strong>                               0.378</li>
<li><strong>Social Media Marketing</strong>                    0.377</li>
<li><strong>San Francisco</strong>                             0.376</li>
<li><strong>Stealth Startups</strong>                          0.362</li>
<li><strong>Yahoo!</strong>                                    0.36</li>
<li><strong>Cascading Style Sheets</strong>                    0.359</li>
<li><strong>Angel Investors</strong>                           0.355</li>
<li><strong>UX Design</strong>                                 0.35</li>
<li><strong>StarCraft</strong>                                 0.348</li>
<li><strong>Los Angeles Lakers</strong>                        0.347</li>
<li><strong>Mountain View</strong>                             0.345</li>
<li><strong>How I Met Your Mother (TV series)</strong>         0.338</li>
<li><strong>Google+</strong>                                   0.337</li>
<li><strong>Ruby on Rails</strong>                             0.333</li>
<li><strong>Reading</strong>                                   0.333</li>
<li><strong>Social Media</strong>                              0.326</li>
<li><strong>China</strong>                                     0.322</li>
<li><strong>Palantir Technologies</strong>                     0.319</li>
<li><strong>Facebook Platform</strong>                         0.315</li>
<li><strong>Basketball</strong>                                0.315</li>
<li><strong>Education</strong>                                 0.314</li>
<li><strong>Business Development</strong>                      0.312</li>
<li><strong>Online &amp; Mobile Payments</strong>                  0.305</li>
<li><strong>Restaurants in San Francisco</strong>              0.302</li>
<li><strong>Technology Companies</strong>                      0.302</li>
<li><strong>Seth Godin</strong>                                0.3</li>
</ul>


<h2>Caltech</h2>

<ul>
<li><strong>Pasadena</strong>                                  0.969</li>
<li><strong>Chess</strong>                                     0.748</li>
<li><strong>Table Tennis</strong>                              0.671</li>
<li><strong>UCLA</strong>                                      0.67</li>
<li><strong>MacBook Pro</strong>                               0.618</li>
<li><strong>Physics</strong>                                   0.618</li>
<li><strong>Haskell</strong>                                   0.582</li>
<li><strong>Los Angeles</strong>                               0.58</li>
<li><strong>Electrical Engineering</strong>                    0.567</li>
<li><strong>Star Trek (movie</strong>                          0.561</li>
<li><strong>Disruptive Technology</strong>                     0.545</li>
<li><strong>Science</strong>                                   0.53</li>
<li><strong>Biology</strong>                                   0.526</li>
<li><strong>Quantum Mechanics</strong>                         0.521</li>
<li><strong>LaTeX</strong>                                     0.514</li>
<li><strong>Mathematics</strong>                               0.488</li>
<li><strong>xkcd</strong>                                      0.488</li>
<li><strong>Genetics &amp; Heredity</strong>                       0.487</li>
<li><strong>Chemistry</strong>                                 0.47</li>
<li><strong>Medicine &amp; Healthcare</strong>                     0.448</li>
<li><strong>Poker</strong>                                     0.445</li>
<li><strong>C++ (programming language)</strong>                0.442</li>
<li><strong>Data Structures</strong>                           0.434</li>
<li><strong>Emacs</strong>                                     0.428</li>
<li><strong>MongoDB</strong>                                   0.423</li>
<li><strong>Neuroscience</strong>                              0.404</li>
<li><strong>Science Fiction</strong>                           0.4</li>
<li><strong>Mac OS X</strong>                                  0.394</li>
<li><strong>Board Games</strong>                               0.387</li>
<li><strong>Computers</strong>                                 0.386</li>
<li><strong>Research</strong>                                  0.385</li>
<li><strong>Finance</strong>                                   0.385</li>
<li><strong>The Future</strong>                                0.379</li>
<li><strong>Linux</strong>                                     0.378</li>
<li><strong>The Colbert Report</strong>                        0.376</li>
<li><strong>The Beatles</strong>                               0.374</li>
<li><strong>The Onion</strong>                                 0.365</li>
<li><strong>Ruby</strong>                                      0.363</li>
<li><strong>Cars &amp; Automobiles</strong>                        0.361</li>
<li><strong>Quantitative Finance</strong>                      0.359</li>
<li><strong>Academia</strong>                                  0.359</li>
<li><strong>Law</strong>                                       0.355</li>
<li><strong>Cooking</strong>                                   0.354</li>
<li><strong>Psychology</strong>                                0.349</li>
<li><strong>Eminem</strong>                                    0.347</li>
<li><strong>Football (Soccer)</strong>                         0.346</li>
<li><strong>Computer Programming</strong>                      0.343</li>
<li><strong>Algorithms</strong>                                0.343</li>
<li><strong>Evolutionary Biology</strong>                      0.337</li>
<li><strong>Behavioral Economics</strong>                      0.335</li>
<li><strong>California</strong>                                0.329</li>
<li><strong>Machine Learning</strong>                          0.326</li>
<li><strong>Futurama</strong>                                  0.324</li>
<li><strong>Social Advice</strong>                             0.324</li>
<li><strong>StarCraft II</strong>                              0.319</li>
<li><strong>Job Interview Questions</strong>                   0.318</li>
<li><strong>Game Theory</strong>                               0.316</li>
<li><strong>This American Life</strong>                        0.315</li>
<li><strong>Economics</strong>                                 0.314</li>
<li><strong>Vim</strong>                                       0.31</li>
<li><strong>Graduate School</strong>                           0.309</li>
<li><strong>Git (revision control)</strong>                    0.306</li>
<li><strong>Computer Science</strong>                          0.303</li>
</ul>


<p>What do we see?</p>

<ul>
<li>First, in a nice validation of this approach, we find that each school is interested in exactly the <strong>locations</strong> we&#8217;d expect: Caltech is interested in <em>Pasadena</em> and <em>Los Angeles</em>; MIT and Harvard are both interested in <em>Boston</em> and <em>Cambridge</em> (Harvard is interested in <em>New York City</em> as well); Stanford is interested in <em>Palo Alto</em>, <em>Northern California</em>, and <em>San Francisco Bay Area</em>; and Berkeley is interested in <em>Berkeley</em>, <em>San Francisco</em>, and <em>Mountain View</em>.</li>
<li>More interestingly, let&#8217;s look at where each school likes to <strong>eat</strong>. Stereotypically, we expect Harvard, Stanford, and Berkeley students to be more outgoing and social, and MIT and Caltech students to be more introverted. This is indeed what we find:

<ul>
<li>Harvard follows <em>Food in New York City</em>; Stanford follows <em>Food &amp; Restaurants in Palo Alto</em>, <em>Restaurants in San Francisco</em>, and <em>Food in San Francisco</em>; and Berkeley follows <em>Restaurants in San Francisco</em> and <em>In-N-Out Burger</em>. In other words, Harvard, Stanford, and Berkeley love eating out.</li>
<li>Caltech, on the other hand, loves <em>Cooking</em>, and MIT loves <em>Healthy Eating</em> &#8211; both signs, perhaps, of a preference for eating in.</li>
</ul>
</li>
<li>And what does each university use to quench their <strong>thirst</strong>? Harvard students like to drink <em>wine</em> (classy!), while Stanford students prefer <em>beer</em> (the social drink of choice).</li>
<li>What about <strong>sports teams</strong>? MIT and Caltech couldn&#8217;t care less, though Harvard follows the <em>Boston Red Sox</em>, Stanford follows the <em>San Francisco Giants</em> (as well as their own <em>Stanford Football</em> and <em>Stanford Cardinal</em>), and Berkeley follows the <em>Los Angeles Lakers</em> (and the <em>California Golden Bears</em>).</li>
<li>For <strong>sports</strong> themselves, MIT students like <em>skiing</em>; Stanford students like <em>general exercise</em>, <em>fitness</em>, and <em>sports</em>; Harvard students like <em>baseball</em>, <em>ultimate frisbee</em>, and <em>skiing</em>; and Berkeley students like <em>snowboarding</em>. Caltech, in a league of its own, enjoys <em>table tennis</em> and <em>chess</em>.</li>
<li>What does each school think of <strong>social</strong>? Caltech students look for Social <em>Advice</em>. Berkeley students are interested in Social <em>Media</em> and Social Media Marketing. MIT, on the more technical side, wants Social <em>Search</em>. Stanford students, predictably, love the whole spectrum of social offerings, from Social <em>Dance</em> and <em>The Social Network</em>, to Social <em>Psychology</em> and Social <em>Advice</em>. (Interestingly, Caltech and Stanford are both interested in Social Advice, though I wonder if it&#8217;s for slightly different reasons.)</li>
<li>What&#8217;s each school&#8217;s relationship with <strong>computers</strong>? Caltech students are interested in Computer <em>Science</em>, MIT hackers are interested in Computer <em>Security</em>, and Stanford students are interested in Human-Computer <em>Interaction</em>.</li>
<li>Digging into the <strong>MIT vs. Caltech</strong> divide a little, we see that Caltech students really are more interested in the pure sciences (<em>Physics, Science, Biology, Quantum Mechanics, Mathematics, Chemistry</em>, etc.), while MIT students are more on the applied and engineering sides (<em>Mechanical Engineering, Engineering, Distributed Databases, Cryptography, Computer Security, Biotechnology, Operating Systems, Compilers</em>, etc.).</li>
<li>Regarding <strong>programming languages</strong>, Caltech students love <em>Haskell</em> (hardcore purity!), while MIT students love <em>Lisp</em>.</li>
<li>What does each school like to <strong>read</strong>, both offline and online? Caltech loves <em>science fiction</em>, <em>xkcd</em>, and <em>The Onion</em>; MIT likes <em>Hacker News</em>; Harvard loves journals, newspapers, and magazines (<em>Huffington Post</em>, the <em><a href="http://stuffwhitepeoplelike.com/2008/01/31/45-the-sunday-new-york-times/">New York Times</a></em>, <em>Fortune, Wall Street Journal, the New Yorker, the Economist</em>, and so on); and Stanford likes <em>TechMeme</em>.</li>
<li>What <strong>movies and television shows</strong> does each school like to watch? Caltech likes <em>Star Trek</em>, the <em>Colbert Report</em>, and <em>Futurama</em>. MIT likes <em>Fight Club</em> (I don&#8217;t know what this has to do with MIT, though I will note that on my first day as a freshman in a new dorm, Fight Club was precisely the movie we all went to a lecture hall to see). Stanford likes <em>The Social Network</em> and <em>Inception</em>. Harvard, rather fittingly, likes <em><a href="http://stuffwhitepeoplelike.com/2009/03/11/123-mad-men/">Mad Men</a></em> and <em><a href="http://stuffwhitepeoplelike.com/2010/09/08/134-the-ted-conference/">Ted Talks</a></em>.</li>
<li>Let&#8217;s look at the <strong>startups</strong> each school follows. MIT, of course, likes <em>Ksplice</em>. Berkeley likes <em>Yelp</em> and <em>Groupon</em>. Stanford likes just about every startup under the sun (<em>Instagram, Flipboard, Tumblr, Path, Color Labs</em>, etc.). And Harvard, that bastion of hard-won influence and prestige? To the surprise of precisely no one, Harvard enjoys <em>Klout</em>.</li>
</ul>


<p>Let&#8217;s end with a summarized view of each school:</p>

<ul>
<li><strong>Caltech</strong> is very much into the sciences (<em>Physics, Biology, Quantum Mechanics, Mathematics</em>, etc.), as well as many pretty nerdy topics (<em>Star Trek, Science Fiction, xkcd, Futurama, Starcraft II</em>, etc.).</li>
<li><strong>MIT</strong> is dominated by everything engineering and tech.</li>
<li><strong>Stanford</strong> loves relationships (<em>interpersonal relationships, people skills, love, network effects, sex, etiquette, dating and relationships, romance</em>), health and appearance (<em>fashion, fitness, nutrition, happiness</em>), and startups (<em>Instagram, Flipboard, Path, Color Labs</em>, etc.).</li>
<li><strong>Berkeley</strong>, sadly, is perhaps too large and diverse for an overall characterization.</li>
<li><strong>Harvard</strong> students are fascinated by famous figures (<em>Jimmy Fallon, Oprah Winfrey, Invaka Trump, Dalai Lama, David Lynch, Al Gore, Bill Gates, Barack Obama</em>), and by prestigious newspapers, journals, and magazines (<em>Fortune, the New York Times, the Wall Street Journal, the Economist</em>, and so on). Other very fitting interests include <em><a href="http://stuffwhitepeoplelike.com/2008/01/21/12-non-profit-organizations/">Kiva</a>, <a href="http://stuffwhitepeoplelike.com/2008/09/01/108-appearing-to-enjoy-classical-music/">classical music</a></em>, and <em><a href="http://www.vanityfair.com/online/daily/2008/06/coldplay">Coldplay</a></em>.</li>
</ul>


<p>*I pulled about 400 followers from each school, and added a couple filters, to try to ensure that followers were actual attendees of the schools rather than general people simply interested in them. Topics are sorted using a naive Bayes score and filtered to have at least 5 counts. Also, a word of warning: my dataset was fairly small and users on Quora are almost certainly not representative of their schools as a whole (though I tried to be rigorous with what I had).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Information Transmission in a Social Network: Dissecting the Spread of a Quora Post]]></title>
    <link href="http://blog.echen.me/2011/09/07/information-transmission-in-a-social-network-dissecting-the-spread-of-a-quora-post/"/>
    <updated>2011-09-07T11:15:11-07:00</updated>
    <id>http://blog.echen.me/2011/09/07/information-transmission-in-a-social-network-dissecting-the-spread-of-a-quora-post</id>
    <content type="html"><![CDATA[<p><strong>tl;dr</strong> See <a href="http://www.youtube.com/watch?v=cZ4Ntg4jQHw">this movie visualization</a> for a case study on how a post propagates through Quora.</p>

<p>How does information spread through a network? Much of Quora&#8217;s appeal, after all, lies in its social graph &#8211; and when you&#8217;ve got a network of users, all broadcasting their activities to their neighbors, information can cascade in multiple ways. How do these social designs affect which users see what?</p>

<p>Think, for example, of what happens when your kid learns a new slang word at school. He doesn&#8217;t confine his use of the word to McKinley Elementary&#8217;s particular boundaries, between the times of 9-3pm &#8211; he introduces it to his friends from other schools at soccer practice as well. A couple months later, he even says it at home for the first time; you like the word so much, you then start using it at work. Eventually, Justin Bieber uses the word in a song, at which point the word&#8217;s popularity really starts to explode.</p>

<p>So how does information propagate through a social network? What types of people does an answer on Quora reach, and how does it reach them? (Do users discover new answers individually, or are hubs of connectors more key?) How does the activity of a post on Quora rise and fall? (Submissions on other sites have limited lifetimes, fading into obscurity soon after an initial spike; how does that change when users are connected and every upvote can revive a post for someone else&#8217;s eyes?)</p>

<p>(I looked at Quora since I had some data from there already available, but I hope the lessons should be fairly applicable in general, to other social networks like Facebook, Twitter, and LinkedIn as well.)</p>

<p>To give an initial answer to some of these questions, I dug into one of my more popular posts, on <a href="http://www.quora.com/Random-Forests/How-do-random-forests-work-in-laymans-terms">a layman&#8217;s introduction to random forests</a>.</p>

<h1>Users, Topics</h1>

<p>Before looking deeper into the voting dynamics of the post, let&#8217;s first get some background on what kinds of users the answer reached.</p>

<p>Here&#8217;s a graph of the topics that question upvoters follow. (Each node is a topic, and every time upvoter X follows both topics A and B, I add an edge between A and B.)</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-unlabeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-unlabeled.png" alt="Upvoters' Topics - Unlabeled" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-labeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoter-topics-labeled.png" alt="Upvoters' Topics - Labeled" /></a></p>

<p>We can see from the graph that upvoters tend to be interested in three kinds of topics:</p>

<ul>
<li><strong>Machine learning and other technical matters</strong> (the green cluster): Classification, Data Mining, Big Data, Information Retrieval, Analytics, Probability, Support Vector Machines, R, Data Science, &#8230;</li>
<li><strong>Startups/Silicon Valley</strong> (the red cluster): Facebook, Lean Startups, Investing, Seed Funding, Angel Investing, Technology Trends, Product Managment, Silicon Valley Mergers and Acquisitions, Asana, Social Games, Quora, Mark Zuckerberg, User Experience, Founders and Entrepreneurs, &#8230;</li>
<li><strong>General Intellectual Topics</strong> (the purple cluster): TED, Science, Book Recommendations, Philosophy, Politics, Self-Improvement, Travel, Life Hacks, &#8230;</li>
</ul>


<p>Also, here&#8217;s the network of the upvoters themselves (there&#8217;s an edge between users A and B if A follows B):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-unlabeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-unlabeled.png" alt="Upvote Network - Unlabeled" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-labeled.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/rf-upvoters-labeled.png" alt="Upvote Network - Labeled" /></a></p>

<p>We can see three main clusters of users:</p>

<ul>
<li>A large group in <strong>green</strong> centered around a lot of power users and Quora employees.</li>
<li>A machine learning group of folks in <strong>orange</strong> centered around people like Oliver Grisel, Christian Langreiter, and Joseph Turian.</li>
<li>A group of people following me, in <strong>purple</strong>.</li>
<li>Plus some smaller clusters in blue and yellow. (There were also a bunch of isolated users, connected to no one, that I filtered out of the picture.)</li>
</ul>


<p>Digging into how these topic and user graphs are related:</p>

<ul>
<li>The orange cluster of users is more heavily into machine learning: 79% of users in that cluster follow more green topics (machine learning and technical topics) than red and purple topics (startups and general intellectual matters).</li>
<li>The green cluster of users is reversed: 77% of users follow more of the red and purple clusters of topics (on startups and general intellectual matters) than machine learning and technical topics.</li>
</ul>


<p>More interestingly, though, we can ask: how do the connections between upvoters relate to the way the post spread?</p>

<h1>Social Voting Dynamics</h1>

<p>So let&#8217;s take a look. Here&#8217;s a visualization I made of upvotes on my answer across time (click <a href="http://www.youtube.com/watch?v=cZ4Ntg4jQHw">here</a> for a larger view).</p>

<iframe width="640" height="510" src="http://www.youtube.com/embed/cZ4Ntg4jQHw " frameborder="0" allowfullscreen></iframe>


<p></p>

<p>To represent the social dynamics of these upvotes, I drew an edge from user A to user B if user A transmitted the post to user B through an upvote. (Specifically, I drew an edge from Alice to Bob if Bob follows Alice and Bob&#8217;s upvote appeared within five days of Alice&#8217;s upvote; this is meant to simulate the idea that Alice was the key intermediary between my post and Bob.)</p>

<p>Also,</p>

<ul>
<li>Green nodes are users with at least one upvote edge.</li>
<li>Blue nodes are users who follow at least one of the topics the post is categorized under (i.e., users who probably discovered the answer by themselves).</li>
<li>Red nodes are users with no connections and who do not follow any of the post&#8217;s topics (i.e, users whose path to the post remain mysterious).</li>
<li>Users increase in size when they produce more connections.</li>
</ul>


<p>Here&#8217;s a play-by-play of the video:</p>

<ul>
<li>On Feb 14 (the day I wrote the answer), there&#8217;s a flurry of activity.</li>
<li>A couple of days later, Tracy Chou gives an upvote, leading to another spike in activity.</li>
<li>Then all&#8217;s quiet until&#8230; bam! Alex Kamil leads to a surge of upvotes, and his upvote finds Ludi Rehak, who starts a small surge of her own. They&#8217;re quickly followed by Christian Langreiter, who starts a small revolution among a bunch of machine learning folks a couple days later.</li>
<li>Then all is pretty calm again, until a couple months later when&#8230; bam! Aditya Sengupta brings in a smashing of his own followers, and his upvote makes its way to Marc Bodnick, who sets off a veritable storm of activity.</li>
</ul>


<p>(Already we can see some relationships between the graph of user connections and the way the post propagated. Many of the users from the orange cluster, for example, come from Alex Kamil and Christian Langreiter&#8217;s upvotes, and many of the users from the green cluster come from Aditya Sengupta and Marc Bodnick&#8217;s upvotes. What&#8217;s interesting, though, is, why didn&#8217;t the cluster of green users appear all at once, like the orange cluster did? People like Kah Seng Tay, Tracy Chou, Venkatesh Rao, and Chad Little upvoted the answer pretty early on, but it wasn&#8217;t until Aditya Sengupta&#8217;s upvote a couple months later that people like Marc Bodnick, Edmond Lau, and many of the other green users (who do indeed follow that first set of folks) discovered the answer. Did the post simply get lost in users&#8217; feeds the first time around? Was the post perhaps ignored until it received enough upvotes to be considered worth reading? Are some users&#8217; upvotes just trusted more than others&#8217;?)</p>

<p>For another view of the upvote dynamics, here&#8217;s a static visualization, where we can again easily see the clusters of activity:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/social-network-transmission/upvote-clusters-labeled-v2.png"><img src="http://dl.dropbox.com/u/10506/blog/social-network-transmission/upvote-clusters-labeled-v2.png" alt="Upvote Temporal Clusters" /></a></p>

<h1>Fin</h1>

<p>There are still many questions it would be interesting to look at; for example,</p>

<ul>
<li>What differentiates users who sparked spikes of activity from users who didn&#8217;t? I don&#8217;t believe it&#8217;s simply number of followers, as many well-connected upvoters did <em>not</em> lead to cascades of shares. Does authority matter?</li>
<li>How far can a post reach? Clearly, the post reached people more than one degree of separation away from me (where one degree of separation is a follower); what does the distribution of degrees look like? Is there any relationship between degree of separation and time of upvote?</li>
<li>What can we say about the people who started following me after reading my answer? Are they fewer degrees of separation away? Are they more interested in machine learning? Have they upvoted any of my answers before? (Perhaps there&#8217;s a certain &#8220;threshold&#8221; of interestingness people need to overflow before they&#8217;re considered acceptable followees.)</li>
</ul>


<p>But to summarize a bit what we&#8217;ve seen so far, here are some statistics on the role the social graph played in spreading the post:</p>

<ul>
<li>There are 5 clusters of activity after the initial post, sparked both by power users and less-connected folks. In an interesting cascade of information, some of these sparks led to further spikes in activity as well (as when Aditya Sengupta&#8217;s upvote found its way to Marc Bodnick, who set off even more activity).</li>
<li>35% of users made their way to my answer because of someone else&#8217;s upvote.</li>
<li>Through these connections, the post reached a fair variety of users: 32% of upvoters don&#8217;t even follow any of the post&#8217;s topics.</li>
<li>77% of upvotes came from users over two weeks <em>after</em> my answer appeared.</li>
<li>If we look only at the upvoters who follow at least one of the post&#8217;s topics, 33% didn&#8217;t see my answer until someone else showed it to them. In other words, a full one-third of people who presumably would have been interested in my post anyways only found it because of their social network.</li>
</ul>


<p>So it looks like the social graph played quite a large part in the post&#8217;s propagation, and I&#8217;ll end with a big shoutout to Stormy Shippy, who provided an awesome set of scripts I used to collect a lot of this data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Latent Dirichlet Allocation]]></title>
    <link href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/"/>
    <updated>2011-08-22T10:50:49-07:00</updated>
    <id>http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>Suppose you have the following set of sentences:</p>

<ul>
<li>I like to eat broccoli and bananas.</li>
<li>I ate a banana and spinach smoothie for breakfast.</li>
<li>Chinchillas and kittens are cute.</li>
<li>My sister adopted a kitten yesterday.</li>
<li>Look at this cute hamster munching on a piece of broccoli.</li>
</ul>


<p>What is latent Dirichlet allocation? It&#8217;s a way of automatically discovering <strong>topics</strong> that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like</p>

<ul>
<li><strong>Sentences 1 and 2</strong>: 100% Topic A</li>
<li><strong>Sentences 3 and 4</strong>: 100% Topic B</li>
<li><strong>Sentence 5</strong>: 60% Topic A, 40% Topic B</li>
<li><strong>Topic A</strong>: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, &#8230; (at which point, you could interpret topic A to be about food)</li>
<li><strong>Topic B</strong>: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, &#8230; (at which point, you could interpret topic B to be about cute animals)</li>
</ul>


<p>The question, of course, is: how does LDA perform this discovery?</p>

<h1>LDA Model</h1>

<p>In more detail, LDA represents documents as <strong>mixtures of topics</strong> that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you</p>

<ul>
<li>Decide on the number of words N the document will have (say, according to a Poisson distribution).</li>
<li>Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals.</li>
<li>Generate each word w_i in the document by:

<ul>
<li>First picking a topic (according to the multinomial distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).</li>
<li>Using the topic to generate the word itself (according to the topic&#8217;s multinomial distribution). For example, if we selected the food topic, we might generate the word &#8220;broccoli&#8221; with 30% probability, &#8220;bananas&#8221; with 15% probability, and so on.</li>
</ul>
</li>
</ul>


<p>Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.</p>

<h2>Example</h2>

<p>Let&#8217;s make an example. According to the above process, when generating some particular document D, you might</p>

<ul>
<li>Pick 5 to be the number of words in D.</li>
<li>Decide that D will be 1/2 about food and 1/2 about cute animals.</li>
<li>Pick the first word to come from the food topic, which then gives you the word &#8220;broccoli&#8221;.</li>
<li>Pick the second word to come from the cute animals topic, which gives you &#8220;panda&#8221;.</li>
<li>Pick the third word to come from the cute animals topic, giving you &#8220;adorable&#8221;.</li>
<li>Pick the fourth word to come from the food topic, giving you &#8220;cherries&#8221;.</li>
<li>Pick the fifth word to come from the food topic, giving you &#8220;eating&#8221;.</li>
</ul>


<p>So the document generated under the LDA model will be &#8220;broccoli panda adorable cherries eating&#8221; (note that LDA is a bag-of-words model).</p>

<h1>Learning</h1>

<p>So now suppose you have a set of documents. You&#8217;ve chosen some fixed number of K topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this? One way (known as collapsed Gibbs sampling) is the following:</p>

<ul>
<li>Go through each document, and randomly assign each word in the document to one of the K topics.</li>
<li>Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).</li>
<li>So to improve on them, for each document d&#8230;

<ul>
<li>Go through each word w in d&#8230;

<ul>
<li>And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word&#8217;s topic with this probability). (Also, I&#8217;m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)</li>
<li>In other words, in this step, we&#8217;re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.</li>
</ul>
</li>
</ul>
</li>
<li>After repeating the previous step a large number of times, you&#8217;ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).</li>
</ul>


<h1>Layman&#8217;s Explanation</h1>

<p>In case the discussion above was a little eye-glazing, here&#8217;s another way to look at LDA in a different domain.</p>

<p>Suppose you&#8217;ve just moved to a new city. You&#8217;re a hipster and an anime fan, so you want to know where the other hipsters and anime geeks tend to hang out. Of course, as a hipster, you know you can&#8217;t just <em>ask</em>, so what do you do?</p>

<p>Here&#8217;s the scenario: you scope out a bunch of different establishments (<strong>documents</strong>) across town, making note of the people (<strong>words</strong>) hanging out in each of them (e.g., Alice hangs out at the mall and at the park, Bob hangs out at the movie theater and the park, and so on). Crucially, you don&#8217;t know the typical interest groups (<strong>topics</strong>) of each establishment, nor do you know the different interests of each person.</p>

<p>So you pick some number K of categories to learn (i.e., you want to learn the K most important kinds of categories people fall into), and start by making a guess as to why you see people where you do. For example, you initially guess that Alice is at the mall because people with interests in X like to hang out there; when you see her at the park, you guess it&#8217;s because her friends with interests in Y like to hang out there; when you see Bob at the movie theater, you randomly guess it&#8217;s because the Z people in this city really like to watch movies; and so on.</p>

<p>Of course, your random guesses are very likely to be incorrect (they&#8217;re random guesses, after all!), so you want to improve on them. One way of doing so is to:</p>

<ul>
<li>Pick a place and a person (e.g., Alice at the mall).</li>
<li>Why is Alice likely to be at the mall? Probably because other people at the mall with the same interests sent her a message telling her to come.</li>
<li>In other words, the more people with interests in X there are at the mall and the stronger Alice is associated with interest X (at all the other places she goes to), the more likely it is that Alice is at the mall because of interest X.</li>
<li>So make a new guess as to why Alice is at the mall, choosing an interest with some probability according to how likely you think it is.</li>
</ul>


<p>Go through each place and person over and over again. Your guesses keep getting better and better (after all, if you notice that lots of geeks hang out at the bookstore, and you suspect that Alice is pretty geeky herself, then it&#8217;s a good bet that Alice is at the bookstore because her geek friends told her to go there; and now that you have a better idea of why Alice is probably at the bookstore, you can use this knowledge in turn to improve your guesses as to why everyone else is where they are), and eventually you can stop updating. Then take a snapshot (or multiple snapshots) of your guesses, and use it to get all the information you want:</p>

<ul>
<li>For each category, you can count the people assigned to that category to figure out what people have this particular interest. By looking at the people themselves, you can interpret the category as well (e.g., if category X contains lots of tall people wearing jerseys and carrying around basketballs, you might interpret X as the &#8220;basketball players&#8221; group).</li>
<li>For each place P and interest category C, you can compute the proportions of people at P because of C (under the current set of assignments), and these give you a representation of P. For example, you might learn that the people who hang out at Barnes &amp; Noble consist of 10% hipsters, 50% anime fans, 10% jocks, and 30% college students.</li>
</ul>


<h1>Real-World Example</h1>

<p>Finally, I applied LDA to a set of Sarah Palin&#8217;s emails a little while ago (see <a href="http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails/">here</a> for the blog post, or <a href="http://sarah-palin.heroku.com/">here</a> for an app that allows you to browse through the emails by the LDA-learned categories), so let&#8217;s give a brief recap. Here are some of the topics that the algorithm learned:</p>

<ul>
<li><strong>Trig/Family/Inspiration</strong>: family, web, mail, god, son, from, congratulations, children, life, child, down, trig, baby, birth, love, you, syndrome, very, special, bless, old, husband, years, thank, best, &#8230;</li>
<li><strong>Wildlife/BP Corrosion</strong>: game, fish, moose, wildlife, hunting, bears, polar, bear, subsistence, management, area, board, hunt, wolves, control, department, year, use, wolf, habitat, hunters, caribou, program, denby, fishing, &#8230;</li>
<li><strong>Energy/Fuel/Oil/Mining:</strong> energy, fuel, costs, oil, alaskans, prices, cost, nome, now, high, being, home, public, power, mine, crisis, price, resource, need, community, fairbanks, rebate, use, mining, villages, &#8230;</li>
<li><strong>Gas</strong>: gas, oil, pipeline, agia, project, natural, north, producers, companies, tax, company, energy, development, slope, production, resources, line, gasline, transcanada, said, billion, plan, administration, million, industry, &#8230;</li>
<li><strong>Education/Waste</strong>: school, waste, education, students, schools, million, read, email, market, policy, student, year, high, news, states, program, first, report, business, management, bulletin, information, reports, 2008, quarter, &#8230;</li>
<li><strong>Presidential Campaign/Elections</strong>: mail, web, from, thank, you, box, mccain, sarah, very, good, great, john, hope, president, sincerely, wasilla, work, keep, make, add, family, republican, support, doing, p.o, &#8230;</li>
</ul>


<p>Here&#8217;s an example of an email which fell 99% into the Trig/Family/Inspiration category (particularly representative words are highlighted in blue):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-email.png"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-email.png" alt="Trig Email" /></a></p>

<p>And here&#8217;s an excerpt from an email which fell 10% into the Presidential Campaign/Election category (in red) and 90% into the Wildlife/BP Corrosion category (in green):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-presidency-email.png"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-presidency-email.png" alt="Wildlife-Presidency Email" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tweets vs. Likes: What gets shared on Twitter vs. Facebook?]]></title>
    <link href="http://blog.echen.me/2011/07/28/tweets-vs-likes-what-gets-shared-on-twitter-vs-facebook/"/>
    <updated>2011-07-28T07:55:29-07:00</updated>
    <id>http://blog.echen.me/2011/07/28/tweets-vs-likes-what-gets-shared-on-twitter-vs-facebook</id>
    <content type="html"><![CDATA[<p>It always strikes me as curious that some posts get a lot of love on Twitter, while others get many more shares on Facebook:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/twitter-beats-fb.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/twitter-beats-fb.png" alt="Twitter Beats FB" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/fb-beats-twitter.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/fb-beats-twitter.png" alt="FB Beats Twitter" /></a></p>

<p>What accounts for this difference? Some of it is surely site-dependent: maybe one blogger has a Facebook page but not a Twitter account, while another has these roles reversed. But even on sites maintained by a single author, tweet-to-likes ratios can vary widely from post to post.</p>

<p>So what kinds of articles tend to be more popular on Twitter, and which spread more easily on Facebook? To take a stab at an answer, I scraped data from a couple of websites over the weekend.</p>

<p><strong>tl;dr</strong> Twitter is still for the <em>techies</em>: articles where the number of tweets greatly outnumber FB likes tend to revolve around software companies and programming. Facebook, on the other hand, appeals to <em>everyone else</em>: yeah, to the masses, and to non-software technical folks in general as well.</p>

<h1>FlowingData</h1>

<p>The first site I looked at was Nathan Yau&#8217;s awesome <a href="http://www.flowingdata.com">FlowingData</a> website on data visualization. To see which articles are more popular on Facebook and which are more popular on Twitter, let&#8217;s sort all the FlowingData articles by their # tweets / # likes ratio.</p>

<p>Here are the 10 posts with the lowest tweets-to-likes ratio (i.e., the posts that were especially popular with Facebook users):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-facebook2.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-facebook2-small.png" alt="FlowingData Facebook" /></a></p>

<ul>
<li><a href="http://flowingdata.com/2011/01/30/what-your-state-is-the-worst-at-united-states-of-shame/">What your state is the worst at – United States of shame</a></li>
<li><a href="http://flowingdata.com/2011/05/13/plush-statistical-distribution-pillows/">Plush statistical distribution pillows</a></li>
<li><a href="http://flowingdata.com/2011/03/22/are-gas-prices-really-that-high/">Are gas prices really that high?</a></li>
<li><a href="http://flowingdata.com/2011/01/21/hey-jude-flowchart/">Hey Jude flowchart</a></li>
<li><a href="http://flowingdata.com/2011/04/28/womens-dress-sizes-demystified/">Women’s dress sizes demystified</a></li>
<li><a href="http://flowingdata.com/2011/02/22/america-is-not-the-best-at-everything/">America is not the best at everything</a></li>
<li><a href="http://flowingdata.com/2011/06/10/what-you-need-to-get-together/">What you need to get together</a></li>
<li><a href="http://flowingdata.com/2011/01/27/dexters-victims-through-season-five/">Dexter’s victims through season five</a></li>
<li><a href="http://flowingdata.com/2011/05/06/correlating-dog/">Correlating dog</a></li>
<li><a href="http://flowingdata.com/2011/02/14/valentines-day-importance/">Valentine’s Day importance</a></li>
</ul>


<p>And here are the 10 posts with the highest tweets-to-like ratio (i.e., the posts especially popular with Twitter users):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-twitter.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-twitter-small.png" alt="FlowingData Twitter" /></a></p>

<ul>
<li><a href="http://flowingdata.com/2011/01/04/delicious-mass-exodus/">Delicious mass exodus</a></li>
<li><a href="http://flowingdata.com/2011/05/25/pew-research-raw-survey-data-now-available/">Pew Research raw survey data now available</a></li>
<li><a href="http://flowingdata.com/2011/02/03/stock-market-predictions-with-twitter/">Stock market predictions with Twitter</a></li>
<li><a href="http://flowingdata.com/2011/01/25/growth-and-usage-of-foursquare-in-2010/">Growth and usage of foursquare in 2010</a></li>
<li><a href="http://flowingdata.com/2011/02/17/sunlight-labs-opens-up-real-time-congress-api/">Sunlight Labs opens up Real Time Congress API</a></li>
<li><a href="http://flowingdata.com/2011/03/25/open-source-data-science-toolkit/">Open-source Data Science Toolkit</a></li>
<li><a href="http://flowingdata.com/2011/01/24/explore-your-linkedin-network-visually-with-inmaps/">Explore your LinkedIn network visually with InMaps</a></li>
<li><a href="http://flowingdata.com/2011/05/03/perceived-vs-actual-country-rankings/">Perceived vs. actual country rankings</a></li>
<li><a href="http://flowingdata.com/2011/04/20/see-what-you-and-others-tweet-about-with-the-topic-explorer/">See what you and others tweet about with the Topic Explorer</a></li>
<li><a href="http://flowingdata.com/2011/04/24/history-of-detainees-at-guantnamo/">History of detainees at Guantánamo</a></li>
</ul>


<p>Notice any differences between the two?</p>

<ul>
<li>Instant gratification infographics, cuteness, comics, and pop culture get liked on Facebook.</li>
<li>APIs, datasets, visualizations related to techie sites (Delicious, foursquare, Twitter, LinkedIn), and picture-less articles get tweeted instead.</li>
</ul>


<p>Interestingly, it also looks like the colors in the top 10 Facebook articles tend to the red end of the spectrum, while the colors in the top 10 Twitter articles tend to the blue end of the spectrum. Does this pattern hold if we look at more data? Here&#8217;s a meta-visualization of the FlowingData articles, sorted by articles popular on Facebook in the top left to articles popular on Twitter in the bottom right (see <a href="http://flowingdata-melted.heroku.com/">here</a> for some interactivity and more details):</p>

<p><a href="http://flowingdata-melted.heroku.com/"><img src="http://dl.dropbox.com/u/10506/blog/flowingdata-metaviz/flowingdata-metaviz.png" alt="FlowingData MetaViz" /></a></p>

<p>It does indeed look like the images at the top (the articles popular on Facebook) are more pink, while the images at the bottom (the articles popular on Twitter) are more blue (though it would be nice to quantify this in some way)!</p>

<p>Furthermore, we can easily see from the grid that articles with no visualizations (represented by lorem ipsum text in the grid) cluster at the bottom. Grabbing some actual numbers, we find that 32% of articles with at least one picture have more shares on Facebook than on Twitter, compared to only 4% of articles with no picture at all.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-viz-effect.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-viz-effect.png" alt="Effect of a visualization" /></a></p>

<p>Finally, let&#8217;s break down the percentage of articles with more Facebook shares by category.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-categories.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-categories.png" alt="FlowingData Categories" /></a></p>

<p>(I filtered the categories so that each category in the plot above contains at least 5 articles.)</p>

<p>What do we find?</p>

<ul>
<li>Articles in the Software, Online Applications, News, and Data sources categories (yawn) get 100% of their shares from Twitter.</li>
<li>Articles tagged with <a href="http://flowingdata.com/category/projects/data-underload/">Data Underload</a> (which seems to contain short and sweet visualizations of everyday things), <a href="http://flowingdata.com/category/miscellaneous-data/">Miscellaneous</a> (which contains lots of comics or comic-like visualizations), and <a href="http://flowingdata.com/category/visualization/infographics/">Infographics</a> get the most shares on Facebook.</li>
<li>This category breakdown matches precisely what we saw in the top 10 examples above.</li>
</ul>


<h1>New Scientist</h1>

<p>When looking at FlowingData, we saw that Twitter users are much bigger on sharing technical articles. But is this true for technical articles in general, or only for programming-related posts? (In my experience with Twitter, I haven&#8217;t seen many people from math and the non-computer sciences.)</p>

<p>To answer, I took articles from the <a href="http://www.newscientist.com/search?rbsection1=Physics+%26+Math&amp;sortby=rbpubdate">Physics &amp; Math</a> and <a href="http://www.newscientist.com/search?rbsection1=tech&amp;sortby=rbpubdate">Technology</a> sections of <a href="http://www.newscientist.com">New Scientist</a>, and</p>

<ul>
<li>Calculated the percentage of shares each article received on Twitter (i.e., # tweets / (# tweets + # likes)).</li>
<li>Grouped articles by their number of tweets rounded to the nearest multiple of 25 (bin #1 contains articles close to 25 tweets, bin #2 contains articles close to 50 tweets, etc.).</li>
<li>Calculated the median percentage of shares on Twitter for each bin.</li>
</ul>


<p>Here&#8217;s a graph of the result:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/tech_vs_physicsmath.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/tech_vs_physicsmath.png" alt="Technology vs. Physics &amp; Math" /></a></p>

<p>Notice that:</p>

<ul>
<li>The technology articles get consistently more shares from Twitter than the physics and math articles do.</li>
<li>Twitter accounts for the majority of the technology shares.</li>
<li>Facebook accounts for the majority of the physics and math shares.</li>
</ul>


<p>So this suggests that Twitter really is for computer technology in particular, not technical matters in general (though it would be nice to look at areas other than physics and math as well).</p>

<h1>Quora</h1>

<p>To get some additional evidence on the computer science vs. math/physics divide, I</p>

<ul>
<li>Scraped about 350 profiles of followers from each of the Computer Science, Software Engineering, Mathematics, and Physics categories on Quora;</li>
<li>Checked each user to see whether they link to their Facebook and Twitter accounts on their profile.</li>
</ul>


<p>Here&#8217;s the ratio of the number of people linking to their Facebook account to the number of people linking to their Twitter account, sliced by topic:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software.png" alt="Math/Physics vs. CS/Software" /></a></p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software-collapsed.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/math-physics-vs-cs-software-collapsed.png" alt="Math/Physics vs. CS/Software, Collapsed" /></a></p>

<p>We find exactly what we expect from the New Scientist data: people following the math and physics categories have noticeably smaller Twitter / Facebook ratios compared to people following the computer science and software engineering categories (i.e., compared to computer scientists and software engineers, mathematicians and physicists are more likely to be on Facebook than on Twitter). What&#8217;s more, this difference is in fact significant: the graphs display individual 90% confidence intervals (which overlap not at all or only slightly), and we do indeed get significance at the 95% level if we look at the differences between categories.</p>

<p>This corroborates the New Scientist evidence that Twitter gets the computer technology shares, while Facebook gets the math and physics shares.</p>

<h1>XKCD</h1>

<p>Finally, let&#8217;s take a look at which XKCD comics are especially popular on Facebook vs. Twitter.</p>

<p>Here are the 10 comics with the highest likes-to-tweets ratio (i.e., the comics especially popular on Facebook):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-facebook.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-facebook-small.png" alt="XKCD Facebook" /></a></p>

<ul>
<li><a href="http://xkcd.com/846/">Dental Nerve</a></li>
<li><a href="http://xkcd.com/861/">Wisdom Teeth</a></li>
<li><a href="http://xkcd.com/876/">Trapped</a></li>
<li><a href="http://xkcd.com/849/">Complex Conjugate</a></li>
<li><a href="http://xkcd.com/854/">Learning to Cook</a></li>
<li><a href="http://xkcd.com/840/">Serious</a></li>
<li><a href="http://xkcd.com/839/">Explorers</a></li>
<li><a href="http://xkcd.com/815/">Mu</a></li>
<li><a href="http://xkcd.com/809/">Los Alamos</a></li>
<li><a href="http://xkcd.com/911/">Magic School Bus</a></li>
</ul>


<p>Here are the 10 comics with the highest tweets-to-likes ratio (i.e., the comics especially popular on Twitter):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-twitter.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/xkcd-twitter-small.png" alt="XKCD Twitter" /></a></p>

<ul>
<li><a href="http://xkcd.com/869/">Server Attention Span</a></li>
<li><a href="http://xkcd.com/818/">Illness</a></li>
<li><a href="http://xkcd.com/865/">Nanobots</a></li>
<li><a href="http://xkcd.com/912/">Manual Override</a></li>
<li><a href="http://xkcd.com/908/">The Cloud</a></li>
<li><a href="http://xkcd.com/810/">Constructive</a></li>
<li><a href="http://xkcd.com/887/">Future Timeline</a></li>
<li><a href="http://xkcd.com/844/">Good Code</a></li>
<li><a href="http://xkcd.com/801/">Golden Hammer</a></li>
<li><a href="http://xkcd.com/906/">Advertising Discovery</a></li>
<li><a href="http://xkcd.com/802/">Online Communities 2</a></li>
</ul>


<p>Note that the XKCD comics popular on Facebook have more of a layman flavor, while the XKCD comics popular on Twitter are much more programming-related:</p>

<ul>
<li>Of the XKCD comics popular on Twitter, one&#8217;s about server attention spans, another&#8217;s about IPv6 addresses, a third is about GNU info pages, another deals with cloud computing, a fifth talks about Java, and the last is about a bunch of techie sites. (This is just like what we saw with the FlowingData visualizations.)</li>
<li>Facebook, on the other hand, gets Ke$ha and Magic School Bus.</li>
<li>And while both top 10&#8217;s contain a flowchart, the one popular on FB is about <em>cooking</em>, while the one popular on Twitter is about <em>code</em>!</li>
<li>What&#8217;s more, if we look at the few technical-ish comics that are more popular on Facebook (the complex conjugate, mu, and Los Alamos comics), we see that they&#8217;re about physics and math, not programming (which matches our findings from the New Scientist articles).</li>
</ul>


<h1>Lesson</h1>

<p>So why should you care? Here&#8217;s one takeaway:</p>

<ul>
<li>If you&#8217;re blogging about technology, programming, and computer science, Twitter is your friend.</li>
<li>But if you&#8217;re blogging about anything else, be it math/physics or pop culture, don&#8217;t rely on a Twitter account alone; your shares are more likely to propagate on Facebook, so make sure to have a Facebook page as well.</li>
</ul>


<h1>What&#8217;s Next?</h1>

<p>The three websites I looked at are all fairly tech-oriented, so it would be nice to gather data from other kinds of websites as well.</p>

<p>And now that we have an idea how Twitter and Facebook compare, the next burning question is surely: <a href="http://finalbossform.com/post/7214184180/google-is-fast-becoming-the-leading-social">what do people share on Google+?!</a></p>

<h1>Addendum</h1>

<p>Let&#8217;s consider the following thought experiment. Suppose you come across the most unpopular article ever written. What will its FB vs. Twitter shares look like? Although no <em>real</em> person will ever share this article, I think Twitter has many more spambots (who tweet out any and every link) than FB does, so maybe unpopular articles will have more tweets than likes by default. Conversely, suppose you come across the most popular article ever written, which everybody wants to share. Then since FB has many more users than Twitter does, maybe popular articles will tend to have more likes than tweets anyways.</p>

<p>Thus, in order to find out which types of articles are <em>especially</em> popular on FB vs. Twitter, instead of looking at tweets-to-likes ratios directly, we could try to remove this baseline popularity effect. (Taking ratios instead of raw number of tweets or raw number of likes is one kind of normalization; this is another.)</p>

<p>So does this scenario (or something similar to it) actually play out in practice?</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-overall-popularity-vs-fb.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-overall-popularity-vs-fb.png" alt="Overall Popularity vs. Facebook" /></a></p>

<p>Here I&#8217;ve plotted the overall popularity of a post (the total number of shares it received on either Twitter or FB) against the percentage of shares on Facebook alone, and we can see that as a post&#8217;s popularity grows, more and more shares do indeed tend to come from Facebook rather than Twitter.</p>

<p>Also, see the posts at the lower end of the popularity scale that are only getting shares on Twitter? Let&#8217;s take a look at the five most unpopular of these:</p>

<ul>
<li><a href="http://flowingdata.com/2011/03/31/flowingdata-is-brought-to-you-by-8/">Flowing Data is brought to you by&#8230; (March 2011 edition)</a> (11 tweets, 0 likes)</li>
<li><a href="http://flowingdata.com/2011/07/05/flowingdata-is-brought-to-you-by-11/">Flowing Data is brought to you by&#8230; (July 2011 edition)</a> (14 tweets, 0 likes)</li>
<li><a href="http://flowingdata.com/2011/06/06/flowingdata-is-brought-to-you-by-10/">Flowing Data is brought to you by&#8230; (June 2011 edition)</a> (17 tweets, 0 likes)</li>
<li><a href="http://flowingdata.com/2011/05/09/flowingdata-is-brought-to-you-by-9/">Flowing Data is brought to you by&#8230; (May 2011 edition)</a> (18 tweets, 0 likes)</li>
<li><a href="http://flowingdata.com/2011/02/28/flowingdata-is-brought-to-you-by-7/">Flowing Data is brought to you by&#8230; (May 2011 edition)</a> (12 tweets, 1 like)</li>
</ul>


<p>Notice that they&#8217;re all shoutouts to FlowingData&#8217;s sponsors! There&#8217;s pretty much no reason any <em>real</em> person would share these on Twitter or Facebook, and indeed, checking Twitter to see who actually tweeted out these links, we see that the tweeters are bots:</p>

<ul>
<li><a href="https://twitter.com/#!/myVisualization/status/77685824224894976">https://twitter.com/#!/myVisualization/status/77685824224894976</a></li>
<li><a href="https://twitter.com/#!/InfographicTwts/status/67668615142457344">https://twitter.com/#!/InfographicTwts/status/6766861514245734</a></li>
<li><a href="https://twitter.com/#!/guysgoogle/status/77644902510493696">https://twitter.com/#!/guysgoogle/status/77644902510493696</a></li>
<li><a href="https://twitter.com/#!/WhereIsYourData/status/77631743292735488">https://twitter.com/#!/WhereIsYourData/status/77631743292735488</a></li>
</ul>


<p>Now let&#8217;s switch to a slightly different view of the above scenario, where I plot number of tweets against number of likes:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-tweets-vs-likes.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/flowingdata-tweets-vs-likes.png" alt="FlowingData Tweets vs. Likes" /></a></p>

<p>We see that as popularity on Twitter increases, so too does popularity on Facebook &#8211; but at a slightly faster rate. (The form of the blue line plotted is roughly $\log(likes) = -3.87 + 1.70 \log(tweets)$.)</p>

<p>So instead of looking at the ratios above, to figure out which articles are popular on FB vs. Twitter, we could look at the residuals of the above plot. Posts with large positive residuals would be posts that are especially popular on FB, and posts with negative residuals would be posts that are especially popular on Twitter.</p>

<p>In practice, however, there wasn&#8217;t much difference between looking at residuals vs. ratios directly when using the datasets I had, so to keep things simple in the main discussion above, I stuck to ratios alone. Still, it&#8217;s another option which might be useful when looking at different questions or different sources of data, so just for completeness, here&#8217;s what the FlowingData results look like if we use residuals instead.</p>

<p>The 10 articles with the highest residuals (i.e., the articles most popular on Facebook):</p>

<ul>
<li><a href="http://flowingdata.com/2011/06/10/what-you-need-to-get-together/">What you need to get together</a></li>
<li><a href="http://flowingdata.com/2011/02/14/valentines-day-importance/">Valentine’s Day importance</a></li>
<li><a href="http://flowingdata.com/2011/01/30/what-your-state-is-the-worst-at-united-states-of-shame/">What your state is the worst at – United States of shame</a></li>
<li><a href="http://flowingdata.com/2011/05/13/plush-statistical-distribution-pillows/">Plush statistical distribution pillows</a></li>
<li><a href="http://flowingdata.com/2011/07/01/hitler-learns-topology/">Hitler learns topology</a></li>
<li><a href="http://flowingdata.com/2011/01/27/dexters-victims-through-season-five/">Dexter’s victims through season five</a></li>
<li><a href="http://flowingdata.com/2011/07/06/access-to-education-where-you-live/">Access to education where you live</a></li>
<li><a href="http://flowingdata.com/2011/03/09/watching-costco-warehouses-open-nationwide/">Watching the growth of Costco warehouses</a></li>
<li><a href="http://flowingdata.com/2011/03/22/are-gas-prices-really-that-high/">Are gas prices really that high?</a></li>
<li><a href="http://flowingdata.com/2011/01/21/flight-safety-esque-beer-pong-guide/">Flight safety-esque beer pong guide</a></li>
</ul>


<p>The 10 articles with the lowest residuals (i.e., the articles most popular on Twitter):</p>

<ul>
<li><a href="http://flowingdata.com/2011/05/25/pew-research-raw-survey-data-now-available/">Pew Research raw survey data now available</a></li>
<li><a href="http://flowingdata.com/2011/01/24/explore-your-linkedin-network-visually-with-inmaps/">Explore your LinkedIn network visually with InMaps</a></li>
<li><a href="http://flowingdata.com/2011/02/03/stock-market-predictions-with-twitter/">Stock market predictions with Twitter</a></li>
<li><a href="http://flowingdata.com/2011/01/04/delicious-mass-exodus/">Delicious mass exodus</a></li>
<li><a href="http://flowingdata.com/2011/03/25/open-source-data-science-toolkit/">Open-source Data Science Toolkit</a></li>
<li><a href="http://flowingdata.com/2011/04/17/business-intelligence-vs-infotainment/">Business intelligence vs. infotainment</a></li>
<li><a href="http://flowingdata.com/2011/04/20/see-what-you-and-others-tweet-about-with-the-topic-explorer/">See what you and others tweet about with the Topic Explorer</a></li>
<li><a href="http://flowingdata.com/2011/01/25/growth-and-usage-of-foursquare-in-2010/">Growth and usage of foursquare in 2010</a></li>
<li><a href="http://flowingdata.com/2011/05/10/flash-vs-html5/">Flash vs. HTML5</a></li>
<li><a href="http://flowingdata.com/2011/06/09/gender-and-time-comparisons-on-twitter/">Gender and time comparisons on Twitter</a></li>
</ul>


<p>Here&#8217;s a density plot of article residuals, split by whether the article has a visualization or not (residuals of picture-free articles are clearly shifted towards the negative end):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/has-viz-residuals.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/has-viz-residuals.png" alt="Residuals" /></a></p>

<p>Here are the mean residuals per category (again, we see that the miscellaneous, data underload, data art, and infographics categories tend to be more popular on Facebook, while the data sources, software, online applications, and news categories tend to be more popular on Twitter):</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/category-residuals.png"><img src="http://dl.dropbox.com/u/10506/blog/likes-vs-tweets/category-residuals.png" alt="Category Residuals" /></a></p>

<p>And that&#8217;s it! In the spirit of these findings, I hope this article gets <a href="http://blog.echen.me/2011/07/28/tweets-vs-likes-what-gets-shared-on-twitter-vs-facebook/?share=facebook&amp;nb=1">liked</a> a little and <a href="https://twitter.com/share?original_referer=http%3A%2F%2Fblog.echen.me%2F2011%2F07%2F28%2Ftweets-vs-likes-what-gets-shared-on-twitter-vs-facebook%2F&amp;source=tweetbutton&amp;text=Tweets%20vs.%20Likes%3A%20What%20gets%20shared%20on%20Twitter%20vs.%20Facebook%3F%3A&amp;url=http%3A%2F%2Fwp.me%2Fpy9AS-6P">tweeted</a> lots and lots.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to Restricted Boltzmann Machines]]></title>
    <link href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/"/>
    <updated>2011-07-18T09:32:52-07:00</updated>
    <id>http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>Suppose you ask a bunch of users to rate a set of movies on a 0-100 scale. In classical <a href="http://en.wikipedia.org/wiki/Factor_analysis">factor analysis</a>, you could then try to explain each movie and user in terms of a set of latent <em>factors</em>. For example, movies like Star Wars and Lord of the Rings might have strong associations with a latent science fiction and fantasy factor, and users who like Wall-E and Toy Story might have strong associations with a latent Pixar factor.</p>

<p>Restricted Boltzmann Machines essentially perform a <em>binary</em> version of factor analysis. (This is one way of thinking about RBMs; there are, of course, others, and lots of different ways to use RBMs, but I&#8217;ll adopt this approach for this post.) Instead of users rating a set of movies on a continuous scale, they simply tell you whether they like a movie or not, and the RBM will try to discover latent factors that can explain the activation of these movie choices.</p>

<p>More technically, a Restricted Boltzmann Machine is a <strong>stochastic neural network</strong> (<em>neural network</em> meaning we have neuron-like units whose binary activations depend on the neighbors they&#8217;re connected to; <em>stochastic</em> meaning these activations have a probabilistic element) consisting of:</p>

<ul>
<li>One layer of <strong>visible units</strong> (users&#8217; movie preferences whose states we know and set);</li>
<li>One layer of <strong>hidden units</strong> (the latent factors we try to learn); and</li>
<li>A bias unit (whose state is always on, and is a way of adjusting for the different inherent popularities of each movie).</li>
</ul>


<p>Furthermore, each visible unit is connected to all the hidden units (this connection is undirected, so each hidden unit is also connected to all the visible units), and the bias unit is connected to all the visible units and all the hidden units. To make learning easier, we restrict the network so that no visible unit is connected to any other visible unit and no hidden unit is connected to any other hidden unit.</p>

<p>For example, suppose we have a set of six movies (Harry Potter, Avatar, LOTR 3, Gladiator, Titanic, and Glitter) and we ask users to tell us which ones they want to watch. If we want to learn two latent units underlying movie preferences &#8211; for example, two natural groups in our set of six movies appear to be SF/fantasy (containing Harry Potter, Avatar, and LOTR 3) and Oscar winners (containing LOTR 3, Gladiator, and Titanic), so we might hope that our latent units will correspond to these categories &#8211; then our RBM would look like the following:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png"><img src="http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png" alt="RBM Example" /></a></p>

<p>(Note the resemblance to a factor analysis graphical model.)</p>

<h1>State Activation</h1>

<p>Restricted Boltzmann Machines, and neural networks in general, work by updating the states of some neurons given the states of others, so let&#8217;s talk about how the states of individual units change. Assuming we know the connection weights in our RBM (we&#8217;ll explain how to learn these below), to update the state of unit $i$:</p>

<ul>
<li>Compute the <strong>activation energy</strong> $a_i = \sum_j w_{ij} x_j$ of unit $i$, where the sum runs over all units $j$ that unit $i$ is connected to, $w_{ij}$ is the weight of the connection between $i$ and $j$, and $x_j$ is the 0 or 1 state of unit $j$. In other words, all of unit $i$&#8217;s neighbors send it a message, and we compute the sum of all these messages.</li>
<li>Let $p_i = \sigma(a_i)$, where $\sigma(x) = 1/(1 + exp(-x))$ is the logistic function. Note that $p_i$ is close to 1 for large positive activation energies, and $p_i$ is close to 0 for negative activation energies.</li>
<li>We then turn unit $i$ on with probability $p_i$, and turn it off with probability $1 - p_i$.</li>
<li>(In layman&#8217;s terms, units that are positively connected to each other try to get each other to share the same state (i.e., be both on or off), while units that are negatively connected to each other are enemies that prefer to be in different states.)</li>
</ul>


<p>For example, let&#8217;s suppose our two hidden units really do correspond to SF/fantasy and Oscar winners.</p>

<ul>
<li>If Alice has told us her six binary preferences on our set of movies, we could then ask our RBM which of the hidden units her preferences activate (i.e., ask the RBM to explain her preferences in terms of latent factors). So the six movies send messages to the hidden units, telling them to update themselves. (Note that even if Alice has declared she wants to watch Harry Potter, Avatar, and LOTR 3, this doesn&#8217;t guarantee that the SF/fantasy hidden unit will turn on, but only that it will turn on with high <em>probability</em>. This makes a bit of sense: in the real world, Alice wanting to watch all three of those movies makes us highly suspect she likes SF/fantasy in general, but there&#8217;s a small chance she wants to watch them for other reasons. Thus, the RBM allows us to <em>generate</em> models of people in the messy, real world.)</li>
<li>Conversely, if we know that one person likes SF/fantasy (so that the SF/fantasy unit is on), we can then ask the RBM which of the movie units that hidden unit turns on (i.e., ask the RBM to generate a set of movie recommendations). So the hidden units send messages to the movie units, telling them to update their states. (Again, note that the SF/fantasy unit being on doesn&#8217;t guarantee that we&#8217;ll always recommend all three of Harry Potter, Avatar, and LOTR 3 because, hey, not everyone who likes science fiction liked Avatar.)</li>
</ul>


<h1>Learning Weights</h1>

<p>So how do we learn the connection weights in our network? Suppose we have a bunch of training examples, where each training example is a binary vector with six elements corresponding to a user&#8217;s movie preferences. Then for each epoch, do the following:</p>

<ul>
<li>Take a training example (a set of six movie preferences). Set the states of the visible units to these preferences.</li>
<li>Next, update the states of the hidden units using the logistic activation rule described above: for the $j$th hidden unit, compute its activation energy $a_j = \sum_i w_{ij} x_i$, and set $x_j$ to 1 with probability $\sigma(a_j)$ and to 0 with probability $1 - \sigma(a_j)$. Then for each edge $e_{ij}$, compute $Positive(e_{ij}) = x_i * x_j$ (i.e., for each pair of units, measure whether they&#8217;re both on).</li>
<li>Now <strong>reconstruct</strong> the visible units in a similar manner: for each visible unit, compute its activation energy $a_i$, and update its state. (Note that this <em>reconstruction</em> may not match the original preferences.) Then update the hidden units again, and compute $Negative(e_{ij}) = x_i * x_j$ for each edge.</li>
<li>Update the weight of each edge $e_{ij}$ by setting $w_{ij} = w_{ij} + L * (Positive(e_{ij}) - Negative(e_{ij}))$, where $L$ is a learning rate.</li>
<li>Repeat over all training examples.</li>
</ul>


<p>Continue until the network converges (i.e., the error between the training examples and their reconstructions falls below some threshold) or we reach some maximum number of epochs.</p>

<p>Why does this update rule make sense? Note that</p>

<ul>
<li>In the first phase, $Positive(e_{ij})$ measures the association between the $i$th and $j$th unit that we <em>want</em> the network to learn from our training examples;</li>
<li>In the &#8220;reconstruction&#8221; phase, where the RBM generates the states of visible units based on its hypotheses about the hidden units alone, $Negative(e_{ij})$ measures the association that the network <em>itself</em> generates (or &#8220;daydreams&#8221; about) when no units are fixed to training data.</li>
</ul>


<p>So by adding $Positive(e_{ij}) - Negative(e_{ij})$ to each edge weight, we&#8217;re helping the network&#8217;s daydreams better match the reality of our training examples.</p>

<p>(You may hear this update rule called <strong>contrastive divergence</strong>, which is basically a funky term for &#8220;approximate gradient descent&#8221;.)</p>

<h1>Examples</h1>

<p>I wrote <a href="https://github.com/echen/restricted-boltzmann-machines">a simple RBM implementation</a> in Python (the code is heavily commented, so take a look if you&#8217;re still a little fuzzy on how everything works), so let&#8217;s use it to walk through some examples.</p>

<p>First, I trained the RBM using some fake data.</p>

<ul>
<li>Alice: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.</li>
<li>Bob: (Harry Potter = 1, Avatar = 0, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). SF/fantasy fan, but doesn&#8217;t like Avatar.</li>
<li>Carol: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.</li>
<li>David: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.</li>
<li>Eric:  (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Oscar winners fan, except for Titanic.</li>
<li>Fred: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.</li>
</ul>


<p>The network learned the following weights:</p>

<pre><code>                 Bias Unit       Hidden 1        Hidden 2
Bias Unit       -0.08257658     -0.19041546      1.57007782 
Harry Potter    -0.82602559     -7.08986885      4.96606654 
Avatar          -1.84023877     -5.18354129      2.27197472 
LOTR 3           3.92321075      2.51720193      4.11061383 
Gladiator        0.10316995      6.74833901     -4.00505343 
Titanic         -0.97646029      3.25474524     -5.59606865 
Glitter         -4.44685751     -2.81563804     -2.91540988
</code></pre>

<p>Note that the first hidden unit seems to correspond to the Oscar winners, and the second hidden unit seems to correspond to the SF/fantasy movies, just as we were hoping.</p>

<p>What happens if we give the RBM a new user, George, who has (Harry Potter = 0, Avatar = 0, LOTR 3 = 0, Gladiator = 1, Titanic = 1, Glitter = 0) as his preferences? It turns the Oscar winners unit on (but not the SF/fantasy unit), correctly guessing that George probably likes movies that are Oscar winners.</p>

<p>What happens if we activate only the SF/fantasy unit, and run the RBM a bunch of different times? In my trials, it turned on Harry Potter, Avatar, and LOTR 3 three times; it turned on Avatar and LOTR 3, but not Harry Potter, once; and it turned on Harry Potter and LOTR 3, but not Avatar, twice. Note that, based on our training examples, these generated preferences do indeed match what we might expect real SF/fantasy fans want to watch.</p>

<h1>Modifications</h1>

<p>I tried to keep the connection-learning algorithm I described above pretty simple, so here are some modifications that often appear in practice:</p>

<ul>
<li>Above, $Negative(e_{ij})$ was determined by taking the product of the $i$th and $j$th units after reconstructing the visible units <em>once</em> and then updating the hidden units again. We could also take the product after some larger number of reconstructions (i.e., repeat updating the visible units, then the hidden units, then the visible units again, and so on); this is slower, but describes the network&#8217;s daydreams more accurately.</li>
<li>Instead of using $Positive(e_{ij})=x_i * x_j$, where $x_i$ and $x_j$ are binary 0 or 1 <em>states</em>, we could also let $x_i$ and/or $x_j$ be activation <em>probabilities</em>. Similarly for $Negative(e_{ij})$.</li>
<li>We could penalize larger edge weights, in order to get a sparser or more regularized model.</li>
<li>When updating edge weights, we could use a momentum factor: we would add to each edge a weighted sum of the current step as described above (i.e., $L * (Positive(e_{ij}) - Negative(e_{ij})$) and the step previously taken.</li>
<li>Instead of using only one training example in each epoch, we could use <em>batches</em> of examples in each epoch, and only update the network&#8217;s weights after passing through all the examples in the batch. This can speed up the learning by taking advantage of fast matrix-multiplication algorithms.</li>
</ul>


<h1>Further</h1>

<p>If you&#8217;re interested in learning more about Restricted Boltzmann Machines, here are some good links.</p>

<ul>
<li><a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">A Practical guide to training restricted Boltzmann machines</a>, by Geoffrey Hinton.</li>
<li>A talk by Andrew Ng on <a href="http://www.youtube.com/watch?v=ZmNOAtZIgIk">Unsupervised Feature Learning and Deep Learning</a>.</li>
<li><a href="http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf">Restricted Boltzmann Machines for Collaborative Filtering</a>. I found this paper hard to read, but it&#8217;s an interesting application to the Netflix Prize.</li>
<li><a href="http://arxiv.org/abs/0908.4425">Geometry of the Restricted Boltzmann Machine</a>. A very readable introduction to RBMs, &#8220;starting with the observation that its Zariski closure is a Hadamard power of the first secant variety of the Segre variety of projective lines&#8221;. (I kid, I kid.)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing Miss USA 2011: Should Evolution be Taught in Schools?]]></title>
    <link href="http://blog.echen.me/2011/07/09/visualizing-miss-usa-2011-should-evolution-be-taught-in-schools/"/>
    <updated>2011-07-09T13:02:05-07:00</updated>
    <id>http://blog.echen.me/2011/07/09/visualizing-miss-usa-2011-should-evolution-be-taught-in-schools</id>
    <content type="html"><![CDATA[<p><strong>tl;dr</strong> Should evolution be taught in schools? Go <a href="http://miss-usa-evolution.heroku.com">here</a> to visualize what the Miss USA contestants think.</p>

<h1>The Visualization</h1>

<p>Education is an area sorely in need of data crunching, so when I ran across <a href="http://www.youtube.com/watch?v=9QBv2CFTSWU&amp;feature=player_embedded#at=56">an amusing video</a> satirizing the Miss USA contestants&#8217; thoughts on teaching evolution, I thought it would be fun to transcribe <a href="youtube=http://www.youtube.com/watch?v=UkBmhM0R2A0&amp;feature=youtu.be#at=41">the original video</a> and visualize the data.</p>

<p>So after some tedious transcription, I built <a href="http://miss-usa-evolution.heroku.com">this app</a> to categorize and visualize the responses in various ways. (Mouseover states to see transcriptions, and click to jump to each contestant&#8217;s response in the video.)</p>

<h1>Example Maps</h1>

<p>Here are some examples of the generated maps.</p>

<p>First, we can look at which contestants believe in evolution (Miss California &#8211; the winner! &#8211; is the only contestant who explicitly states she believes in evolution; the red states explicitly state they do not believe in evolution; the grey states don&#8217;t make clear what they believe):</p>

<p><a href="http://miss-usa-evolution.heroku.com/?column=believes"><img src="http://dl.dropbox.com/u/10506/blog/miss-usa-evolution/believe-evolution.png" alt="Believe Evolution" /></a></p>

<p>We can also see which contestants believe evolution should be taught in schools (green states say yes, red states say no, grey states say maybe or that it should be a choice):</p>

<p><a href="http://miss-usa-evolution.heroku.com/?column=should_teach"><img src="http://dl.dropbox.com/u/10506/blog/miss-usa-evolution/teach-evolution.png" alt="Teach Evolution" /></a></p>

<p>Here is a rough Darwin-friendliness score I assigned to each response (worst in dark red to best in dark green):</p>

<p><a href="http://miss-usa-evolution.heroku.com/?column=score"><img src="http://dl.dropbox.com/u/10506/blog/miss-usa-evolution/score.png" alt="Score" /></a></p>

<p>This map shows which contestants believe schools should teach creationism:</p>

<p><a href="http://miss-usa-evolution.heroku.com/?column=teach_creationism"><img src="http://dl.dropbox.com/u/10506/blog/miss-usa-evolution/teach-creationism.png" alt="Teach Creationism" /></a></p>

<p>And this map shows which contestants mention science in their response:</p>

<p><a href="http://miss-usa-evolution.heroku.com/?column=mentions_science"><img src="http://dl.dropbox.com/u/10506/blog/miss-usa-evolution/mention-science.png" alt="Mention Science" /></a></p>

<p>So while there aren&#8217;t any striking patterns (I was hoping for a bright swath of red across the most religious states), the data&#8217;s still fun to look at, and it&#8217;s helpful to see that Alabama, whose <a href="http://www.youtube.com/watch?v=UkBmhM0R2A0&amp;feature=youtu.be#at=42">strongly anti-evolution</a> candidate is the only one colored red in both of the first two maps, does seem to be a bit of an outlier with respect to the rest of the nation.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Topic Modeling the Sarah Palin Emails]]></title>
    <link href="http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails/"/>
    <updated>2011-06-27T17:19:42-07:00</updated>
    <id>http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails</id>
    <content type="html"><![CDATA[<h1>LDA-based Email Browser</h1>

<p>Earlier this month, several thousand emails from Sarah Palin&#8217;s time as governor of Alaska were <a href="http://sunlightlabs.com/blog/2011/sarahs-inbox/">released</a>. The emails weren&#8217;t organized in any fashion, though, so to make them easier to browse, I&#8217;ve been working on some topic modeling (in particular, using latent Dirichlet allocation) to separate the documents into different groups.</p>

<p>I threw up <a href="http://sarah-palin.heroku.com/">a simple demo app</a> to view the organized documents <a href="http://sarah-palin.heroku.com/">here</a>.</p>

<h1>What is Latent Dirichlet Allocation?</h1>

<p>Briefly, given a set of documents, LDA tries to learn the latent topics underlying the set. It represents each document as a mixture of topics (generated from a Dirichlet distribution), each of which emits words with a certain probability.</p>

<p>For example, given the sentence &#8220;I listened to Justin Bieber and Lady Gaga on the radio while driving around in my car&#8221;, an LDA model might represent this sentence as 75% about music (a topic which, say, emits the words <em>Bieber</em> with 10% probability, <em>Gaga</em> with 5% probability, <em>radio</em> with 1% probability, and so on) and 25% about cars (which might emit <em>driving</em> with 15% probability and <em>cars</em> with 10% probability).</p>

<p>If you&#8217;re familiar with latent semantic analysis, you can think of LDA as a generative version. (For a more in-depth explanation, I wrote an introduction to LDA <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">here</a>.)</p>

<h1>Sarah Palin Email Topics</h1>

<p>Here&#8217;s a sample of the topics learnt by the model, as well as the top words for each topic. (Names, of course, are based on my own interpretation.)</p>

<ul>
<li><a href="http://sarah-palin.heroku.com/topics/24"><strong>Wildlife/BP Corrosion</strong></a>: game, fish, moose, wildlife, hunting, bears, polar, bear, subsistence, management, area, board, hunt, wolves, control, department, year, use, wolf, habitat, hunters, caribou, program, denby, fishing, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/0"><strong>Energy/Fuel/Oil/Mining</strong></a>: energy, fuel, costs, oil, alaskans, prices, cost, nome, now, high, being, home, public, power, mine, crisis, price, resource, need, community, fairbanks, rebate, use, mining, villages, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/19"><strong>Trig/Family/Inspiration</strong></a>: family, web, mail, god, son, from, congratulations, children, life, child, down, trig, baby, birth, love, you, syndrome, very, special, bless, old, husband, years, thank, best, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/6"><strong>Gas</strong></a>: gas, oil, pipeline, agia, project, natural, north, producers, companies, tax, company, energy, development, slope, production, resources, line, gasline, transcanada, said, billion, plan, administration, million, industry, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/12"><strong>Education/Waste</strong></a>: school, waste, education, students, schools, million, read, email, market, policy, student, year, high, news, states, program, first, report, business, management, bulletin, information, reports, 2008, quarter, …</li>
<li><a href="http://sarah-palin.heroku.com/topics/15"><strong>Presidential Campaign/Elections</strong></a>: mail, web, from, thank, you, box, mccain, sarah, very, good, great, john, hope, president, sincerely, wasilla, work, keep, make, add, family, republican, support, doing, p.o, …</li>
</ul>


<p>Here&#8217;s a sample email from the wildlife topic:</p>

<p><a href="http://sarah-palin.heroku.com/emails/6719"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-email.png" alt="Wildlife Email" /></a></p>

<p>I also thought the classification for <a href="http://sarah-palin.heroku.com/emails/12900">this email</a> was really neat: the LDA model labeled it as 10% in the <a href="http://sarah-palin.heroku.com/topics/15">Presidential Campaign/Elections</a> topic and 90% in the <a href="http://sarah-palin.heroku.com/topics/24">Wildlife</a> topic, and it&#8217;s precisely a wildlife-based protest against Palin as a choice for VP:</p>

<p><a href="http://sarah-palin.heroku.com/emails/12900"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/wildlife-vp.png" alt="Wildlife-VP Protest" /></a></p>

<h1>Future Analysis</h1>

<p>In a future post, I&#8217;ll perhaps see if we can glean any interesting patterns from the email topics. For example, for a quick graph now, if we look at the percentage of emails in the <a href="http://sarah-palin.heroku.com/topics/19">Trig/Family/Inspiration topic</a> across time, we see that there&#8217;s a spike in April 2008 &#8211; exactly (and unsurprisingly) the month in which Trig was born.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-topic.png"><img src="http://dl.dropbox.com/u/10506/blog/palin-browser/trig-topic.png" alt="Trig" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spork: Spoon or Fork?]]></title>
    <link href="http://blog.echen.me/2011/06/14/spork-spoon-or-fork/"/>
    <updated>2011-06-14T23:32:04-07:00</updated>
    <id>http://blog.echen.me/2011/06/14/spork-spoon-or-fork</id>
    <content type="html"><![CDATA[<p>There&#8217;s <a href="http://www.youtube.com/watch?v=ruJ76-o5lxU">an adorable scene</a> in <em>WALL-E</em> where, unable to decide whether a spork belongs with his spoon collection or fork collection, WALL-E enters an infinite loop and spontaneously combusts*.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/spork/spork-indecision-2.gif"><img src="http://dl.dropbox.com/u/10506/blog/spork/spork-indecision-2.gif" alt="Spork" /></a></p>

<p>This led me to the following timeless question: is a spork more spoon or fork?</p>

<p>To answer, I did a little digging. First, I pulled images of spoons and forks from <a href="http://www.bing.com/images">Bing</a> (natch) and used these to train a spoon-vs.-fork SVM (using the excellent <a href="http://scikit-learn.sourceforge.net/">scikit-learn</a> package).</p>

<p>Using leave-one-out cross-validation, the classifier showed roughly 85% accuracy: 4 out of 20 spoons were misclassified as forks, and 2 out of 20 forks were misclassified as spoons. Not bad, considering the limited dataset and lack of tuning.</p>

<p>Finally, I applied the classifier to my spork images.</p>

<p>The results? 2 out of 20 sporks were classified as forks and the remaining 18 were classified as spoons, thereby definitively proving that <strong>a spork is 10% fork and 90% spoon</strong>.</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/spork/spork-decomposition.png"><img src="http://dl.dropbox.com/u/10506/blog/spork/spork-decomposition.png" alt="Spork Decomposition" /></a></p>

<p>*Not really.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TWSS: Building a That's What She Said Classifier]]></title>
    <link href="http://blog.echen.me/2011/05/05/twss-building-a-thats-what-she-said-classifier/"/>
    <updated>2011-05-05T00:09:17-07:00</updated>
    <id>http://blog.echen.me/2011/05/05/twss-building-a-thats-what-she-said-classifier</id>
    <content type="html"><![CDATA[<p>There&#8217;s been a <a href="www.cs.washington.edu/homes/brun/pubs/pubs/Kiddon11.pdf">fun paper</a> on <a href="http://www.urbandictionary.com/define.php?term=that's%20what%20she%20said">That&#8217;s What She Said</a> identification making the rounds recently, so I spent some time over the weekend building my own classifier.</p>

<p><strong>tl;dr</strong> A simple unigram Naive Bayes model works pretty well, with performance on the lines of 0.969 precision and 0.823 recall. There&#8217;s a demo <a href="http://twss-classifier.heroku.com/">here</a>. I also ran the classifier over some fairy tale text.</p>

<h1>Naive Bayes Classifier</h1>

<p>To start, I used a super simple Naive Bayes classifier, trained on unigrams (with add-one smoothing). (See the appendix at the end for details.)</p>

<h2>Unigram vs. Bigram Model</h2>

<p>Here are the most predictive features, after training a Naive Bayes model with unigrams:</p>

<pre><code>unigram         p(twss|unigram)
pull            0.9724889822144924
bigger          0.9614677503890157
wet             0.959004244327654
hard            0.9527628206878138
stick           0.9505783678914388
hole            0.9443870318715991
oh              0.9432941279908561
replied         0.943294127990856
fast            0.943294127990856
longer          0.9397415371025485
</code></pre>

<p>Just to compare, here are some of the most predictive features in a bigram model (I added START and END tokens to the beginning and end of each sentence, in order to have some contextual features):</p>

<pre><code>bigram          p(twss|bigram)
it in           0.9801434151851175
START wow       0.9705079286853889
START oh        0.9473580156961879
its too         0.9350522640444204
pull out        0.9187779331523677
too big         0.9187779331523677
START man       0.9113755525471394
hard END        0.9113755525471394
put it          0.9071442285463515
that thing      0.9024886021363793
stick it        0.9024886021363793
my god          0.9024886021363793
go in           0.8916207044791409
START ugh       0.8916207044791409
make it         0.8916207044791409
its so          0.8916207044791409
</code></pre>

<p>Amusingly, sentences starting with <em>wow</em>, <em>oh</em>, and <em>ugh</em>, tend to be good candidates for TWSS sentences, as well as sentences containing <em>it&#8217;s too</em> and <em>it&#8217;s so</em>.</p>

<p>Here&#8217;s a precision-recall curve comparing the two models. We see that, on our datasets, a unigram classifier handily beats a bigram classifier:</p>

<p><a href="http://dl.dropbox.com/u/10506/blog/twss/precision-recall.png"><img src="http://dl.dropbox.com/u/10506/blog/twss/precision-recall.png" alt="Precision-Recall" /></a></p>

<h1>Darling it&#8217;s better</h1>

<p>Although our classifier gives excellent performance on our test set, one question is how well it generalizes to other sources of data.</p>

<p>I didn&#8217;t have another set of positive examples to draw from, but I took a set of fairy tales on Project Gutenberg to form a new batch of negative examples. On this new set of fairy tale sentences, the classifier misclassified only 81 out of 912 of the examples (assuming they should all be classified as &#8220;not TWSS&#8221;), which is a pretty low error rate of 8.89%.</p>

<p>What do these misclassifications look like? Here are some of the false positives it made:</p>

<h2>Aladdin</h2>

<ul>
<li>&#8220;The African magician carries it carefully wrapt up in his bosom,&#8221; said the princess; &#8220;and this I can assure you, because he pulled it out before me, and showed it to me in triumph.&#8221;</li>
<li>It is vanished; but I had no concern in its removal.</li>
<li>&#8220;My son,&#8221; said he, &#8220;what a man you are to do such surprising things always in the twinkling of an eye!&#8221;</li>
<li>&#8220;Sire,&#8221; replied Aladdin, &#8220;I have not the least reason to complain of your conduct, since you did nothing but what your duty required.&#8221;</li>
<li>&#8220;With your leave, mother,&#8221; replied Aladdin, &#8220;I shall now take care how I sell a lamp which may be so serviceable both to you and me.&#8221;</li>
</ul>


<p><a href="http://dl.dropbox.com/u/10506/blog/twss/aladdin.png"><img src="http://dl.dropbox.com/u/10506/blog/twss/aladdin.png" alt="http://dl.dropbox.com/u/10506/blog/twss/aladdin.png" /></a></p>

<h2>Hansel and Gretel</h2>

<ul>
<li>&#8220;Oh dear,&#8221; he said, &#8220;do let me go and see the hunt; I cannot restrain myself.&#8221;</li>
<li>When they awoke it was dark night, and poor Grethel began to cry, and said, &#8220;Oh, how shall we get out of the wood?&#8221; But Hansel comforted her.</li>
<li>&#8220;But remember,&#8221; she said, &#8220;I must lock the cottage door against those huntsmen, so when you come back in the evening, and knock, I shall not admit you, unless you say, &#8216;Dear little sister let me in.&#8217;</li>
</ul>


<h2>Snow White</h2>

<ul>
<li>One was too long, another too short; so she tried them all till she came to the seventh, and that was so comfortable that she laid herself down, and was soon fast asleep.</li>
<li>&#8220;Oh yes, I will try,&#8221; said Snow-white.</li>
</ul>


<p>Note that a lot of these really are TWSS sentences! (Now we know where <a href="http://www.snopes.com/disney/films/mermaid.asp">those Disney artists</a> got their inspiration.) So the classifier appears to generalize surprisingly well.</p>

<h1>A Demo</h1>

<p>I created <a href="http://twss-classifier.heroku.com/">a small Sinatra app</a> to play around with the classifier, and deployed it <a href="http://twss-classifier.heroku.com/">here</a>. I also put the code <a href="https://github.com/echen/twss-classifier">on Github</a>.</p>

<h1>Appendix: Details</h1>

<h2>Datasets</h2>

<p>To grab some TWSS examples, my first thought was to turn to <a href="http://search.twitter.com/search?q=twss">twitter</a>. However, the twitter data turned out to be incredibly noisy (lots of junk obscuring not terribly funny examples), so taking a cue from the K&amp;B paper, I pulled data from three sources:</p>

<ul>
<li>First, I scraped <a href="http://twssstories.com">twss stories</a> for positive training data (sentences for which a TWSS response is appropriate). Since only the last part in quotes from each submission is truly relevant to being a TWSS, I kept the quote and threw away the rest of each submission.</li>
<li>Next, I scraped <a href="http://textsfromlastnight.com">Texts from Last Night</a> and <a href="http://www.fmylife.com">FMyLife</a> for negative training data. Since TWSS jokes are usually made in response to <em>sentences</em> (the last sentence you say), I split each submission into sentences, and each sentence formed a single negative training example.</li>
</ul>


<p>Finally, I normalized each sentence by converting to lowercase and removing any punctuation.</p>

<h2>The Algorithm</h2>

<p>To train and test the classifier, I used the examples I gathered to form training and test sets, each consisting of:</p>

<ul>
<li>1000 positive examples from twss stories.</li>
<li>500 negative examples from Texts from Last Night.</li>
<li>500 negative examples from FMyLife.</li>
</ul>


<h2>Performance of the Unigram Model</h2>

<p>Since we want to optimize for precision (we hope the things we classify as TWSS sentences really are TWSS sentences) over recall (it&#8217;s fine if we don&#8217;t always respond TWSS to every TWSS sentence), let&#8217;s use 0.99 probability as our threshold for classifying a sentence as TWSS. (This is equivalent to setting a low prior for the probability of being a TWSS.)</p>

<p>With this threshold, a unigram Naive Bayes classifier has the following performance on a test set:</p>

<ul>
<li>True positives: 823</li>
<li>False negatives: 177</li>
<li>True negatives: 974</li>
<li>False positives: 26</li>
<li>Precision = 0.969</li>
<li>Recall = 0.823</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Filtering for English Tweets: Unsupervised Language Detection on Twitter]]></title>
    <link href="http://blog.echen.me/2011/05/01/unsupervised-language-detection-algorithms/"/>
    <updated>2011-05-01T16:28:07-07:00</updated>
    <id>http://blog.echen.me/2011/05/01/unsupervised-language-detection-algorithms</id>
    <content type="html"><![CDATA[<p>(See a demo <a href="http://babel-fett.heroku.com/">here</a>.)</p>

<p>While working on a Twitter sentiment analysis project, I ran into the problem of needing to filter out all non-English tweets. (Asking the Twitter API for English-only tweets doesn&#8217;t seem to work, as it nonetheless returns tweets in Spanish, Portuguese, Dutch, Russian, and a couple other languages.)</p>

<p>Since I didn&#8217;t have any labeled data, I thought it would be fun to build an <strong>unsupervised</strong> language classifier. In particular, using an EM algorithm to build a naive Bayes model of English vs. non-English n-gram probabilities turned out to work quite well, so here&#8217;s a description.</p>

<h1>EM Algorithm</h1>

<p>Let&#8217;s recall the naive Bayes algorithm: given a tweet (a set of <em>character</em> n-grams), we estimate its language to be the language $L$ that maximizes</p>

<p>$$P(language = L | ngrams) \propto P(ngrams | language = L) P(language = L)$$</p>

<p>Thus, we need to estimate $P(ngram | language = L)$ and $P(language = L)$.</p>

<p>This would be easy <strong>if we knew the language of each tweet</strong>, since we could estimate</p>

<ul>
<li>$P(xyz| language = English)$ as #(number of times &#8220;xyz&#8221; is a trigram in the English tweets) / #(total trigrams in the English tweets)</li>
<li>$P(language = English)$ as the proportion of English tweets.</li>
</ul>


<p>Or, it would also be easy <strong>if we knew the n-gram probabilities for each language</strong>, since we could use Bayes&#8217; theorem to compute the language <em>probabilities</em> for each tweet, and then take a weighted variant of the previous paragraph.</p>

<p><strong>The problem is that we know neither of these.</strong> So what the EM algorithm says is that that we can simply <strong>guess</strong>:</p>

<ul>
<li>Pretend we know the language of each tweet (by randomly assigning them at the beginning).</li>
<li>Using this guess, we can compute the n-gram probabilities for each language.</li>
<li>Using the n-gram probabilities for each language, we can recompute the language probabilities of each tweet.</li>
<li>Using these recomputed language probabilities, we can recompute the n-gram probabilities.</li>
<li>And so on, recomputing the language probabilities and n-gram probabilities over and over. While our guesses will be off in the beginning, the probabilities will eventually converge to (locally) minimize the likelihood. (In my tests, my language detector would sometimes correctly converge to an English detector, and sometimes it would converge to an English-and-Dutch detector.)</li>
</ul>


<h2>EM Analogy for the Layman</h2>

<p>Why does this work? Suppose you suddenly move to New York, and you want a way to differentiate between tourists and New Yorkers based on their activities. Initially, you don&#8217;t know who&#8217;s a tourist and who&#8217;s a New Yorker, and you don&#8217;t know which are touristy activities and which are not. So you randomly place people into two groups A and B. (You randomly assign all tweets to a language)</p>

<p>Now, given all the people in group A, you notice that a large number of them visit the Statue of Liberty; similarly, you notice that a large number of people in group B walk really quickly. (You notice that one set of words often has the n-gram &#8220;ing&#8221;, and that another set of words often has the n-gram &#8220;ias&#8221;; that is, you fix the language probabilities for each tweet, and recompute the n-gram probabilities for each language.)</p>

<p>So you start to put people visiting the Statue of Liberty in group A, and you start to put fast walkers in group B. (You fix the n-gram probabilities for each language, and recompute the language probabilities for each tweet.)</p>

<p>With your new A and B groups, you notice more differentiating factors: group A people tend to carry along cameras, and group B people tend to be more finance-savvy.</p>

<p>So you start to put camera-carrying folks in group A, and finance-savvy folks in group B.</p>

<p>And so on. Eventually, you settle on two groups of people and differentiating activities: people who walk slowly and visit the Statue of Liberty, and busy-looking people who walk fast and don&#8217;t visit. Assuming there are more native New Yorkers than tourists, you can then guess that the natives are the larger group.</p>

<h1>Results</h1>

<p>I wrote some Ruby code to implement the above algorithm, and trained it on half a million tweets, using English and &#8220;not English&#8221; as my two languages. The results looked surprisingly good from just eyeballing:</p>

<p><a href="https://img.skitch.com/20110303-qfrnb8gstgheh4xech4iutfskd.jpg"><img src="https://img.skitch.com/20110303-qfrnb8gstgheh4xech4iutfskd.jpg" alt="Example Results" /></a></p>

<p>But in order to get some hard metrics and to tune parameters (e.g., n-gram size), I needed a labeled dataset. So I pulled a set of English-language and Spanish-language documents from Project Gutenberg, and split them to form training and test sets (the training set consisted of 2000 lines of English and 1000 lines of Spanish, and  1000 lines of English and 1000 lines of Spanish for the test set).</p>

<p>Trained on bigrams, the detector resulted in:</p>

<ul>
<li>991 true positives (English lines correctly classified as English)</li>
<li>9 false negatives (English lines incorrectly classified as Spanish</li>
<li>11 false positives (Spanish lines incorrectly classified as English)</li>
<li>989 true negatives (Spanish lines correctly classified as English)</li>
</ul>


<p>for a precision of 0.989 and a recall of 0.991.</p>

<p>Trained on trigrams, the detector resulted in:</p>

<ul>
<li>992 true positives</li>
<li>8 false negatives</li>
<li>10 false positives</li>
<li>990 true negatives</li>
</ul>


<p>for a precision of 0.990 and a recall of 0.992.</p>

<p>Also, when I looked at the sentences the detector was making errors on, I saw that they almost always consisted of only one or two words (e.g., the incorrectly classified sentences were lines like &#8220;inmortal&#8221;, &#8220;autumn&#8221;, and &#8220;salir&#8221;). So the detector pretty much never made a mistake on a normal sentence!</p>

<h1>Code/Demo</h1>

<p>I put the code on <a href="https://github.com/echen/unsupervised-language-identification">my Github account</a>, and a quick <a href="http://babel-fett.heroku.com/">demo app</a>, trained on trigrams from tweets with lang=&#8221;en&#8221; according to the Twitter API, is <a href="http://babel-fett.heroku.com/">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Eigensheep]]></title>
    <link href="http://blog.echen.me/2011/05/01/eigensheep/"/>
    <updated>2011-05-01T15:23:31-07:00</updated>
    <id>http://blog.echen.me/2011/05/01/eigensheep</id>
    <content type="html"><![CDATA[<p>Aaron Koblin&#8217;s <a href="http://www.thesheepmarket.com/">Sheep Market</a> visualization is an awesome use of Mechanical Turk. But it&#8217;d be even more awesome if the grid were <em>ordered</em>, so I decided to try projecting the sheep onto two dimensions.</p>

<h1>Principal Sheep Components</h1>

<p>After screenshotting the first 50 sheep from the market and normalizing their size and color, here&#8217;s what a PCA projection looks like (<a href="http://dl.dropbox.com/u/10506/eigensheep.png">click</a> for a larger view):</p>

<p><a href="http://dl.dropbox.com/u/10506/eigensheep.png"><img src="http://dl.dropbox.com/u/10506/eigensheep.png" alt="Projected Sheep" /></a></p>

<p>Notice how the stroke widths get thicker as we move to the right (i.e., <strong>the first principal component seems to measure the blackness of the sheep</strong>), and the amount of wool on the sheep&#8217;s body increases as we move up (i.e., <strong>the second principal component seems to measure the wooliness of the sheep</strong>).</p>

<p>It&#8217;s also pretty neat how all the sheep with black heads and black legs (sheep 35, 16, 32, 31, and 19) get clumped together:</p>

<p><a href="http://dl.dropbox.com/u/10506/eigensheep-black-heads.png"><img src="http://dl.dropbox.com/u/10506/eigensheep-black-heads.png" alt="Projected Sheep, Black Heads/Legs Circled" /></a></p>

<p>And I think the sheep on the left (next to and inside the dense cluster) seem much more poorly drawn &#8211; they look more like camels, dogs, unicorns, or bugs than actual sheep.</p>

<h1>Code</h1>

<p>In a bit more detail, I used the poor man&#8217;s Mechanical Turk (myself) to screenshot the first 50 sheep from the market, trying to hug the sheep as closely as possible to ensure proper alignment. Next, I used the <a href="http://www.pythonware.com/products/pil/">Python Imaging Library</a> to resize the images to 150x150px, convert them to grayscale, and flatten them into the rows of a matrix.</p>

<p>In case anyone else wants to play with the sheep images, I put the <a href="https://github.com/echen/eigensheep">code</a> on my <a href="https://github.com/echen/eigensheep">Github</a> account.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Choosing a Machine Learning Classifier]]></title>
    <link href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/"/>
    <updated>2011-04-27T18:43:15-07:00</updated>
    <id>http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier</id>
    <content type="html"><![CDATA[<p>How do you know what machine learning algorithm to choose for your classification problem? Of course, if you really care about accuracy, your best bet is to test out a couple different ones (making sure to try different parameters within each algorithm as well), and select the best one by cross-validation. But if you&#8217;re simply looking for a &#8220;good enough&#8221; algorithm for your problem, or a place to start, here are some general guidelines I&#8217;ve found to work well over the years.</p>

<h1>How large is your training set?</h1>

<p>If your training set is small, high bias/low variance classifiers (e.g., Naive Bayes) have an advantage over low bias/high variance classifiers (e.g., kNN), since the latter will overfit. But low bias/high variance classifiers start to win out as your training set grows (they have lower asymptotic error), since high bias classifiers aren&#8217;t powerful enough to provide accurate models.</p>

<p>You can also think of this as a generative model vs. discriminative model distinction.</p>

<h1>Advantages of some particular algorithms</h1>

<p><strong>Advantages of Naive Bayes:</strong> Super simple, you&#8217;re just doing a bunch of counts. If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. And even if the NB assumption doesn&#8217;t hold, a NB classifier still often does a great job in practice. A good bet if  want something fast and easy that performs pretty well. Its main disadvantage is that it can&#8217;t learn interactions between features (e.g., it can&#8217;t learn that although you love movies with Brad Pitt and Tom Cruise, you hate movies where they&#8217;re together).</p>

<p><strong>Advantages of Logistic Regression:</strong> Lots of ways to regularize your model, and you don&#8217;t have to worry as much about your features being correlated, like you do in Naive Bayes. You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust classification thresholds, to say when you&#8217;re unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.</p>

<p><strong>Advantages of Decision Trees:</strong> Easy to interpret and explain (for some people &#8211; I&#8217;m not sure I fall into this camp). They easily handle feature interactions and they&#8217;re non-parametric, so you don&#8217;t have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end). One disadvantage is that they don&#8217;t support online learning, so you have to rebuild your tree when new examples come on. Another disadvantage is that they easily overfit, but that&#8217;s where ensemble methods like random forests (or boosted trees) come in. Plus, random forests are often the winner for lots of problems in classification (usually slightly ahead of SVMs, I believe), they&#8217;re fast and scalable, and you don&#8217;t have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days.</p>

<p><strong>Advantages of SVMs:</strong> High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you&#8217;re data isn&#8217;t linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive, hard to interpret, and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.</p>

<h1>But&#8230;</h1>

<p>Recall, though, that better data often beats better algorithms, and designing good features goes a long way. And if you have a huge dataset, then whichever classification algorithm you use might not matter so much in terms of classification performance (so choose your algorithm based on speed or ease of use instead).</p>

<p>And to reiterate what I said above, if you really care about accuracy, you should definitely try a bunch of different classifiers and select the best one by cross-validation. Or, to take a lesson from the Netflix Prize (and Middle Earth), just use an ensemble method to choose them all.</p>
]]></content>
  </entry>
  
</feed>
