
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Counting Clusters - Edwin Chen's Blog</title>
  <meta name="author" content="Edwin Chen">

  
  <meta name="description" content="Given a set of datapoints, we often want to know how many clusters the datapoints form. The gap statistic and the prediction strength are two &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.echen.me/2011/03/19/counting-clusters">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/feed/" rel="alternate" title="Edwin Chen's Blog" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-29005692-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Edwin Chen's Blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/feed/" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.echen.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Counting Clusters</h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-03-19T04:22:16-07:00" pubdate data-updated="true">Mar 19<span>th</span>, 2011</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>Given a set of datapoints, we often want to know how many clusters the datapoints form. The <strong>gap statistic</strong> and the <strong>prediction strength</strong> are two practical algorithms for choosing the number of clusters.</p>

<h1>Gap Statistic</h1>

<p>The <a href="http://www.stanford.edu/~hastie/Papers/gap.pdf">gap statistic algorithm</a> works as follows:</p>

<p>For each i from 1 up to some maximum number of clusters,</p>

<ol>
<li><p>Run a k-means algorithm on the original dataset to find i clusters, and sum the distance of all points from their cluster mean. Call this sum the <strong>dispersion</strong>.</p></li>
<li><p>Generate a set of <em>reference</em> datasets (of the same size as the original). One simple way of generating a reference dataset is to sample uniformly from the original dataset&#8217;s bounding rectangle; a more sophisticated approach is take into account the original dataset&#8217;s shape by sampling, say, from a rectangle formed from the original dataset&#8217;s principal components.</p></li>
<li><p>Calculate the dispersion of each of these reference datasets, and take their mean.</p></li>
<li><p>Define the ith <strong>gap</strong> by: log(mean dispersion of reference datasets) - log(dispersion of original dataset).</p></li>
</ol>


<p>Once we&#8217;ve calculated all the gaps (we can add confidence intervals as well; see <a href="http://www.stanford.edu/~hastie/Papers/gap.pdf">the original paper</a> for the formula), we can select the number of clusters to be the one that gives the maximum gap. (Sidenote: I view the gap statistic as a very statistical-minded algorithm, since it compares the original dataset against a set of reference &#8220;control&#8221; datasets.)</p>

<p>For example, here I&#8217;ve generated three Gaussian clusters:</p>

<p><a href="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d.png"><img src="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d.png" alt="Three Gaussian Clusters" /></a></p>

<p>And running the gap statistic algorithm, we see that it correctly detects the number of clusters to be three:</p>

<p><a href="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d_gaps.png"><img src="https://github.com/echen/gap-statistic/raw/master/examples/3_clusters_2d_gaps.png" alt="Gap Statistic on Three Gaussian Clusters" /></a></p>

<p>For a sample R implementation of the gap statistic, see the Github repository <a href="https://github.com/echen/gap-statistic">here</a>.</p>

<h1>Prediction Strength</h1>

<p>Another cluster-counting algorithm is the <a href="http://www-stat.stanford.edu/~tibs/ftp/predstr.ps">prediction strength algorithm</a>. In contrast to the gap statistic (which, as mentioned above, I find very statistically), I see prediction strength as taking a more machine learning viewpoint, since it&#8217;s formulated as a supervised learning problem validated against a test set.</p>

<p>To calculate prediction strength, for each i from 1 up to some maximum number of clusters:</p>

<ol>
<li><p>Divide the dataset into two groups, a training set and a test set.</p></li>
<li><p>Run a k-means algorithm on each set to find i clusters.</p></li>
<li><p>For each <em>test</em> cluster, count the proportion of pairs of points in that cluster that would remain in the same cluster, if each were assigned to its closest <em>training</em> cluster mean.</p></li>
<li><p>The minimum over these proportions is the <strong>prediction strength</strong> for i clusters.</p></li>
</ol>


<p>Once we&#8217;ve calculated the prediction strength for each number of clusters, we select the number of clusters to be the maximum i such that the prediction strength for i is greater than some threshold. (The paper suggests 0.8 - 0.9 as a good threshold, and I&#8217;ve seen 0.8 work well in practice.)</p>

<p>Here&#8217;s the prediction strength algorithm run on the same example above:</p>

<p><a href="https://github.com/echen/prediction-strength/raw/master/examples/3_clusters_2d_ps.png"><img src="https://github.com/echen/prediction-strength/raw/master/examples/3_clusters_2d_ps.png" alt="Prediction Strength on Three Gaussian Clusters" /></a></p>

<p>Again, check out a sample R implementation of the prediction strength <a href="https://github.com/echen/prediction-strength">here</a>.</p>

<p>In practice, I tend to prefer using the gap statistic algorithm, since it&#8217;s a little easier to code and it doesn&#8217;t require selecting an arbitrary threshold like the prediction strength does. I&#8217;ve also found that it gives slightly better results (though the original prediction strength paper has the opposite finding).</p>

<h1>Appendix</h1>

<p>I ended up giving a brief description of two very common clustering algorithms, <strong>k-means</strong> and <strong>Gaussian mixture models</strong> in the comments, so I figured I might as well bring them up here.</p>

<h2>k-means algorithm</h2>

<p>Suppose we have a set of datapoints that we want to cluster. We want to learn two things:</p>

<ul>
<li>A description of the clusters themselves (so that if new points come in, we can assign them to a cluster).</li>
<li>Which clusters our current points fall into.</li>
</ul>


<p>We start by initializing k cluster centers (e.g., by randomly choosing k points among our datapoints). Then we repeatedly</p>

<ul>
<li><strong>Step A</strong>: Assign each datapoint to the nearest cluster center.</li>
<li><strong>Step B</strong>: Update all the cluster centers: for each cluster i, take the mean over all points currently in the cluster, and update cluster center i to be this mean.</li>
<li>(Repeat steps A and B above until the cluster assignments stop changing.)</li>
</ul>


<p>And that&#8217;s pretty much it for k-means.</p>

<h2>k-means from an EM point of View</h2>

<p>To ease the transition into Gaussian mixture models,
let&#8217;s also describe the k-means algorithm using <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM</a> language.</p>

<p>Note that if we knew for certain either 1) the exact cluster centers or 2) the cluster each point belonged to, we could trivially solve k-means, since</p>

<ul>
<li>If we knew the exact cluster centers, all we&#8217;d have to do is assign each point to its nearest cluster center, and we&#8217;d be done.</li>
<li>If we knew which cluster each point belonged to, we could pick the cluster center by simply taking the mean over all points in that cluster.</li>
</ul>


<p>The problem is that we know <em>neither</em> of these, and so we alternate between making educated guesses of each one:</p>

<ul>
<li>In A step above, we pretend that we know the cluster centers, and based off this pretense, we guess which cluster each point belongs to. (This is also known as the <strong>E step</strong> in the EM algorithm.)</li>
<li>In the B step above, we do the reverse: we pretend that we know which cluster each point belongs to, and then try to guess the cluster centers. (This is also known as the <strong>M step</strong> in EM.)</li>
</ul>


<p>Our guesses keep getting better and better, and eventually we&#8217;ll converge.</p>

<h2>Gaussian Mixture Models</h2>

<p>k-means has a <em>hard</em> notion of clustering: point X either belongs to cluster C or it doesn&#8217;t. But sometimes we want a <em>soft</em> notion instead: point X belongs to cluster C with probability p (according to a Gaussian kernel). This is where <a href="http://en.wikipedia.org/wiki/Mixture_model">Gaussian mixture modeling</a> comes in.</p>

<p>To run a GMM, we start by initializing $k$ Gaussians (say, by randomly choosing $k$ points to be the centers of the Gaussians and by setting the variance of each Gaussians to be the overall variance), and then we repeatedly:</p>

<ul>
<li><strong>E Step</strong>: Pretend we know the parameters of each Gaussian cluster, and assign each datapoint to Gaussian cluster i with appropriate probability.</li>
<li><strong>M Step</strong>: Pretend we know the probabilities that each point belongs to a given cluster. Using these probabilities, update the means and variances of each Gaussian cluster: the new mean for cluster i is the weighted mean over all points (where the weight of each point X is the probability that X belongs to cluster i), and similarly for the new variance.</li>
</ul>


<p>This is exactly like k-means in the EM formulation, except we replace the binary clustering formula with Gaussian kernels.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Edwin Chen</span></span>

      








  


<time datetime="2011-03-19T04:22:16-07:00" pubdate data-updated="true">Mar 19<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/categories/expository/'>expository</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://blog.echen.me/2011/03/19/counting-clusters/" data-via="echen" data-counturl="http://blog.echen.me/2011/03/19/counting-clusters/" >Tweet</a>
  
  
  <div class="g-plusone" data-size="medium"></div>
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2011/03/14/prime-numbers-and-the-riemann-zeta-function/" title="Previous Post: Prime Numbers and the Riemann Zeta Function">&laquo; Prime Numbers and the Riemann Zeta Function</a>
      
      
        <a class="basic-alignment right" href="/2011/03/22/harry-potter-and-the-keyword-chaos-unsupervised-statistically-significant-phrases/" title="next Post: Unsupervised Statistically Significant Phrases">Unsupervised Statistically Significant Phrases &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Edwin Chen</h1>
  
  <p>Data scientist at Twitter. Previously math and linguistics at MIT, quantitative trading at Clarium Capital.</p>
  
  <p>I like math, machine learning, linguistics, crowdsourcing, visualization, interactive data analysis.</p>
  
  <p>I contribute to a couple open-source projects, particularly <a href="https://github.com/twitter/clockworkraven">Clockwork Raven</a>, an evaluation tool released as part of the human computation system I've been building, and <a href="https://github.com/twitter/scalding">Scalding</a>, Twitter's functional programming-inspired Hadoop framework, which I'm writing a book for.</p>

  <p>Email: hello[at]echen.me<br/>
  Twitter: <a href="https://twitter.com/#!/echen">@echen</a><br/>
  Other: <a href="https://github.com/echen">Github</a>, <a href="https://plus.google.com/113804726252165471503/">Google+</a>, <a href="http://www.linkedin.com/in/edwinchen1">LinkedIn</a>, <a href="http://quora.com/edwin-chen-1">Quora</a></p>
</section><section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">Edge Prediction in a Social Graph: My Solution to Facebook's User Recommendation Contest on Kaggle</a>
      </li>
    
      <li class="post">
        <a href="/2012/07/06/soda-vs-pop-with-twitter/">Soda vs. Pop with Twitter</a>
      </li>
    
      <li class="post">
        <a href="/2012/04/25/making-the-most-of-mechanical-turk-tips-and-best-practices/">Making the Most of Mechanical Turk: Tips and Best Practices</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Infinite Mixture Models with Nonparametric Bayes and the Dirichlet Process</a>
      </li>
    
      <li class="post">
        <a href="/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/">Instant interactive visualization with d3 + ggplot2</a>
      </li>
    
      <li class="post">
        <a href="/2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding/">Movie recommendations and more via MapReduce and Scalding</a>
      </li>
    
      <li class="post">
        <a href="/2012/01/17/quick-introduction-to-ggplot2/">Quick Introduction to ggplot2</a>
      </li>
    
      <li class="post">
        <a href="/2012/01/03/introduction-to-conditional-random-fields/">Introduction to Conditional Random Fields</a>
      </li>
    
      <li class="post">
        <a href="/2011/10/24/winning-the-netflix-prize-a-summary/">Winning the Netflix Prize: A Summary</a>
      </li>
    
      <li class="post">
        <a href="/2011/09/29/stuff-harvard-people-like/">Stuff Harvard People Like</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("echen", 5, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/echen" class="twitter-follow-button" data-show-count="true">Follow @echen</a>
  
</section>


  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Edwin Chen -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'edwinchen';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://blog.echen.me/2011/03/19/counting-clusters/';
        var disqus_url = 'http://blog.echen.me/2011/03/19/counting-clusters/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>